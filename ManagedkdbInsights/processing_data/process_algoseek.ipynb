{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "47bb4665-c0eb-4f58-9749-64e13a1682a7",
   "metadata": {},
   "source": [
    "# Processing TAQ Data\n",
    "Example of how to process a collection of csv files, that are compressed, and arranged in a bespoke directory structure. The structure is **ROOT**/**DATE**/**A-Z First Letter of Ticker**/**TICKER**.csv.gz. All files come from and S3 bucket.\n",
    "\n",
    "## Architecture and Data Flow\n",
    "![Architecture](images/csv_arch.png \"Architecture\")\n",
    "\n",
    "To process the csv.gz files, first the provided tarball with data [algoseek-marketdata.tar.gz](algoseek-marketdata.tar.gz) is untarred and copied to an S3 bucket FinSpace can read from. To process the data files, the files are copied into the general purpose cluster, processed into memory by the cluster. The memory contents are then first saved to disk and then added to the managed database. Once the database has been updated the dataview of the database is updated to present the latest database state (with the added data), and the clusters using the database are also updated to use the latest state of the view. Finally the historical database (HDB) is queried to show the new data available for query from the HDB. \n",
    "\n",
    "## Algoseek LLC Data\n",
    "Trade and Quote data has been provided by [AlgoSeek LLC](https://www.algoseek.com/), you can learn more about their data offerings from their home page.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7b0c521d-ddc4-4c71-b83d-6f655103c51b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "import os\n",
    "import subprocess\n",
    "import boto3\n",
    "import json\n",
    "import datetime\n",
    "\n",
    "from env import *\n",
    "import pykx as kx\n",
    "import awswrangler as wr\n",
    "\n",
    "from managed_kx import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a2a91712-3f1d-4041-86de-c70543bfb0ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pykx.Identity(pykx.q('::'))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ----------------------------------------------------------------\n",
    "DB_NAME=\"DEMO_DB\"\n",
    "DBVIEW_NAME=f\"{DB_NAME}_VIEW\"\n",
    "SCALING_GROUP_NAME=\"DEMO_SCALING_GROUP\"\n",
    "VOLUME_NAME=\"DEMO_SHARED_VOLUME\"\n",
    "CODEBASE=\"demo\"\n",
    "CLUSTER_NAME=\"demo_csv_cluster\"\n",
    "\n",
    "HDB_CLUSTER_NAME=\"demo_hdb_cluster\"\n",
    "\n",
    "# S3 Destinations\n",
    "S3_CODE_PATH=\"code\"\n",
    "S3_DATA_PATH=\"data\"\n",
    "SOURCE_DATA_DIR=\"demo\"\n",
    "# ----------------------------------------------------------------\n",
    "WORKING_DIR=f\"/opt/kx/app/shared/{VOLUME_NAME}/{CLUSTER_NAME}\"\n",
    "\n",
    "LOCAL_DATA_HOME=\"algoseek-marketdata/us-equity-taq-faang\"\n",
    "\n",
    "## YOU MUST CHANGE THIS TO A DIRECTORY THAT WORKS FOR YOUR ACCOUNT\n",
    "S3_DATA_HOME=f\"s3://kdb-demo-{ACCOUNT_ID}-kms/algoseek-marketdata/us-equity-taq-faang\"\n",
    "\n",
    "# days supplied in Tarball\n",
    "start = datetime.date(2021,1,4)\n",
    "end = datetime.date(2021,1,5)\n",
    "\n",
    "dlist = pd.date_range(start, end, freq='B')\n",
    "\n",
    "ALL_DATES = []\n",
    "\n",
    "# convert all to date\n",
    "for d in dlist:\n",
    "    ALL_DATES.append(d.date())\n",
    "\n",
    "# ----------------------------------------------------------------\n",
    "\n",
    "# set pykx local q console width and height\n",
    "kx.q(\"\\c 500 500\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "22dd100d-d0aa-4f13-a305-502a86041b6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Defaults ...\n"
     ]
    }
   ],
   "source": [
    "# Create AWS Session for working with FinSpace service\n",
    "session=None\n",
    "\n",
    "if AWS_ACCESS_KEY_ID is None:\n",
    "    print(\"Using Defaults ...\")\n",
    "    # create AWS session: using access variables\n",
    "    session = boto3.Session()\n",
    "else:\n",
    "    print(\"Using variables ...\")\n",
    "    session = boto3.Session(\n",
    "        aws_access_key_id=AWS_ACCESS_KEY_ID,\n",
    "        aws_secret_access_key=AWS_SECRET_ACCESS_KEY,\n",
    "        aws_session_token=AWS_SESSION_TOKEN\n",
    "    )\n",
    "\n",
    "# create finspace client\n",
    "client = session.client(service_name='finspace', endpoint_url=ENDPOINT_URL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa2e3baf-910f-4361-a5ac-272f8aca7946",
   "metadata": {},
   "source": [
    "# Stage TAQ Data to S3\n",
    "Copy the supplied sample TAQ data to an S3 bucket. Then the data will be processed by GP cluster from the S3 location. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9685e738-4608-446b-a21d-4f8861eab0f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf algoseek-marketdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9ce2a015-fdba-4834-a118-4dfa7ace5e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "!tar xzf algoseek-marketdata.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ce0a5eee-b441-48da-825e-ee19d579190b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-08-13 16:25:55   15021638 algoseek-marketdata/us-equity-taq-faang/2021/20210104/A/AMZN.csv.gz\n",
      "2024-08-13 16:25:55   19321692 algoseek-marketdata/us-equity-taq-faang/2021/20210104/F/FB.csv.gz\n",
      "2024-08-13 16:25:55   14132728 algoseek-marketdata/us-equity-taq-faang/2021/20210104/G/GOOG.csv.gz\n",
      "2024-08-13 16:25:55   10610310 algoseek-marketdata/us-equity-taq-faang/2021/20210104/N/NFLX.csv.gz\n",
      "2024-08-13 16:25:55   11285284 algoseek-marketdata/us-equity-taq-faang/2021/20210105/A/AMZN.csv.gz\n",
      "2024-08-13 16:25:55   11544605 algoseek-marketdata/us-equity-taq-faang/2021/20210105/F/FB.csv.gz\n",
      "2024-08-13 16:25:55    9736261 algoseek-marketdata/us-equity-taq-faang/2021/20210105/G/GOOG.csv.gz\n",
      "2024-08-13 16:25:55    8014976 algoseek-marketdata/us-equity-taq-faang/2021/20210105/N/NFLX.csv.gz\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Stage TAQ data to S3\n",
    "if AWS_ACCESS_KEY_ID is not None:\n",
    "    cp = f\"\"\"\n",
    "export AWS_ACCESS_KEY_ID={AWS_ACCESS_KEY_ID}\n",
    "export AWS_SECRET_ACCESS_KEY={AWS_SECRET_ACCESS_KEY}\n",
    "export AWS_SESSION_TOKEN={AWS_SESSION_TOKEN}\n",
    "\n",
    "aws s3 rm --recursive {S3_DATA_HOME} --quiet\n",
    "aws s3 sync --exclude .DS_Store {LOCAL_DATA_HOME} {S3_DATA_HOME} --quiet\n",
    "aws s3 ls --recursive {S3_DATA_HOME}/\n",
    "\"\"\"\n",
    "else:\n",
    "    cp = f\"\"\"\n",
    "aws s3 rm --recursive {S3_DATA_HOME} --quiet\n",
    "aws s3 sync --exclude .DS_Store {LOCAL_DATA_HOME} {S3_DATA_HOME} --quiet\n",
    "aws s3 ls --recursive {S3_DATA_HOME}/\n",
    "\"\"\"\n",
    "    \n",
    "# execute the S3 copy\n",
    "os.system(cp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1709c470-bd30-4a16-8059-dc72be6096f4",
   "metadata": {},
   "source": [
    "# Before State of HDB\n",
    "This is the state/contents of the HDB before we process the CSV files as new date. There will be a table (taq) with no data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "43745eaf-db1b-4613-a75f-a8e25c3c7b02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Tables and Counts\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>taq</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "pykx.Dictionary(pykx.q('taq| 0'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Table: taq\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>t</th>\n",
       "      <th>f</th>\n",
       "      <th>a</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>c</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <td>\"d\"</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ticker</th>\n",
       "      <td>\"s\"</td>\n",
       "      <td></td>\n",
       "      <td>p</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Timestamp</th>\n",
       "      <td>\"n\"</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>EventType</th>\n",
       "      <td>\"s\"</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Price</th>\n",
       "      <td>\"f\"</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Quantity</th>\n",
       "      <td>\"j\"</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Exchange</th>\n",
       "      <td>\"s\"</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Conditions</th>\n",
       "      <td>\"s\"</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "pykx.KeyedTable(pykx.q('\n",
       "c         | t f a\n",
       "----------| -----\n",
       "date      | d    \n",
       "Ticker    | s   p\n",
       "Timestamp | n    \n",
       "EventType | s    \n",
       "Price     | f    \n",
       "Quantity  | j    \n",
       "Exchange  | s    \n",
       "Conditions| s    \n",
       "'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rows</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "pykx.KeyedTable(pykx.q('\n",
       "date| rows\n",
       "----| ----\n",
       "'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>rows</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th>Ticker</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "pykx.KeyedTable(pykx.q('\n",
       "date Ticker| rows\n",
       "-----------| ----\n",
       "'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>Ticker</th>\n",
       "      <th>Timestamp</th>\n",
       "      <th>EventType</th>\n",
       "      <th>Price</th>\n",
       "      <th>Quantity</th>\n",
       "      <th>Exchange</th>\n",
       "      <th>Conditions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "pykx.Table(pykx.q('\n",
       "date Ticker Timestamp EventType Price Quantity Exchange Conditions\n",
       "------------------------------------------------------------------\n",
       "'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Query the HDB for before state\n",
    "hdb = get_pykx_connection(client, \n",
    "                          environmentId=ENV_ID, clusterName=HDB_CLUSTER_NAME, \n",
    "                          userName=KDB_USERNAME, boto_session=session)\n",
    "\n",
    "tables = hdb(\"tables[]\").py()\n",
    "\n",
    "# inventory of tables in the database and rows in each\n",
    "print(80*'=')\n",
    "print(\"Tables and Counts\")\n",
    "display( hdb(\"tables[]!count each value each tables[]\") )\n",
    "\n",
    "# For each table: schema, and samples and counts\n",
    "for t in tables:\n",
    "    print(80*'=')\n",
    "    print (f'Table: {t}')\n",
    "    print(80*'-')\n",
    "    display( hdb(f\"meta {t}\") )\n",
    "    display( hdb(f\"select rows:count i by date from {t}\") )\n",
    "    display( hdb(f\"select rows:count i by date,Ticker from {t}\") )\n",
    "    display( hdb(f\"select from {t} where date = max date, i<3\") )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6787c3f0-f396-4683-9edc-f86de0786844",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Process from Cluster\n",
    "Connect to the GP cluster and use it to process the list of external S3 files (csv.gz format). The list of S3 files to process will be given to the cluster and processed with the process_s3_csvgz function in q provided by this notebook.\n",
    "\n",
    "## parse_csvgz\n",
    "```\n",
    "parse_csvgz:{[schema;file] (schema;enlist csv) 0: .Q.gz \"c\"$read1 hsym `$ string file};\n",
    "```\n",
    "This function will process a a local file (csv.gz) into an in-memory table taking a local file and schema as arguments. The file string is turned into a file handle using [hsym](https://code.kx.com/q/ref/hsym/), then the file contents is streamed using [read1](https://code.kx.com/q/ref/read1/), which is then uncompressed using [.Q.gz](https://code.kx.com/q/ref/dotq/#gz-gzip). The resulting csv stream is then parsed into a table using [enlist](https://code.kx.com/q/ref/enlist/) using the provided schema\n",
    "\n",
    "Arguments:   \n",
    "- schema: string for the table [q reference](https://code.kx.com/q/ref/file-text/#load-csv)   \n",
    "- file: file (csv.gz format) to parse   \n",
    "\n",
    "## process_s3_csvgz\n",
    "```\n",
    "process_s3_csvgz:{[schema;work_dir;s3_object]\n",
    "    r:.aws.s3.get_object[s3_object;work_dir];\n",
    "    raze {t:parse_csvgz[x;`$y]; hdel hsym`$y; t}[schema] each r`containerFileDestinationPath\n",
    "};\n",
    "```\n",
    "This function will process an S3 based file (csv.gz) into an in-memory table, levering the parse_csvgz function. The function will copy the S3 file to a working directory (arguments of function), process it with parse_csvgz and clean up after itself by deleting the copied file.\n",
    "\n",
    "The function copies the S3 object to the local working directory with [.aws.s3.get_object](https://docs.aws.amazon.com/finspace/latest/userguide/interacting-with-kdb-q-apis.html). Then gives the copied file location to the already defined function **parse_csvgz** to turn the file contents into a table (t) using the given schema, the file is then deleted with [hdel](https://code.kx.com/q/ref/hdel/) and the table t is returned. The [each](https://code.kx.com/q/ref/each/) function is used because the containerFileDestinationPath returned by [.aws.s3.get_object](https://docs.aws.amazon.com/finspace/latest/userguide/interacting-with-kdb-q-apis.html) is a column of a table returned by the get_object function, a list of files copied. \n",
    "\n",
    "Arguments:   \n",
    "- schema: string for the table [q reference](https://code.kx.com/q/ref/file-text/#load-csv)   \n",
    "- work_dir: path on local machine for where S3 object (files) will be copied to before   \n",
    "- s3_object: S3 object (.csv.gz file) that is the file to read into a table             \n",
    "\n",
    "# Parsing List of Files\n",
    "Parse the list (slist) of S3 objects into one table\n",
    "\n",
    "```\n",
    "taq:raze{.Q.gc[]; process_s3_csvgz [x; y; z]}[\"DNSSFJSS\";wd] peach slist;\n",
    "```\n",
    "The function uses [peach](https://code.kx.com/q/ref/each/) to process each S3 object in slist to a table using already defined function **process_s3_csvgz** creating a list of tables, then that list of tables is converted into one table using [raze](https://code.kx.com/q/ref/raze/).\n",
    "\n",
    "For memory efficiency, there is a call to free up memory using [.Q.gc](https://code.kx.com/q/ref/dotq/#gc-garbage-collect).\n",
    "\n",
    "# Add to Database: Create the Changeset\n",
    "Will create a changeset for the database that includes the two tables (df1 and df2) and will create a changeset as a new date partition of the database with the in-memory tables splayed to disk for today's date.\n",
    "\n",
    "Code run on the cluster saves in-memory tables to disk and then adds those files to the maanged database as a changeset.\n",
    "\n",
    "### dirR\n",
    "```\n",
    "diR:{$[11h=type d:key x;raze x,.z.s each` sv/:x,/:d;d]};\n",
    "```\n",
    "This function will recursively list the contents of a given directory x\n",
    "\n",
    "### nuke\n",
    "```\n",
    "nuke:hdel each desc diR@;\n",
    "```\n",
    "This function will delete all contents of the given directory and the given directory as well.\n",
    "\n",
    "### pdpft\n",
    "```\n",
    "pdpft:{[d;p;f;t] \n",
    "    i:iasc t f; \n",
    "    tab:.Q.en[d;`. t]; \n",
    "    .[{[d;t;i;c;a]@[d;c;:;a t[c]i]}[d:.Q.par[d;p;t];tab;i;;]]peach flip(c;)(::;`p#)f=c:cols t; \n",
    "    @[d;`.d;:;f,c where not f=c]; t \n",
    "};\n",
    "```\n",
    "This is a parallel version of [.Q.pdft](https://code.kx.com/q/ref/dotq/#dpft-save-table), will use available slave threads to save splayed table columns in parallel\n",
    "\n",
    "### saveTables\n",
    "```\n",
    "saveTables:{[db;path;d]\n",
    "    .aws.get_latest_sym_file[db;path];\n",
    "    t:tables`.;\n",
    "    t@:where `g=attr each t@\\:`Ticker;\n",
    "    {pdpft[hsym`$x;y;`Ticker;z]}[path;d] each tables`.;\n",
    "/    {.Q.dpft[hsym`$x;y;`Ticker;z]}[path;d] each tables`.;\n",
    "\n",
    "    dt:string d;\n",
    "    dict:flip`input_path`database_path`change_type!(\n",
    "        (`$path,dt;`$path,\"sym\");\n",
    "        (`$\"/\",dt,\"/\";`$\"/\");`PUT`PUT);\n",
    "    cid:.aws.create_changeset[db;dict];\n",
    "    \n",
    "    nuke hsym`$path,string[d];\n",
    "    hdel hsym`$path,\"sym\";\n",
    "\n",
    "    @[;`Ticker;`g#] each t;\n",
    "    .Q.gc[];\n",
    "\n",
    "    cid\n",
    "};\n",
    "```\n",
    "\n",
    "This function will save all in-memory tables in the global namespace with an index (g) [attribute](https://code.kx.com/q/ref/attr/). The function prepares for saving tables by first copying the database's (**db**) most recent sym file using [get_latest_sym_file](https://docs.aws.amazon.com/finspace/latest/userguide/interacting-with-kdb-q-apis.html) to **path**. A list of tables in the global namespace with index attribute is constructed and each of those tables saved using [.Q.dpft](https://code.kx.com/q/ref/dotq/#dpts-save-table-unsorted-with-symtable) to **path** that will also update the copied database sym file also in **path**. To add the changeset, an inventory of files for the changeset is put into the dictionary **dict** and the changeset created (added) to the database with the [.aws.create_changeset](https://docs.aws.amazon.com/finspace/latest/userguide/interacting-with-kdb-q-apis.html) function. To clean up, the sym file and date directory are then deleted and finally memory is cleaned up with [.Q.gz](https://code.kx.com/q/ref/dotq/#gz-gzip).\n",
    "\n",
    "Arguments:\n",
    "- db: name of database to update   \n",
    "- path: file path for saving tables   \n",
    "- d: date partition to create when saving   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "549ebbba-1412-4279-860a-d7c84bc6e0be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the full connection string\n",
    "conn_str = get_kx_connection_string(client, \n",
    "                                  environmentId=ENV_ID, clusterName=CLUSTER_NAME, \n",
    "                                   userName=KDB_USERNAME, boto_session=session)\n",
    "\n",
    "host, port, username, password = parse_connection_string(conn_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70db3213-19af-4b56-9caf-d88bcdf20b63",
   "metadata": {},
   "source": [
    "## Define Functions on Cluster\n",
    "Use the q magic to open an IPC connection the the cluster and send the functions used for processing the data files.\n",
    "\n",
    "### Functions Defined\n",
    "- parse_csvgz   \n",
    "- process_s3_csvgz    \n",
    "- diR   \n",
    "- nuke   \n",
    "- pdpft   \n",
    "- saveTables   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "de4ce80a-c60e-47dc-b91e-c09d7ff2b637",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%q --host $host --port $port --user $username --pass $password\n",
    "\\c 500 500 \n",
    "\n",
    "parse_csvgz:{[schema;file] (schema;enlist csv) 0: .Q.gz \"c\"$read1 hsym `$ string file};\n",
    "\n",
    "process_s3_csvgz:{[schema;work_dir;s3_object]\n",
    "    r:.aws.s3.get_object[s3_object;work_dir];\n",
    "    {t:parse_csvgz[x;`$y]; hdel hsym`$y; t}[schema] first r`containerFileDestinationPath\n",
    "    };\n",
    "\n",
    "diR:{$[11h=type d:key x;raze x,.z.s each` sv/:x,/:d;d]};\n",
    "nuke:hdel each desc diR@;\n",
    "\n",
    "pdpft:{[d;p;f;t] \n",
    "    i:iasc t f; \n",
    "    tab:.Q.en[d;`. t]; \n",
    "    .[{[d;t;i;c;a]@[d;c;:;a t[c]i]}[d:.Q.par[d;p;t];tab;i;;]]peach flip(c;)(::;`p#)f=c:cols t; \n",
    "    @[d;`.d;:;f,c where not f=c]; t \n",
    "    };\n",
    "\n",
    "saveTables:{[db;path;d]\n",
    "    .aws.get_latest_sym_file[db;path];\n",
    "    t:tables`.;\n",
    "    t@:where `g=attr each t@\\:`Ticker;\n",
    "    {pdpft[hsym`$x;y;`Ticker;z]}[path;d] each tables`.;\n",
    "/    {.Q.dpft[hsym`$x;y;`Ticker;z]}[path;d] each tables`.;\n",
    "\n",
    "    dt:string d;\n",
    "    dict:flip`input_path`database_path`change_type!(\n",
    "        (`$path,dt;`$path,\"sym\");\n",
    "        (`$\"/\",dt,\"/\";`$\"/\");`PUT`PUT);\n",
    "    cid:.aws.create_changeset[db;dict];\n",
    "    \n",
    "    nuke hsym`$path,string[d];\n",
    "    hdel hsym`$path,\"sym\";\n",
    "\n",
    "    @[;`Ticker;`g#] each t;\n",
    "    .Q.gc[];\n",
    "\n",
    "    cid\n",
    "    };"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fbec9dde-315e-4bf4-a0b8-f2b88a9a7218",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Date: 2021-01-04\n",
      "2021-01-04 S3 Objects: 4\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['s3://kdb-demo-829845998889-kms/algoseek-marketdata/us-equity-taq-faang/2021/20210104/A/AMZN.csv.gz']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['s3://kdb-demo-829845998889-kms/algoseek-marketdata/us-equity-taq-faang/2021/20210104/N/NFLX.csv.gz']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-08-13 16:25:59.581899: Processing chunk: 4 remaining: 4\n",
      "     taq: 8,970,726\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'After Ingest....'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>used</th>\n",
       "      <td>1008714288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>heap</th>\n",
       "      <td>1073741824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>peak</th>\n",
       "      <td>1073741824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wmax</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mmap</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mphy</th>\n",
       "      <td>133052764160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>syms</th>\n",
       "      <td>3933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>symw</th>\n",
       "      <td>198623</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "pykx.Dictionary(pykx.q('\n",
       "used| 1008714288\n",
       "heap| 1073741824\n",
       "peak| 1073741824\n",
       "wmax| 0\n",
       "mmap| 0\n",
       "mphy| 133052764160\n",
       "syms| 3933\n",
       "symw| 198623\n",
       "'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "2885681152"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "402653184"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tables and Counts\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>taq</th>\n",
       "      <td>8970726</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "pykx.Dictionary(pykx.q('taq| 8970726'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'Before Saving....'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>used</th>\n",
       "      <td>1059047648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>heap</th>\n",
       "      <td>1140850688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>peak</th>\n",
       "      <td>1543503872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wmax</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mmap</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mphy</th>\n",
       "      <td>133052764160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>syms</th>\n",
       "      <td>3933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>symw</th>\n",
       "      <td>198623</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "pykx.Dictionary(pykx.q('\n",
       "used| 1059047648\n",
       "heap| 1140850688\n",
       "peak| 1543503872\n",
       "wmax| 0\n",
       "mmap| 0\n",
       "mphy| 133052764160\n",
       "syms| 3933\n",
       "symw| 198623\n",
       "'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving changeset\n",
      "{'id': b'9MimR9zTlknSQvrQWoe3jg', 'status': b'PENDING'}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'New Changeset: 9MimR9zTlknSQvrQWoe3jg'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Status is IN_PROGRESS, total wait 0:00:00, waiting 10 sec ...\n",
      "Status is IN_PROGRESS, total wait 0:00:10, waiting 10 sec ...\n",
      "**Done**\n",
      "Elapsed Time for 2021-01-04: 0:00:35.250433\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'After Saving....'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>used</th>\n",
       "      <td>1059050560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>heap</th>\n",
       "      <td>1140850688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>peak</th>\n",
       "      <td>2013265920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wmax</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mmap</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mphy</th>\n",
       "      <td>133052764160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>syms</th>\n",
       "      <td>3978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>symw</th>\n",
       "      <td>202577</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "pykx.Dictionary(pykx.q('\n",
       "used| 1059050560\n",
       "heap| 1140850688\n",
       "peak| 2013265920\n",
       "wmax| 0\n",
       "mmap| 0\n",
       "mphy| 133052764160\n",
       "syms| 3978\n",
       "symw| 202577\n",
       "'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Date: 2021-01-05\n",
      "2021-01-05 S3 Objects: 4\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['s3://kdb-demo-829845998889-kms/algoseek-marketdata/us-equity-taq-faang/2021/20210105/A/AMZN.csv.gz']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['s3://kdb-demo-829845998889-kms/algoseek-marketdata/us-equity-taq-faang/2021/20210105/N/NFLX.csv.gz']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-08-13 16:26:34.772127: Processing chunk: 4 remaining: 4\n",
      "     taq: 6,174,244\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'After Ingest....'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>used</th>\n",
       "      <td>505402848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>heap</th>\n",
       "      <td>536870912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>peak</th>\n",
       "      <td>2013265920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wmax</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mmap</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mphy</th>\n",
       "      <td>133052764160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>syms</th>\n",
       "      <td>3988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>symw</th>\n",
       "      <td>203265</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "pykx.Dictionary(pykx.q('\n",
       "used| 505402848\n",
       "heap| 536870912\n",
       "peak| 2013265920\n",
       "wmax| 0\n",
       "mmap| 0\n",
       "mphy| 133052764160\n",
       "syms| 3988\n",
       "symw| 203265\n",
       "'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "1342177280"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "201326592"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tables and Counts\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>taq</th>\n",
       "      <td>6174244</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "pykx.Dictionary(pykx.q('taq| 6174244'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'Before Saving....'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>used</th>\n",
       "      <td>538958240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>heap</th>\n",
       "      <td>603979776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>peak</th>\n",
       "      <td>2013265920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wmax</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mmap</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mphy</th>\n",
       "      <td>133052764160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>syms</th>\n",
       "      <td>3988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>symw</th>\n",
       "      <td>203265</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "pykx.Dictionary(pykx.q('\n",
       "used| 538958240\n",
       "heap| 603979776\n",
       "peak| 2013265920\n",
       "wmax| 0\n",
       "mmap| 0\n",
       "mphy| 133052764160\n",
       "syms| 3988\n",
       "symw| 203265\n",
       "'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving changeset\n",
      "{'id': b'qsimSBjoiigHEVs3XptVXw', 'status': b'PENDING'}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'New Changeset: qsimSBjoiigHEVs3XptVXw'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Status is IN_PROGRESS, total wait 0:00:00, waiting 10 sec ...\n",
      "**Done**\n",
      "Elapsed Time for 2021-01-05: 0:00:20.144780\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'After Saving....'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>used</th>\n",
       "      <td>538959264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>heap</th>\n",
       "      <td>603979776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>peak</th>\n",
       "      <td>2013265920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wmax</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mmap</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mphy</th>\n",
       "      <td>133052764160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>syms</th>\n",
       "      <td>4020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>symw</th>\n",
       "      <td>206386</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "pykx.Dictionary(pykx.q('\n",
       "used| 538959264\n",
       "heap| 603979776\n",
       "peak| 2013265920\n",
       "wmax| 0\n",
       "mmap| 0\n",
       "mphy| 133052764160\n",
       "syms| 4020\n",
       "symw| 206386\n",
       "'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "gp = get_pykx_connection(client, \n",
    "                        environmentId=ENV_ID, clusterName=CLUSTER_NAME, \n",
    "                        userName=KDB_USERNAME, boto_session=session)\n",
    "\n",
    "# Number of files for a date to process at a time, will still process all files\n",
    "# Allows for monitoring process more easily when there are thousands of files for a date\n",
    "LEN_CHUNK=500\n",
    "\n",
    "for d in ALL_DATES:\n",
    "    print(f'Date: {d}')\n",
    "    s=datetime.datetime.now()\n",
    "\n",
    "    # search for S3 objects\n",
    "    s3_search=f'{S3_DATA_HOME}/{d.year}/{d.strftime(\"%Y%m%d\")}/*/*.csv.gz'\n",
    "\n",
    "    # Get list of S3 objects that will be processed into the table\n",
    "    slist = wr.s3.list_objects(s3_search)\n",
    "#    slist = slist[:300] ################################################################# DEBUGGING ONLY\n",
    "\n",
    "    # Number of files to be processed\n",
    "    print(f\"{d} S3 Objects: {len(slist)}\")\n",
    "    display(slist[:1])\n",
    "    print('...')\n",
    "    display(slist[-1:])\n",
    "    \n",
    "    num_files = len(slist)\n",
    "\n",
    "    # Continue if list is empty\n",
    "    if (num_files == 0):\n",
    "        print(f'List is empty for: {d}')\n",
    "        continue\n",
    "          \n",
    "    # send working directory to cluster, this is where each S3 object will be copied to before processing into a table\n",
    "    gp['wd'] = WORKING_DIR\n",
    "\n",
    "    # send first chunk of slist to cluster\n",
    "    this_chunk = slist[:LEN_CHUNK]\n",
    "    gp['slist'] = this_chunk\n",
    "\n",
    "    # process in chunks\n",
    "    print(f\"{datetime.datetime.now()}: Processing chunk: {len(this_chunk)} remaining: {len(slist)}\", flush=True)\n",
    "    \n",
    "    # Process the list of S3 files into one table (df) \n",
    "    gp('taq:raze{process_s3_csvgz [x; y; z]}[\"DNSSFJSS\";wd] peach slist')\n",
    "    \n",
    "    cnt = gp('count taq').py()\n",
    "    print(f\"     taq: {cnt:,}\")\n",
    "\n",
    "    # processed the first chunk, remove them\n",
    "    del slist[:LEN_CHUNK]\n",
    "    \n",
    "    # process remaining chunks\n",
    "    while len(slist) > 0:\n",
    "        # send chunk of slist to cluster\n",
    "        this_chunk = slist[:LEN_CHUNK]\n",
    "        gp['slist'] = this_chunk\n",
    "        \n",
    "        print(f\"{datetime.datetime.now()}: Processing chunk: {len(this_chunk)} remaining: {len(slist)}\", flush=True)\n",
    "\n",
    "        # Process the list of S3 files into one table (df) \n",
    "        gp('more_taq:raze{process_s3_csvgz [x; y; z]}[\"DNSSFJSS\";wd] peach slist')\n",
    "\n",
    "        # insert new values to taq\n",
    "        gp('`taq insert more_taq')\n",
    "        \n",
    "        # delete more_taq\n",
    "        gp('delete more_taq from `.')\n",
    "\n",
    "        # new length of taq\n",
    "        cnt = gp('count taq').py()\n",
    "        print(f\"     taq: {cnt:,}\")\n",
    "        \n",
    "        # garbage collect\n",
    "        gp('.Q.gc[]')\n",
    "        kx.q('.Q.gc[]')\n",
    "        \n",
    "        # delete LEN_CHUNK from slist\n",
    "        del slist[:LEN_CHUNK]\n",
    "        \n",
    "    print()\n",
    "\n",
    "    # display workspace after ingest\n",
    "    display(\"After Ingest....\")\n",
    "    display( gp(\".Q.w[]\") )\n",
    "\n",
    "    # Delete the Date column (this will be the partition as date in schema)\n",
    "    gp('delete Date from `taq')\n",
    "    display( gp('.Q.gc[]').py() )\n",
    "\n",
    "    # Group attribute on Ticker\n",
    "    gp('update `g#Ticker from `taq')\n",
    "    display( gp('.Q.gc[]').py() )\n",
    "\n",
    "    # tables we have now\n",
    "    print(\"Tables and Counts\", flush=True)\n",
    "    display( gp(\"tables[]!count each value each tables[]\") )\n",
    "\n",
    "    # display workspace before saving\n",
    "    display(\"Before Saving....\")\n",
    "    display( gp(\".Q.w[]\") )\n",
    "\n",
    "    # send date to cluster\n",
    "    print(\"Saving changeset\")\n",
    "    gp[\"dt\"] = d\n",
    "\n",
    "    # save tables and collect the changeset ID\n",
    "    cmd = f'cid:saveTables[\"{DB_NAME}\";\"{WORKING_DIR}/\";dt]'\n",
    "    gp(cmd)\n",
    "\n",
    "    print( gp('cid').py() ) \n",
    "    \n",
    "    # Newly created changset ID\n",
    "    changeset_id = str(gp(\"cid`id\"))\n",
    "    display( f'New Changeset: {changeset_id}' )\n",
    "\n",
    "    # Wait for the changeset to ingest\n",
    "    wait_for_changeset_status(get_client(), environmentId=ENV_ID, databaseName=DB_NAME, changesetId=changeset_id, show_wait=True)\n",
    "    print(\"**Done**\")\n",
    "    \n",
    "    e = datetime.datetime.now()\n",
    "    print(f\"Elapsed Time for {d}: {e - s}\")\n",
    "\n",
    "    cnt = gp(\"count taq\").py()\n",
    "\n",
    "    # display workspace after saving\n",
    "    display(\"After Saving....\")\n",
    "    display( gp(\".Q.w[]\") )\n",
    "\n",
    "    # CLEAN UP ---------------------------------\n",
    "    # delete table\n",
    "    gp('delete taq from `.')\n",
    "    gp('.Q.gc[]')\n",
    "    kx.q('.Q.gc[]')\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f23816e7-0b6b-4ca0-b2f9-3f5dd91a4a32",
   "metadata": {},
   "source": [
    "# Update the HDB\n",
    "Now that the database has been populated with new data, update the HDB's view to reflect the latest changeset_id and query its contents to confirm the data from the CSVs are now in the tables of the database HDB is serving up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c9f88df6-765b-4319-b7bb-f375707077fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataview DEMO_DB_VIEW exists but needs updating, updating...\n"
     ]
    }
   ],
   "source": [
    "# get the list of changesets in the database\n",
    "c_set_list = list_kx_changesets(client, environmentId=ENV_ID, databaseName=DB_NAME)\n",
    "\n",
    "if len(c_set_list) != 0:\n",
    "    # sort by create time\n",
    "    c_set_list = sorted(c_set_list, key=lambda d: d['createdTimestamp']) \n",
    "    latest_changeset = c_set_list[-1]['changesetId']\n",
    "\n",
    "    # Check if dataview already exists and is set to the requested changeset_id\n",
    "    resp = get_kx_dataview(client=client, environmentId=ENV_ID, databaseName=DB_NAME, dataviewName=DBVIEW_NAME)\n",
    "\n",
    "    if resp is None:\n",
    "        resp = client.create_kx_dataview(\n",
    "            environmentId = ENV_ID, \n",
    "            databaseName=DB_NAME, \n",
    "            dataviewName=DBVIEW_NAME,\n",
    "            azMode='SINGLE',\n",
    "            availabilityZoneId=AZ_ID,\n",
    "            changesetId=latest_changeset, # latest changeset_id\n",
    "            segmentConfigurations=[\n",
    "                { \n",
    "                    'volumeName': VOLUME_NAME,\n",
    "                    'dbPaths': ['/*'],  # cache all of database\n",
    "    #                \"onDemand\": True,   # cache data onDemand (on read) else will ensure all is cached\n",
    "                }\n",
    "            ],\n",
    "    #        readWrite=True,\n",
    "            autoUpdate=False,\n",
    "            description = f'Dataview of database'\n",
    "        )\n",
    "    elif resp['changesetId'] != latest_changeset:\n",
    "        print(f\"Dataview {DBVIEW_NAME} exists but needs updating, updating...\")\n",
    "        resp = client.update_kx_dataview(environmentId=ENV_ID, \n",
    "            databaseName=DB_NAME, \n",
    "            dataviewName=DBVIEW_NAME, \n",
    "            changesetId=latest_changeset, \n",
    "            segmentConfigurations=[\n",
    "                {'dbPaths': ['/*'], 'volumeName': VOLUME_NAME}\n",
    "            ]\n",
    "        )\n",
    "    else:\n",
    "        print(f\"Dataview {DBVIEW_NAME} exists with current changeset: {latest_changeset}\")\n",
    "    \n",
    "else:\n",
    "    # no changesets, do NOT create view\n",
    "    print(f\"No changeset in database: {DB_NAME}, Dataview {DBVIEW_NAME} not created\")        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c561d456-d562-4135-9a33-b0a63863c3e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataview: DEMO_DB_VIEW status is UPDATING, total wait 0:00:00, waiting 30 sec ...\n",
      "Dataview: DEMO_DB_VIEW status is UPDATING, total wait 0:00:30, waiting 30 sec ...\n",
      "Dataview: DEMO_DB_VIEW status is UPDATING, total wait 0:01:00, waiting 30 sec ...\n",
      "Dataview: DEMO_DB_VIEW status is UPDATING, total wait 0:01:30, waiting 30 sec ...\n",
      "Dataview: DEMO_DB_VIEW status is UPDATING, total wait 0:02:00, waiting 30 sec ...\n",
      "Dataview: DEMO_DB_VIEW status is UPDATING, total wait 0:02:30, waiting 30 sec ...\n",
      "Dataview: DEMO_DB_VIEW status is UPDATING, total wait 0:03:00, waiting 30 sec ...\n",
      "Dataview: DEMO_DB_VIEW status is UPDATING, total wait 0:03:30, waiting 30 sec ...\n",
      "Dataview: DEMO_DB_VIEW status is UPDATING, total wait 0:04:00, waiting 30 sec ...\n",
      "Dataview: DEMO_DB_VIEW status is UPDATING, total wait 0:04:30, waiting 30 sec ...\n",
      "Dataview: DEMO_DB_VIEW status is UPDATING, total wait 0:05:00, waiting 30 sec ...\n",
      "Dataview: DEMO_DB_VIEW status is UPDATING, total wait 0:05:30, waiting 30 sec ...\n",
      "Dataview: DEMO_DB_VIEW status is UPDATING, total wait 0:06:00, waiting 30 sec ...\n",
      "Dataview: DEMO_DB_VIEW status is now ACTIVE, total wait 0:06:30\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'databaseName': 'DEMO_DB',\n",
       " 'dataviewName': 'DEMO_DB_VIEW',\n",
       " 'azMode': 'SINGLE',\n",
       " 'availabilityZoneId': 'use1-az6',\n",
       " 'changesetId': 'qsimSBjoiigHEVs3XptVXw',\n",
       " 'segmentConfigurations': [{'dbPaths': ['/*'],\n",
       "   'volumeName': 'DEMO_SHARED_VOLUME',\n",
       "   'onDemand': False}],\n",
       " 'activeVersions': [{'changesetId': 'qsimSBjoiigHEVs3XptVXw',\n",
       "   'segmentConfigurations': [{'dbPaths': ['/*'],\n",
       "     'volumeName': 'DEMO_SHARED_VOLUME',\n",
       "     'onDemand': False}],\n",
       "   'attachedClusters': [],\n",
       "   'createdTimestamp': datetime.datetime(2024, 8, 13, 16, 26, 56, 661000, tzinfo=tzlocal()),\n",
       "   'versionId': '2simSDGKpYgeMZHWdb7nBg'},\n",
       "  {'changesetId': 'bsimNHCWawQBAubqAbhyBA',\n",
       "   'segmentConfigurations': [{'dbPaths': ['/*'],\n",
       "     'volumeName': 'DEMO_SHARED_VOLUME',\n",
       "     'onDemand': False}],\n",
       "   'attachedClusters': ['demo_csv_cluster', 'demo_hdb_cluster'],\n",
       "   'createdTimestamp': datetime.datetime(2024, 8, 13, 15, 53, 10, 488000, tzinfo=tzlocal()),\n",
       "   'versionId': 'fMimOLwsU0PXXjOjWnq3yg'}],\n",
       " 'description': 'Dataview of database',\n",
       " 'autoUpdate': False,\n",
       " 'readWrite': False,\n",
       " 'environmentId': 'jlcenjvtkgzrdek2qqv7ic',\n",
       " 'createdTimestamp': datetime.datetime(2024, 8, 13, 15, 53, 10, 393000, tzinfo=tzlocal()),\n",
       " 'lastModifiedTimestamp': datetime.datetime(2024, 8, 13, 16, 33, 27, 820000, tzinfo=tzlocal()),\n",
       " 'status': 'ACTIVE'}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# wait for view to be ready\n",
    "wait_for_dataview_status(client=client, environmentId=ENV_ID, databaseName=DB_NAME, dataviewName=DBVIEW_NAME, show_wait=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "33593bdf-ffd1-4df8-a94f-a60fa2dd9548",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update the HDB Cluster to use updated view of database\n",
    "resp=client.update_kx_cluster_databases(environmentId=ENV_ID, \n",
    "    clusterName=HDB_CLUSTER_NAME, \n",
    "    databases=[\n",
    "        {'databaseName': DB_NAME, 'dataviewName': DBVIEW_NAME}\n",
    "    ],\n",
    "    deploymentConfiguration={\n",
    "        'deploymentStrategy': 'ROLLING'\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "eb15182b-f8e1-47ec-b7c1-84532b5cf304",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster: demo_hdb_cluster status is UPDATING, total wait 0:00:00, waiting 30 sec ...\n",
      "Cluster: demo_hdb_cluster status is UPDATING, total wait 0:00:30, waiting 30 sec ...\n",
      "Cluster: demo_hdb_cluster status is UPDATING, total wait 0:01:00, waiting 30 sec ...\n",
      "Cluster: demo_hdb_cluster status is UPDATING, total wait 0:01:30, waiting 30 sec ...\n",
      "Cluster: demo_hdb_cluster status is UPDATING, total wait 0:02:00, waiting 30 sec ...\n",
      "Cluster: demo_hdb_cluster status is UPDATING, total wait 0:02:30, waiting 30 sec ...\n",
      "Cluster: demo_hdb_cluster status is UPDATING, total wait 0:03:00, waiting 30 sec ...\n",
      "Cluster: demo_hdb_cluster status is UPDATING, total wait 0:03:30, waiting 30 sec ...\n",
      "Cluster: demo_hdb_cluster status is UPDATING, total wait 0:04:00, waiting 30 sec ...\n",
      "Cluster: demo_hdb_cluster status is UPDATING, total wait 0:04:30, waiting 30 sec ...\n",
      "Cluster: demo_hdb_cluster status is UPDATING, total wait 0:05:00, waiting 30 sec ...\n",
      "Cluster: demo_hdb_cluster status is UPDATING, total wait 0:05:30, waiting 30 sec ...\n",
      "Cluster: demo_hdb_cluster status is UPDATING, total wait 0:06:00, waiting 30 sec ...\n",
      "Cluster: demo_hdb_cluster status is UPDATING, total wait 0:06:30, waiting 30 sec ...\n",
      "Cluster: demo_hdb_cluster status is now RUNNING, total wait 0:07:00\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'status': 'RUNNING',\n",
       " 'clusterName': 'demo_hdb_cluster',\n",
       " 'clusterType': 'HDB',\n",
       " 'volumes': [{'volumeName': 'DEMO_SHARED_VOLUME', 'volumeType': 'NAS_1'}],\n",
       " 'databases': [{'databaseName': 'DEMO_DB',\n",
       "   'dataviewConfiguration': {'dataviewName': 'DEMO_DB_VIEW',\n",
       "    'dataviewVersionId': '2simSDGKpYgeMZHWdb7nBg',\n",
       "    'changesetId': 'qsimSBjoiigHEVs3XptVXw',\n",
       "    'segmentConfigurations': [{'dbPaths': ['/*'],\n",
       "      'volumeName': 'DEMO_SHARED_VOLUME',\n",
       "      'onDemand': False}]}}],\n",
       " 'clusterDescription': 'Created with create_all notebook',\n",
       " 'releaseLabel': '1.0',\n",
       " 'vpcConfiguration': {'vpcId': 'vpc-0fe2b9c50f3ad382f',\n",
       "  'securityGroupIds': ['sg-0c99f1cfb9c3c7fd9'],\n",
       "  'subnetIds': ['subnet-04052219ec25b062b'],\n",
       "  'ipAddressType': 'IP_V4'},\n",
       " 'commandLineArguments': [{'key': 's', 'value': '2'}],\n",
       " 'executionRole': 'arn:aws:iam::829845998889:role/kdb-all-user',\n",
       " 'lastModifiedTimestamp': datetime.datetime(2024, 8, 13, 16, 40, 34, 471000, tzinfo=tzlocal()),\n",
       " 'azMode': 'SINGLE',\n",
       " 'availabilityZoneId': 'use1-az6',\n",
       " 'createdTimestamp': datetime.datetime(2024, 8, 13, 15, 59, 57, 177000, tzinfo=tzlocal()),\n",
       " 'scalingGroupConfiguration': {'scalingGroupName': 'DEMO_SCALING_GROUP',\n",
       "  'memoryLimit': 32768,\n",
       "  'memoryReservation': 6,\n",
       "  'nodeCount': 3}}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Wait for the HDB cluster to update\n",
    "wait_for_cluster_status(client, environmentId=ENV_ID, clusterName=HDB_CLUSTER_NAME, show_wait=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3363563d-89a1-4342-ad8e-883911991184",
   "metadata": {},
   "source": [
    "# Query the HDB \n",
    "Show the new data by querying the tables in the HDB. The data has changed from the data that was just added to the database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3d8a5da6-9e14-4456-8fd8-de2285072962",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Tables and Counts\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>taq</th>\n",
       "      <td>15144970</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "pykx.Dictionary(pykx.q('taq| 15144970'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Table: taq\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>t</th>\n",
       "      <th>f</th>\n",
       "      <th>a</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>c</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <td>\"d\"</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ticker</th>\n",
       "      <td>\"s\"</td>\n",
       "      <td></td>\n",
       "      <td>p</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Timestamp</th>\n",
       "      <td>\"n\"</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>EventType</th>\n",
       "      <td>\"s\"</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Price</th>\n",
       "      <td>\"f\"</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Quantity</th>\n",
       "      <td>\"j\"</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Exchange</th>\n",
       "      <td>\"s\"</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Conditions</th>\n",
       "      <td>\"s\"</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "pykx.KeyedTable(pykx.q('\n",
       "c         | t f a\n",
       "----------| -----\n",
       "date      | d    \n",
       "Ticker    | s   p\n",
       "Timestamp | n    \n",
       "EventType | s    \n",
       "Price     | f    \n",
       "Quantity  | j    \n",
       "Exchange  | s    \n",
       "Conditions| s    \n",
       "'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>Ticker</th>\n",
       "      <th>Timestamp</th>\n",
       "      <th>EventType</th>\n",
       "      <th>Price</th>\n",
       "      <th>Quantity</th>\n",
       "      <th>Exchange</th>\n",
       "      <th>Conditions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "pykx.Table(pykx.q('\n",
       "date Ticker Timestamp EventType Price Quantity Exchange Conditions\n",
       "------------------------------------------------------------------\n",
       "'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>Ticker</th>\n",
       "      <th>Timestamp</th>\n",
       "      <th>EventType</th>\n",
       "      <th>Price</th>\n",
       "      <th>Quantity</th>\n",
       "      <th>Exchange</th>\n",
       "      <th>Conditions</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2021.01.05</td>\n",
       "      <td>AMZN</td>\n",
       "      <td>0D04:00:00.021680902</td>\n",
       "      <td>TRADE</td>\n",
       "      <td>3190.01</td>\n",
       "      <td>63</td>\n",
       "      <td>ARCA</td>\n",
       "      <td>80000401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2021.01.05</td>\n",
       "      <td>AMZN</td>\n",
       "      <td>0D04:00:00.023083159</td>\n",
       "      <td>QUOTE BID</td>\n",
       "      <td>2000f</td>\n",
       "      <td>400</td>\n",
       "      <td>ARCA</td>\n",
       "      <td>00000001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2021.01.05</td>\n",
       "      <td>AMZN</td>\n",
       "      <td>0D04:00:00.023083159</td>\n",
       "      <td>QUOTE ASK</td>\n",
       "      <td>0f</td>\n",
       "      <td>0</td>\n",
       "      <td>ARCA</td>\n",
       "      <td>00000001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2021.01.05</td>\n",
       "      <td>AMZN</td>\n",
       "      <td>0D04:00:00.023083159</td>\n",
       "      <td>QUOTE BID NB</td>\n",
       "      <td>2000f</td>\n",
       "      <td>400</td>\n",
       "      <td>ARCA</td>\n",
       "      <td>00000001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2021.01.05</td>\n",
       "      <td>AMZN</td>\n",
       "      <td>0D04:00:00.023121746</td>\n",
       "      <td>QUOTE BID</td>\n",
       "      <td>2600f</td>\n",
       "      <td>100</td>\n",
       "      <td>ARCA</td>\n",
       "      <td>00000001</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "pykx.Table(pykx.q('\n",
       "date       Ticker Timestamp            EventType    Price   Quantity Exchange Conditions\n",
       "----------------------------------------------------------------------------------------\n",
       "2021.01.05 AMZN   0D04:00:00.021680902 TRADE        3190.01 63       ARCA     80000401  \n",
       "2021.01.05 AMZN   0D04:00:00.023083159 QUOTE BID    2000    400      ARCA     00000001  \n",
       "2021.01.05 AMZN   0D04:00:00.023083159 QUOTE ASK    0       0        ARCA     00000001  \n",
       "2021.01.05 AMZN   0D04:00:00.023083159 QUOTE BID NB 2000    400      ARCA     00000001  \n",
       "2021.01.05 AMZN   0D04:00:00.023121746 QUOTE BID    2600    100      ARCA     00000001  \n",
       "'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rows</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2021.01.04</th>\n",
       "      <td>8970726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021.01.05</th>\n",
       "      <td>6174244</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "pykx.KeyedTable(pykx.q('\n",
       "date      | rows   \n",
       "----------| -------\n",
       "2021.01.04| 8970726\n",
       "2021.01.05| 6174244\n",
       "'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>rows</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th>Ticker</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"4\" valign=\"top\">2021.01.04</th>\n",
       "      <th>AMZN</th>\n",
       "      <td>2221268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FB</th>\n",
       "      <td>2913516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GOOG</th>\n",
       "      <td>2245605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NFLX</th>\n",
       "      <td>1590337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"4\" valign=\"top\">2021.01.05</th>\n",
       "      <th>AMZN</th>\n",
       "      <td>1685421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FB</th>\n",
       "      <td>1720887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GOOG</th>\n",
       "      <td>1550965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NFLX</th>\n",
       "      <td>1216971</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "pykx.KeyedTable(pykx.q('\n",
       "date       Ticker| rows   \n",
       "-----------------| -------\n",
       "2021.01.04 AMZN  | 2221268\n",
       "2021.01.04 FB    | 2913516\n",
       "2021.01.04 GOOG  | 2245605\n",
       "2021.01.04 NFLX  | 1590337\n",
       "2021.01.05 AMZN  | 1685421\n",
       "2021.01.05 FB    | 1720887\n",
       "2021.01.05 GOOG  | 1550965\n",
       "2021.01.05 NFLX  | 1216971\n",
       "'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Query the HDB for after state\n",
    "hdb = get_pykx_connection(client, \n",
    "                          environmentId=ENV_ID, clusterName=HDB_CLUSTER_NAME, \n",
    "                          userName=KDB_USERNAME, boto_session=session)\n",
    "\n",
    "tables = hdb(\"tables[]\").py()\n",
    "\n",
    "# inventory of tables in the database and rows in each\n",
    "print(80*'=')\n",
    "print(\"Tables and Counts\")\n",
    "display( hdb(\"tables[]!count each value each tables[]\") )\n",
    "\n",
    "# For each table: schema, and samples and counts\n",
    "for t in tables:\n",
    "    print(80*'=')\n",
    "    print (f'Table: {t}')\n",
    "    print(80*'-')\n",
    "    display( hdb(f\"meta {t}\") )\n",
    "    display( hdb(f\"select from {t} where date = min date, i<5\") )\n",
    "    display( hdb(f\"select from {t} where date = max date, i<5\") )\n",
    "    display( hdb(f\"select rows:count i by date from {t}\") )\n",
    "    display( hdb(f\"select rows:count i by date,Ticker from {t}\") )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2fff8ca5-ac6f-428a-a135-584f6b8d54e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last Run: 2024-08-13 16:40:43.019225\n"
     ]
    }
   ],
   "source": [
    "print( f\"Last Run: {datetime.datetime.now()}\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93cf8045-ae15-4b60-b323-835ee9e44005",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
