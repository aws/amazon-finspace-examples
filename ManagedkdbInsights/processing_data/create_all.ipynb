{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "28bea13b-67bd-4a0e-8eab-3b8ffd37259e",
   "metadata": {},
   "source": [
    "# CSV Processing: Create Resources\n",
    "\n",
    "This notebook will create all the needed infrastructure to demonstrate how to use FinSpace with Managed kdb Insights to process csv.gz files on S3 into a managed matabase using managed clusters.\n",
    "\n",
    "## Architecture\n",
    "![Architecture](images/csv_arch.png \"Architecture\")\n",
    "\n",
    "A managed database is created and initially populated with an empty database by creating an initial changeset with an empty table. The empty database files are staged to an S3 bucket that FinSpace has access to and function [create_kx_changeset](https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/finspace/client/create_kx_changeset.html) is used to populate the managed database with the changset. Once the database has been populated, a shared volume and scaling group (infrastructure components) are created using [create_kx_volume](https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/finspace/client/create_kx_volume.html) and [create_kx_scaling_group](https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/finspace/client/create_kx_scaling_group.html). With the volume created, a dataview of the (still empty) database is created with [create_kx_dataview](https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/finspace/client/create_kx_dataview.html). Finally two clusters are created with [create_kx_cluster](https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/finspace/client/create_kx_cluster.html), a general purpose cluster (GP) for processing the csv.gz files, and an historical database (HDB) to serve up the data for query.\n",
    "\n",
    "## Processing Data\n",
    "This notebook sets up the necessary infrastructure to process data, please see the [process_algoseek](process_algoseek.ipynb) notebook for how this is done.\n",
    "\n",
    "## Clean Up\n",
    "To delete the infrastucture, please run the [delete_all](delete_all.ipynb) notebook\n",
    "\n",
    "## Algoseek LLC Data\n",
    "Trade and Quote data has been provided by [AlgoSeek LLC](https://www.algoseek.com/), you can learn more about their data offerings from their home page.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a58bc1d5-b390-441a-98db-4063029cceff",
   "metadata": {},
   "source": [
    "## Imports and Constants\n",
    "Import necessary python libraries and define global variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0d5f1d4a-ed45-44e3-bf75-9bdb75fcddbb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "import boto3\n",
    "import json\n",
    "import datetime\n",
    "\n",
    "import pykx as kx\n",
    "\n",
    "from managed_kx import *\n",
    "from env import *\n",
    "\n",
    "from config import *\n",
    "\n",
    "# ----------------------------------------------------------------\n",
    "\n",
    "NODE_TYPE=\"kx.sg.xlarge\"\n",
    "\n",
    "DATABASE_CONFIG=[{ \n",
    "    'databaseName': DB_NAME,\n",
    "    'dataviewName': DBVIEW_NAME\n",
    "    }]\n",
    "CODE_CONFIG={ 's3Bucket': S3_BUCKET, 's3Key': f'{S3_CODE_PATH}/{CODEBASE}.zip' }\n",
    "\n",
    "NAS1_CONFIG= {\n",
    "        'type': 'SSD_250',\n",
    "        'size': 1200\n",
    "}\n",
    "\n",
    "CMD_ARGS=[\n",
    "    { 'key': 's', 'value': '2' }, \n",
    "    { 'key': 'dbname', 'value': DB_NAME}, \n",
    "    { 'key': 'AWS_ZIP_DEFAULT', 'value': '17,2,6' },\n",
    "]\n",
    "\n",
    "HDB_CMD_ARGS=[\n",
    "    { 'key': 's', 'value': '2' }, \n",
    "    { 'key': 'dbname', 'value': DB_NAME}, \n",
    "]\n",
    "\n",
    "VPC_CONFIG={ \n",
    "    'vpcId': VPC_ID,\n",
    "    'securityGroupIds': SECURITY_GROUPS,\n",
    "    'subnetIds': SUBNET_IDS,\n",
    "    'ipAddressType': 'IP_V4' \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3cfe7d89-9f5d-4ceb-ac8c-1f5054a6f15a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Using credentials and create service client\n",
    "session = boto3.Session()\n",
    "\n",
    "# create finspace client\n",
    "client = session.client(service_name='finspace')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17c759c4-ee6c-45c5-a9f6-6acacea3a3be",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Create Managed Database\n",
    "Create a managed database in Managed kdb Insights using the API [create_kx_database](https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/finspace/client/delete_kx_database.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d55bd8d3-5629-46f9-bc1f-47bb0308dc0a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"createdTimestamp\": \"2024-11-26 19:16:09.825000+00:00\",\n",
      "    \"databaseArn\": \"arn:aws:finspace:us-east-1:829845998889:kxEnvironment/jlcenjvtkgzrdek2qqv7ic/kxDatabase/DEMO_DB\",\n",
      "    \"databaseName\": \"DEMO_DB\",\n",
      "    \"description\": \"Basictick kdb database\",\n",
      "    \"environmentId\": \"jlcenjvtkgzrdek2qqv7ic\",\n",
      "    \"lastCompletedChangesetId\": \"jMm0844KLNXdmbXIReAjEQ\",\n",
      "    \"lastModifiedTimestamp\": \"2024-11-26 19:16:47.496000+00:00\",\n",
      "    \"numBytes\": 24805,\n",
      "    \"numChangesets\": 1,\n",
      "    \"numFiles\": 11\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# assume database exists\n",
    "create_db=False\n",
    "\n",
    "try:\n",
    "    resp = client.get_kx_database(environmentId=ENV_ID, databaseName=DB_NAME)\n",
    "    resp.pop('ResponseMetadata', None)\n",
    "except:\n",
    "    # does not exist, will create\n",
    "    create_db=True\n",
    "\n",
    "if create_db:\n",
    "    print(f\"CREATING Database: {DB_NAME}\")\n",
    "    resp = client.create_kx_database(environmentId=ENV_ID, databaseName=DB_NAME, description=\"Basictick kdb database\")\n",
    "    resp.pop('ResponseMetadata', None)\n",
    "\n",
    "    print(f\"CREATED Database: {DB_NAME}\")\n",
    "\n",
    "print(json.dumps(resp,sort_keys=True,indent=4,default=str))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b4a873e-c9fa-4f63-8a3f-3a6aae9e59e0",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Create and Stage Data on S3\n",
    "## Create Empty Database\n",
    "With a local q instance, create initial in-memory table populated from a small csv file, then add that data to the managed database. Think of this as the first date of data in the database. We will ensure the table is initially empty.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "26172431-769c-49c9-91b0-728f9a81502f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!rm -rf $SOURCE_DATA_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ab1ab88d-3646-436b-a883-479d694cdf8c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "taq\n",
      "c            | t f a\n",
      "-------------| -----\n",
      "Timestamp    | n    \n",
      "EventType    | s    \n",
      "Ticker       | s   g\n",
      "Price        | f    \n",
      "Quantity     | j    \n",
      "Exchange     | s    \n",
      "Conditions   | s    \n",
      "FileName     | s    \n",
      "FileExtension| s    \n",
      "Timestamp EventType Ticker Price Quantity Exchange Conditions FileName FileEx..\n",
      "-----------------------------------------------------------------------------..\n"
     ]
    }
   ],
   "source": [
    "%%q\n",
    "/ create empty table\n",
    "taq:([]\n",
    "    Timestamp:`timespan$();\n",
    "    EventType:`symbol$();\n",
    "    Ticker:`symbol$();\n",
    "    Price:`float$();\n",
    "    Quantity:`long$();\n",
    "    Exchange:`symbol$();\n",
    "    Conditions:`symbol$();\n",
    "    FileName:`symbol$();\n",
    "    FileExtension:`symbol$() )\n",
    "\n",
    "/ set attribute on table\n",
    "update `g#Ticker from `taq\n",
    "\n",
    "/ Schema\n",
    "meta taq\n",
    "\n",
    "/ show the table contents (its empty)\n",
    "taq\n",
    "\n",
    "/ Save the table locally to a date partition\n",
    "d:2021.01.01;\n",
    "path:\"demo\";\n",
    "\n",
    "{.Q.dpft[hsym`$x;y;`Ticker;z]}[path;d] each tables`.;\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "287da0a7-ede6-4f5f-b33f-cb25b510f7d4",
   "metadata": {},
   "source": [
    "### Contents of Database on Disk\n",
    "Show the saved tables of database on the file system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dc6d3026-e582-4d3a-9dde-96297dc65ffd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "demo:\n",
      "total 8\n",
      "drwxrwxr-x 3 ec2-user ec2-user 4096 Nov 26 19:18 2021.01.01\n",
      "-rw-rw-r-- 1 ec2-user ec2-user    8 Nov 26 19:18 sym\n",
      "\n",
      "demo/2021.01.01:\n",
      "total 4\n",
      "drwxrwxr-x 2 ec2-user ec2-user 4096 Nov 26 19:18 taq\n",
      "\n",
      "demo/2021.01.01/taq:\n",
      "total 40\n",
      "-rw-rw-r-- 1 ec2-user ec2-user 4096 Nov 26 19:18 Conditions\n",
      "-rw-rw-r-- 1 ec2-user ec2-user 4096 Nov 26 19:18 EventType\n",
      "-rw-rw-r-- 1 ec2-user ec2-user 4096 Nov 26 19:18 Exchange\n",
      "-rw-rw-r-- 1 ec2-user ec2-user 4096 Nov 26 19:18 FileExtension\n",
      "-rw-rw-r-- 1 ec2-user ec2-user 4096 Nov 26 19:18 FileName\n",
      "-rw-rw-r-- 1 ec2-user ec2-user   16 Nov 26 19:18 Price\n",
      "-rw-rw-r-- 1 ec2-user ec2-user   16 Nov 26 19:18 Quantity\n",
      "-rw-rw-r-- 1 ec2-user ec2-user 4176 Nov 26 19:18 Ticker\n",
      "-rw-rw-r-- 1 ec2-user ec2-user   16 Nov 26 19:18 Timestamp\n"
     ]
    }
   ],
   "source": [
    "!ls -lR $SOURCE_DATA_DIR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93de4967-ac0f-47c1-b7a7-3be7bc312f0b",
   "metadata": {},
   "source": [
    "### Stage the Database on S3\n",
    "Copy the database files to S3 using the AWS CLI command [aws s3 cp](https://docs.aws.amazon.com/cli/latest/reference/s3/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ff3c0b91-92b5-48aa-aa6b-321b34bcf48c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                           PRE 2021.01.01/\n",
      "2024-11-26 19:18:09          8 sym\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Stage the local hdb database to S3\n",
    "S3_DEST=f\"s3://{S3_BUCKET}/{S3_DATA_PATH}/{SOURCE_DATA_DIR}/\"\n",
    "\n",
    "cp = \"\"\n",
    "\n",
    "if AWS_ACCESS_KEY_ID is not None:\n",
    "    cp = f\"\"\"\n",
    "export AWS_ACCESS_KEY_ID={AWS_ACCESS_KEY_ID} --quiet\n",
    "export AWS_SECRET_ACCESS_KEY={AWS_SECRET_ACCESS_KEY}\n",
    "export AWS_SESSION_TOKEN={AWS_SESSION_TOKEN}\n",
    "\"\"\"\n",
    "\n",
    "cp += f\"\"\"\n",
    "aws s3 rm --recursive {S3_DEST} --quiet\n",
    "aws s3 sync --exclude .DS_Store {SOURCE_DATA_DIR} {S3_DEST} --quiet\n",
    "aws s3 ls {S3_DEST}\n",
    "\"\"\"\n",
    "\n",
    "# execute the S3 copy\n",
    "os.system(cp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a4b0caa-cd47-4a9a-bef1-4f229d74a063",
   "metadata": {},
   "source": [
    "## Add Data to Database\n",
    "Add the disk data to the managed database using the API [create_kx_changeset](https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/finspace/client/create_kx_changeset.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "28decb39-f1e1-460d-873e-ddc30c431320",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# add the changeset if database has no changeset\n",
    "c_set_list = list_kx_changesets(client, environmentId=ENV_ID, databaseName=DB_NAME)\n",
    "\n",
    "if len(c_set_list) == 0:\n",
    "    print(\"Adding Changeset to Empty database\")\n",
    "    changes=[]\n",
    "\n",
    "    dir_list = os.listdir(f\"{SOURCE_DATA_DIR}\")\n",
    "\n",
    "    for f in dir_list:\n",
    "        if os.path.isdir(f\"{SOURCE_DATA_DIR}/{f}\"):\n",
    "            changes.append( { 'changeType': 'PUT', 's3Path': f\"{S3_DEST}{f}/\", 'dbPath': f\"/{f}/\" } )\n",
    "        else:\n",
    "            changes.append( { 'changeType': 'PUT', 's3Path': f\"{S3_DEST}{f}\", 'dbPath': f\"/\" } )\n",
    "\n",
    "    if len(dir_list) == 0:\n",
    "        changes.append( { 'changeType': 'PUT', 's3Path': f\"{S3_DEST}\", 'dbPath': f\"/\" } )\n",
    "\n",
    "    resp = client.create_kx_changeset(environmentId=ENV_ID, databaseName=DB_NAME, \n",
    "        changeRequests=changes)\n",
    "\n",
    "    resp.pop('ResponseMetadata', None)\n",
    "    changeset_id = resp['changesetId']\n",
    "\n",
    "    print(\"Changeset...\")\n",
    "    print(json.dumps(resp,sort_keys=True,indent=4,default=str))\n",
    "else:\n",
    "    changeset_id = c_set_list[0]['changesetId']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b3444ecf-25d5-4c43-902f-93caddb60370",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**Done**\n"
     ]
    }
   ],
   "source": [
    "wait_for_changeset_status(client, environmentId=ENV_ID, databaseName=DB_NAME, changesetId=changeset_id, show_wait=True)\n",
    "print(\"**Done**\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2be424f-9c29-41a1-bb97-1f2b11e13286",
   "metadata": {},
   "source": [
    "# Managed Database Changesets\n",
    "List the changesets for the Managed database using [list_kx_changesets](https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/finspace/client/list_kx_changesets.html) and get details of each changeset with [get_kx_changeset](https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/finspace/client/get_kx_changeset.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "81ac6ec6-efd1-4349-bd84-8d41637eb5c9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n",
      "Database: DEMO_DB, Changesets: 1 \n",
      "====================================================================================================\n",
      "  Changeset: jMm0844KLNXdmbXIReAjEQ: Created: 2024-11-26 19:16:13.975000+00:00 (COMPLETED)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_5b84d\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th id=\"T_5b84d_level0_col0\" class=\"col_heading level0 col0\" >changeType</th>\n",
       "      <th id=\"T_5b84d_level0_col1\" class=\"col_heading level0 col1\" >s3Path</th>\n",
       "      <th id=\"T_5b84d_level0_col2\" class=\"col_heading level0 col2\" >dbPath</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td id=\"T_5b84d_row0_col0\" class=\"data row0 col0\" >PUT</td>\n",
       "      <td id=\"T_5b84d_row0_col1\" class=\"data row0 col1\" >s3://kdb-demo-829845998889-kms/data/demo/2021.01.01/</td>\n",
       "      <td id=\"T_5b84d_row0_col2\" class=\"data row0 col2\" >/2021.01.01/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_5b84d_row1_col0\" class=\"data row1 col0\" >PUT</td>\n",
       "      <td id=\"T_5b84d_row1_col1\" class=\"data row1 col1\" >s3://kdb-demo-829845998889-kms/data/demo/sym</td>\n",
       "      <td id=\"T_5b84d_row1_col2\" class=\"data row1 col2\" >/</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7fb676aab3a0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "note_str = \"\"\n",
    "\n",
    "c_set_list = list_kx_changesets(client, environmentId=ENV_ID, databaseName=DB_NAME)\n",
    "\n",
    "if len(c_set_list) == 0:\n",
    "    note_str = \"<<No changesets>>\"\n",
    "    \n",
    "print(100*\"=\")\n",
    "print(f\"Database: {DB_NAME}, Changesets: {len(c_set_list)} {note_str}\")\n",
    "print(100*\"=\")\n",
    "\n",
    "# sort by create time\n",
    "c_set_list = sorted(c_set_list, key=lambda d: d['createdTimestamp']) \n",
    "\n",
    "for c in c_set_list:\n",
    "    c_set_id = c['changesetId']\n",
    "    print(f\"  Changeset: {c_set_id}: Created: {c['createdTimestamp']} ({c['status']})\")\n",
    "    c_rqs = client.get_kx_changeset(environmentId=ENV_ID, databaseName=DB_NAME, changesetId=c_set_id)['changeRequests']\n",
    "\n",
    "    chs_pdf = pd.DataFrame.from_dict(c_rqs).style.hide(axis='index')\n",
    "    display(chs_pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dae0232-3666-491f-8891-dae30e12c9d8",
   "metadata": {},
   "source": [
    "# Create Scaling Group\n",
    "The scaling group represents the total compute avilable to the application. All clusters will be placed into the scaling group ans share the compute and memory of the scaling group. Create the scaling group with the API [create_kx_scaling_group](https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/finspace/client/create_kx_scaling_group.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "613be7f8-fb82-4415-b30c-186ed470dba4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scaling Group DEMO_SCALING_GROUP exists\n"
     ]
    }
   ],
   "source": [
    "# Check if scaling group exits, only create if it does not\n",
    "resp = get_kx_scaling_group(client=client, environmentId=ENV_ID, scalingGroupName=SCALING_GROUP_NAME)\n",
    "\n",
    "if resp is None:\n",
    "    resp = client.create_kx_scaling_group(\n",
    "        environmentId = ENV_ID, \n",
    "        scalingGroupName = SCALING_GROUP_NAME,\n",
    "        hostType=NODE_TYPE,\n",
    "        availabilityZoneId = AZ_ID\n",
    "    )\n",
    "else:\n",
    "    print(f\"Scaling Group {SCALING_GROUP_NAME} exists\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6943fb16-8989-4199-a0fd-5c7c0d5aa56e",
   "metadata": {},
   "source": [
    "# Create Shared Volume\n",
    "The shared volume is a common storage device for the application. Every cluster using the shared volume will have a writable directory named after the cluster, can read the directories named after other clusters in the application using the volume. Create the volume with the API [create_kx_volume](https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/finspace/client/create_kx_volume.html). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e4a8a247-d029-4f9b-aaf5-c6e2ffe200a1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Volume DEMO_SHARED_VOLUME exists\n"
     ]
    }
   ],
   "source": [
    "# Check if volume already exists before trying to create one\n",
    "resp = get_kx_volume(client=client, environmentId=ENV_ID, volumeName=VOLUME_NAME)\n",
    "\n",
    "if resp is None:\n",
    "    resp = client.create_kx_volume(\n",
    "        environmentId = ENV_ID, \n",
    "        volumeType = 'NAS_1',\n",
    "        volumeName = VOLUME_NAME,\n",
    "        description = 'Shared volume between TP and RDB',\n",
    "        nas1Configuration = NAS1_CONFIG,\n",
    "        azMode='SINGLE',\n",
    "        availabilityZoneIds=[ AZ_ID ]    \n",
    "    )\n",
    "else:\n",
    "    print(f\"Volume {VOLUME_NAME} exists\")    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e718537-8853-4f21-8cc7-36b489e380c4",
   "metadata": {},
   "source": [
    "# Wait for Volume and Scaling Group\n",
    "Before proceeding to use Volumes and Scaling groups, wait for their creation to complete.\n",
    "\n",
    "Volume will be used by the dataview.    \n",
    "Dataview and Scaling Group will be used by the clusters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a3ed8931-e458-4ffb-83cc-0aa4da4d9f98",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scaling Group: DEMO_SCALING_GROUP status is now ACTIVE, total wait 0:00:00\n",
      "** Scaling Group DONE **\n",
      "Volume: DEMO_SHARED_VOLUME status is now ACTIVE, total wait 0:00:00\n",
      "** Volume DONE **\n"
     ]
    }
   ],
   "source": [
    "# wait for the scaling group to create\n",
    "wait_for_scaling_group_status(client=client, environmentId=ENV_ID, scalingGroupName=SCALING_GROUP_NAME, show_wait=True)\n",
    "print(\"** Scaling Group DONE **\")\n",
    "\n",
    "# wait for the volume to create\n",
    "wait_for_volume_status(client=client, environmentId=ENV_ID, volumeName=VOLUME_NAME, show_wait=True)\n",
    "print(\"** Volume DONE **\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe41eaeb-9c8e-44d3-b8bc-f354142f9140",
   "metadata": {},
   "source": [
    "# Create Dataview\n",
    "Create a dataview, for a specific (static) version of the database and have all of its data cached using the shared volume. Create the view with the API [create_kx_dataview](https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/finspace/client/create_kx_dataview.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "03434316-4ccc-420d-adee-715e6eb1bcd6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataview DEMO_DB_VIEW exists with current changeset: jMm0844KLNXdmbXIReAjEQ\n"
     ]
    }
   ],
   "source": [
    "# do changesets exist?\n",
    "c_set_list = list_kx_changesets(client, environmentId=ENV_ID, databaseName=DB_NAME)\n",
    "\n",
    "if len(c_set_list) != 0:\n",
    "    # sort by create time\n",
    "    c_set_list = sorted(c_set_list, key=lambda d: d['createdTimestamp']) \n",
    "    latest_changeset = c_set_list[-1]['changesetId']\n",
    "\n",
    "    # Check if dataview already exists and is set to the requested changeset_id\n",
    "    resp = get_kx_dataview(client=client, environmentId=ENV_ID, databaseName=DB_NAME, dataviewName=DBVIEW_NAME)\n",
    "\n",
    "    if resp is None:\n",
    "        resp = client.create_kx_dataview(\n",
    "            environmentId = ENV_ID, \n",
    "            databaseName=DB_NAME, \n",
    "            dataviewName=DBVIEW_NAME,\n",
    "            azMode='SINGLE',\n",
    "            availabilityZoneId=AZ_ID,\n",
    "            changesetId=latest_changeset, # latest changeset_id\n",
    "            segmentConfigurations=[\n",
    "                { \n",
    "                    'volumeName': VOLUME_NAME,\n",
    "                    'dbPaths': ['/*'],  # cache all of database\n",
    "                }\n",
    "            ],\n",
    "            autoUpdate=False,\n",
    "            description = f'Dataview of database'\n",
    "        )\n",
    "    elif resp['changesetId'] != latest_changeset:\n",
    "        print(f\"Dataview {DBVIEW_NAME} exists but needs updating...\")\n",
    "        resp = client.update_kx_dataview(environmentId=ENV_ID, \n",
    "            databaseName=DB_NAME, \n",
    "            dataviewName=DBVIEW_NAME, \n",
    "            changesetId=latest_changeset, \n",
    "            segmentConfigurations=[\n",
    "                {'dbPaths': ['/*'], 'volumeName': VOLUME_NAME}\n",
    "            ]\n",
    "        )\n",
    "    else:\n",
    "        print(f\"Dataview {DBVIEW_NAME} exists with current changeset: {latest_changeset}\")\n",
    "\n",
    "else:\n",
    "    # no changesets, do NOT create view\n",
    "    print(f\"No changeset in database: {DB_NAME}, Dataview {DBVIEW_NAME} not created\")        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "734ab7da-ef78-4701-9a58-1eeacbd9c557",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataview: DEMO_DB_VIEW status is CREATING, total wait 0:00:00, waiting 30 sec ...\n",
      "Dataview: DEMO_DB_VIEW status is CREATING, total wait 0:00:30, waiting 30 sec ...\n",
      "Dataview: DEMO_DB_VIEW status is CREATING, total wait 0:01:00, waiting 30 sec ...\n",
      "Dataview: DEMO_DB_VIEW status is CREATING, total wait 0:01:30, waiting 30 sec ...\n",
      "Dataview: DEMO_DB_VIEW status is CREATING, total wait 0:02:00, waiting 30 sec ...\n",
      "Dataview: DEMO_DB_VIEW status is CREATING, total wait 0:02:30, waiting 30 sec ...\n",
      "Dataview: DEMO_DB_VIEW status is CREATING, total wait 0:03:00, waiting 30 sec ...\n",
      "Dataview: DEMO_DB_VIEW status is CREATING, total wait 0:03:30, waiting 30 sec ...\n",
      "Dataview: DEMO_DB_VIEW status is CREATING, total wait 0:04:00, waiting 30 sec ...\n",
      "Dataview: DEMO_DB_VIEW status is CREATING, total wait 0:04:30, waiting 30 sec ...\n",
      "Dataview: DEMO_DB_VIEW status is now ACTIVE, total wait 0:05:00\n",
      "** Dataview DONE **\n"
     ]
    }
   ],
   "source": [
    "# wait for the view to create\n",
    "wait_for_dataview_status(client=client, environmentId=ENV_ID, databaseName=DB_NAME, dataviewName=DBVIEW_NAME, show_wait=True)\n",
    "print(\"** Dataview DONE **\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dea431b0-c501-46bb-b72a-a5eb80a335b0",
   "metadata": {},
   "source": [
    "# Create Clusters\n",
    "With foundation resources now completed, create the needed clusters for the application. GP clsuter will be used for processing CVS files, the HDB cluster will serve up the contents of the database. \n",
    "\n",
    "Clusters are created using the API [create_kx_cluster](https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/finspace/client/create_kx_cluster.html). The existance of a clusters is determined with [get_kx_cluster](https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/finspace/client/get_kx_cluster.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5180ffb5-4ca1-4191-a115-8b24b74878c8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating: demo_csv_cluster\n"
     ]
    }
   ],
   "source": [
    "# does cluster already exist?\n",
    "resp = get_kx_cluster(client, environmentId=ENV_ID, clusterName=CLUSTER_NAME)\n",
    "\n",
    "if resp is not None:\n",
    "    print(f\"Cluster: {CLUSTER_NAME} already exists\")\n",
    "else:\n",
    "    print(f\"Creating: {CLUSTER_NAME}\")\n",
    "\n",
    "    resp = client.create_kx_cluster(\n",
    "        environmentId=ENV_ID, \n",
    "        clusterName=CLUSTER_NAME,\n",
    "        clusterType=\"GP\",\n",
    "        releaseLabel = '1.0',\n",
    "        executionRole=EXECUTION_ROLE,\n",
    "        databases=DATABASE_CONFIG,\n",
    "        scalingGroupConfiguration={\n",
    "            'memoryReservation': 6,\n",
    "            'nodeCount': 1,\n",
    "            'scalingGroupName': SCALING_GROUP_NAME,\n",
    "        },\n",
    "        savedownStorageConfiguration = { 'volumeName': VOLUME_NAME },\n",
    "        clusterDescription=\"Created with create_all notebook\",\n",
    "        commandLineArguments=CMD_ARGS,\n",
    "        azMode=AZ_MODE,\n",
    "        availabilityZoneId=AZ_ID,\n",
    "        vpcConfiguration=VPC_CONFIG\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4f22533f-708c-4e41-99f6-41c9e4781256",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating: demo_hdb_cluster\n"
     ]
    }
   ],
   "source": [
    "# cluster already exists\n",
    "resp = get_kx_cluster(client, environmentId=ENV_ID, clusterName=HDB_CLUSTER_NAME)\n",
    "\n",
    "if resp is not None:\n",
    "    print(f\"Cluster: {HDB_CLUSTER_NAME} already exists\")\n",
    "else:\n",
    "    print(f\"Creating: {HDB_CLUSTER_NAME}\")\n",
    "\n",
    "    resp = client.create_kx_cluster(\n",
    "        environmentId=ENV_ID, \n",
    "        clusterName=HDB_CLUSTER_NAME,\n",
    "        clusterType='HDB',\n",
    "        releaseLabel = '1.0',\n",
    "        executionRole=EXECUTION_ROLE,\n",
    "        databases=DATABASE_CONFIG,\n",
    "        scalingGroupConfiguration={\n",
    "            'memoryLimit': 32*1024,\n",
    "            'memoryReservation': 6,\n",
    "            'nodeCount': 3,\n",
    "            'scalingGroupName': SCALING_GROUP_NAME,\n",
    "        },\n",
    "        clusterDescription=\"Created with create_all notebook\",\n",
    "        commandLineArguments=HDB_CMD_ARGS,\n",
    "        azMode=AZ_MODE,\n",
    "        availabilityZoneId=AZ_ID,\n",
    "        vpcConfiguration=VPC_CONFIG\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8491391-8c01-4190-a2bd-23fa888bf781",
   "metadata": {},
   "source": [
    "## Wait for all clusters to finish creating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0946ca26-c4b0-410d-ade5-18a47bf2318a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster: demo_csv_cluster status is PENDING, total wait 0:00:00, waiting 30 sec ...\n",
      "Cluster: demo_csv_cluster status is CREATING, total wait 0:00:30, waiting 30 sec ...\n",
      "Cluster: demo_csv_cluster status is CREATING, total wait 0:01:00, waiting 30 sec ...\n",
      "Cluster: demo_csv_cluster status is CREATING, total wait 0:01:30, waiting 30 sec ...\n",
      "Cluster: demo_csv_cluster status is CREATING, total wait 0:02:00, waiting 30 sec ...\n",
      "Cluster: demo_csv_cluster status is CREATING, total wait 0:02:30, waiting 30 sec ...\n",
      "Cluster: demo_csv_cluster status is CREATING, total wait 0:03:00, waiting 30 sec ...\n",
      "Cluster: demo_csv_cluster status is CREATING, total wait 0:03:30, waiting 30 sec ...\n",
      "Cluster: demo_csv_cluster status is CREATING, total wait 0:04:00, waiting 30 sec ...\n",
      "Cluster: demo_csv_cluster status is CREATING, total wait 0:04:30, waiting 30 sec ...\n",
      "Cluster: demo_csv_cluster status is CREATING, total wait 0:05:00, waiting 30 sec ...\n",
      "Cluster: demo_csv_cluster status is CREATING, total wait 0:05:30, waiting 30 sec ...\n",
      "Cluster: demo_csv_cluster status is CREATING, total wait 0:06:00, waiting 30 sec ...\n",
      "Cluster: demo_csv_cluster status is CREATING, total wait 0:06:30, waiting 30 sec ...\n",
      "Cluster: demo_csv_cluster status is CREATING, total wait 0:07:00, waiting 30 sec ...\n",
      "Cluster: demo_csv_cluster status is CREATING, total wait 0:07:30, waiting 30 sec ...\n",
      "Cluster: demo_csv_cluster status is CREATING, total wait 0:08:00, waiting 30 sec ...\n",
      "Cluster: demo_csv_cluster status is CREATING, total wait 0:08:30, waiting 30 sec ...\n",
      "Cluster: demo_csv_cluster status is CREATING, total wait 0:09:00, waiting 30 sec ...\n",
      "Cluster: demo_csv_cluster status is CREATING, total wait 0:09:30, waiting 30 sec ...\n",
      "Cluster: demo_csv_cluster status is CREATING, total wait 0:10:00, waiting 30 sec ...\n",
      "Cluster: demo_csv_cluster status is CREATING, total wait 0:10:30, waiting 30 sec ...\n",
      "Cluster: demo_csv_cluster status is CREATING, total wait 0:11:00, waiting 30 sec ...\n",
      "Cluster: demo_csv_cluster status is CREATING, total wait 0:11:30, waiting 30 sec ...\n",
      "Cluster: demo_csv_cluster status is CREATING, total wait 0:12:00, waiting 30 sec ...\n",
      "Cluster: demo_csv_cluster status is CREATING, total wait 0:12:30, waiting 30 sec ...\n",
      "Cluster: demo_csv_cluster status is CREATING, total wait 0:13:00, waiting 30 sec ...\n",
      "Cluster: demo_csv_cluster status is CREATING, total wait 0:13:30, waiting 30 sec ...\n",
      "Cluster: demo_csv_cluster status is CREATING, total wait 0:14:00, waiting 30 sec ...\n",
      "Cluster: demo_csv_cluster status is CREATING, total wait 0:14:30, waiting 30 sec ...\n",
      "Cluster: demo_csv_cluster status is now RUNNING, total wait 0:15:00\n",
      "Cluster: demo_hdb_cluster status is now RUNNING, total wait 0:00:00\n",
      "** ALL DONE **\n"
     ]
    }
   ],
   "source": [
    "# Wait for clusters to start\n",
    "for c in clusters:\n",
    "    wait_for_cluster_status(client, environmentId=ENV_ID, clusterName=c, show_wait=True)\n",
    "\n",
    "print(\"** ALL DONE **\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e91a23b-b100-4763-9c50-c819f5824202",
   "metadata": {
    "tags": []
   },
   "source": [
    "# All Processes Running\n",
    "All resources are now created, database has data, and the clusters are up and running.\n",
    "\n",
    "Now move onto the [process_algoseek](process_algoseek.ipynb) notebook to use this infrastructure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "86f33240-bb12-49f3-8d9c-5783c25eb182",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last Run: 2024-11-26 19:38:33.370397\n"
     ]
    }
   ],
   "source": [
    "print(f\"Last Run: {datetime.datetime.now()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81f05fa3-fbe0-443b-986e-428fb1ca4ae6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
