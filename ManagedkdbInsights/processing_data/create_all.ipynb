{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "28bea13b-67bd-4a0e-8eab-3b8ffd37259e",
   "metadata": {},
   "source": [
    "# CSV Processing: Create Resources\n",
    "\n",
    "This noptebook will create all the needed infrastructure to demonstrate how to use FinSpace with Managed kdb Insights to process csv.gz files on S3 into a managed matabase using managed clusters.\n",
    "\n",
    "## Architecture\n",
    "![Architecture](csv_arch.png \"Architecture\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a58bc1d5-b390-441a-98db-4063029cceff",
   "metadata": {},
   "source": [
    "## Imports and Constants\n",
    "Import necessary python libraries and define global variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0d5f1d4a-ed45-44e3-bf75-9bdb75fcddbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "import boto3\n",
    "import json\n",
    "import datetime\n",
    "\n",
    "import pykx as kx\n",
    "\n",
    "from managed_kx import *\n",
    "from env import *\n",
    "\n",
    "# ----------------------------------------------------------------\n",
    "DB_NAME=\"DEMO_DB\"\n",
    "DBVIEW_NAME=f\"{DB_NAME}_VIEW\"\n",
    "SCALING_GROUP_NAME=\"DEMO_SCALING_GROUP\"\n",
    "VOLUME_NAME=\"DEMO_SHARED_VOLUME\"\n",
    "CODEBASE=\"demo\"\n",
    "CLUSTER_NAME=\"demo_csv_cluster\"\n",
    "\n",
    "HDB_CLUSTER_NAME=\"demo_hdb_cluster\"\n",
    "\n",
    "# S3 Destinations\n",
    "S3_CODE_PATH=\"code\"\n",
    "S3_DATA_PATH=\"data\"\n",
    "SOURCE_DATA_DIR=\"demo\"\n",
    "\n",
    "# this file will seed the database (used for table schema)\n",
    "CSV_FILE='AMZN-100.csv.gz'\n",
    "# ----------------------------------------------------------------\n",
    "\n",
    "NODE_TYPE=\"kx.sg.4xlarge\"\n",
    "\n",
    "DATABASE_CONFIG=[{ \n",
    "    'databaseName': DB_NAME,\n",
    "    'dataviewName': DBVIEW_NAME\n",
    "    }]\n",
    "CODE_CONFIG={ 's3Bucket': S3_BUCKET, 's3Key': f'{S3_CODE_PATH}/{CODEBASE}.zip' }\n",
    "\n",
    "NAS1_CONFIG= {\n",
    "        'type': 'SSD_250',\n",
    "        'size': 1200\n",
    "}\n",
    "\n",
    "CMD_ARGS=[\n",
    "    { 'key': 's', 'value': '4' }, \n",
    "    { 'key': 'dbname', 'value': DB_NAME}, \n",
    "    { 'key': 'AWS_ZIP_DEFAULT', 'value': '17,2,6' },\n",
    "]\n",
    "\n",
    "HDB_CMD_ARGS=[\n",
    "    { 'key': 's', 'value': '4' }, \n",
    "]\n",
    "\n",
    "# Local q instance\n",
    "gp = kx.q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3cfe7d89-9f5d-4ceb-ac8c-1f5054a6f15a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Defaults ...\n"
     ]
    }
   ],
   "source": [
    "# Get credentials and create service client\n",
    "session=None\n",
    "\n",
    "if AWS_ACCESS_KEY_ID is None:\n",
    "    print(\"Using Defaults ...\")\n",
    "    # create AWS session: using access variables\n",
    "    session = boto3.Session()\n",
    "else:\n",
    "    print(\"Using variables ...\")\n",
    "    session = boto3.Session(\n",
    "        aws_access_key_id=AWS_ACCESS_KEY_ID,\n",
    "        aws_secret_access_key=AWS_SECRET_ACCESS_KEY,\n",
    "        aws_session_token=AWS_SESSION_TOKEN\n",
    "    )\n",
    "\n",
    "# create finspace client\n",
    "client = session.client(service_name='finspace', endpoint_url=ENDPOINT_URL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17c759c4-ee6c-45c5-a9f6-6acacea3a3be",
   "metadata": {},
   "source": [
    "# Create Managed Database\n",
    "Create a managed database in Managed kdb Insights using the API [create_kx_database](https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/finspace/client/delete_kx_database.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d55bd8d3-5629-46f9-bc1f-47bb0308dc0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CREATING Database: DEMO_DB\n",
      "CREATED Database: DEMO_DB\n",
      "{\n",
      "    \"createdTimestamp\": \"2024-05-09 13:55:16.925000+00:00\",\n",
      "    \"databaseArn\": \"arn:aws:finspace:us-east-1:829845998889:kxEnvironment/jlcenjvtkgzrdek2qqv7ic/kxDatabase/DEMO_DB\",\n",
      "    \"databaseName\": \"DEMO_DB\",\n",
      "    \"description\": \"Basictick kdb database\",\n",
      "    \"environmentId\": \"jlcenjvtkgzrdek2qqv7ic\",\n",
      "    \"lastModifiedTimestamp\": \"2024-05-09 13:55:16.925000+00:00\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# assume it exists\n",
    "create_db=False\n",
    "\n",
    "try:\n",
    "    resp = client.get_kx_database(environmentId=ENV_ID, databaseName=DB_NAME)\n",
    "    resp.pop('ResponseMetadata', None)\n",
    "except:\n",
    "    # does not exist, will create\n",
    "    create_db=True\n",
    "\n",
    "if create_db:\n",
    "    print(f\"CREATING Database: {DB_NAME}\")\n",
    "    resp = client.create_kx_database(environmentId=ENV_ID, databaseName=DB_NAME, description=\"Basictick kdb database\")\n",
    "    resp.pop('ResponseMetadata', None)\n",
    "\n",
    "    print(f\"CREATED Database: {DB_NAME}\")\n",
    "\n",
    "print(json.dumps(resp,sort_keys=True,indent=4,default=str))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b4a873e-c9fa-4f63-8a3f-3a6aae9e59e0",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Create and Stage Data on S3\n",
    "## Create Empty Database\n",
    "With a local q instance, create initial in-memory table populated from a small csv file, then add that data to the managed database. Think of this as the first date of data in the database. We will ensure the table is initially empty.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "26172431-769c-49c9-91b0-728f9a81502f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bd558758-e04c-431a-a9f1-eba7819d3359",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>t</th>\n",
       "      <th>f</th>\n",
       "      <th>a</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>c</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Timestamp</th>\n",
       "      <td>\"n\"</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>EventType</th>\n",
       "      <td>\"s\"</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ticker</th>\n",
       "      <td>\"s\"</td>\n",
       "      <td></td>\n",
       "      <td>g</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Price</th>\n",
       "      <td>\"f\"</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Quantity</th>\n",
       "      <td>\"j\"</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Exchange</th>\n",
       "      <td>\"s\"</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Conditions</th>\n",
       "      <td>\"s\"</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "pykx.KeyedTable(pykx.q('\n",
       "c         | t f a\n",
       "----------| -----\n",
       "Timestamp | n    \n",
       "EventType | s    \n",
       "Ticker    | s   g\n",
       "Price     | f    \n",
       "Quantity  | j    \n",
       "Exchange  | s    \n",
       "Conditions| s    \n",
       "'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Timestamp</th>\n",
       "      <th>EventType</th>\n",
       "      <th>Ticker</th>\n",
       "      <th>Price</th>\n",
       "      <th>Quantity</th>\n",
       "      <th>Exchange</th>\n",
       "      <th>Conditions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [Timestamp, EventType, Ticker, Price, Quantity, Exchange, Conditions]\n",
       "Index: []"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# have the local q instance process the csv.gz file into a table\n",
    "gp(f'''\n",
    "taq:(\"DNSSFJSS\";enlist csv) 0: .Q.gz \"c\"$read1 hsym `$\"{CSV_FILE}\";\n",
    "''')\n",
    "\n",
    "# delete the Date column\n",
    "gp(\"delete Date from `taq\")\n",
    "\n",
    "# delete all rows from table\n",
    "gp(\"delete from `taq\")\n",
    "\n",
    "# set attribute on table\n",
    "gp(\"update `g#Ticker from `taq\")\n",
    "\n",
    "# schema \n",
    "display(gp('meta taq'))\n",
    "\n",
    "# table contents\n",
    "display(gp('taq').pd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b3f95af1-25b6-421d-92e4-21745ea9a4f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pykx.Identity(pykx.q('::'))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save the table in a date partition (Jan 1 2024)\n",
    "kx.q('''\n",
    "d:2024.01.01;\n",
    "path:\"demo\";\n",
    "\n",
    "{.Q.dpft[hsym`$x;y;`Ticker;z]}[path;d] each tables`.;\n",
    "''')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "287da0a7-ede6-4f5f-b33f-cb25b510f7d4",
   "metadata": {},
   "source": [
    "### Contents of Database on Disk\n",
    "Show the saved tables of database on the file system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dc6d3026-e582-4d3a-9dde-96297dc65ffd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024.01.01  sym\n"
     ]
    }
   ],
   "source": [
    "!ls demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1f99f0d8-9c8d-4a13-88b5-e4f2651ea40e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 36\n",
      "drwxrwxr-x 2 ec2-user ec2-user  127 May  9 13:55 .\n",
      "drwxrwxr-x 3 ec2-user ec2-user   17 May  9 13:55 ..\n",
      "-rw-rw-r-- 1 ec2-user ec2-user 4096 May  9 13:55 Conditions\n",
      "-rw-rw-r-- 1 ec2-user ec2-user   70 May  9 13:55 .d\n",
      "-rw-rw-r-- 1 ec2-user ec2-user 4096 May  9 13:55 EventType\n",
      "-rw-rw-r-- 1 ec2-user ec2-user 4096 May  9 13:55 Exchange\n",
      "-rw-rw-r-- 1 ec2-user ec2-user   16 May  9 13:55 Price\n",
      "-rw-rw-r-- 1 ec2-user ec2-user   16 May  9 13:55 Quantity\n",
      "-rw-rw-r-- 1 ec2-user ec2-user 4176 May  9 13:55 Ticker\n",
      "-rw-rw-r-- 1 ec2-user ec2-user   16 May  9 13:55 Timestamp\n"
     ]
    }
   ],
   "source": [
    "!ls -la demo/2024.01.01/*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93de4967-ac0f-47c1-b7a7-3be7bc312f0b",
   "metadata": {},
   "source": [
    "### Stage the Database on S3\n",
    "Copy the database files to S3 using the AWS CLI command [aws s3 cp](https://docs.aws.amazon.com/cli/latest/reference/s3/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ff3c0b91-92b5-48aa-aa6b-321b34bcf48c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                           PRE 2024.01.01/\n",
      "2024-05-09 13:55:19          8 sym\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Stage the local hdb database to S3\n",
    "S3_DEST=f\"s3://{S3_BUCKET}/{S3_DATA_PATH}/{SOURCE_DATA_DIR}/\"\n",
    "\n",
    "if AWS_ACCESS_KEY_ID is not None:\n",
    "    cp = f\"\"\"\n",
    "export AWS_ACCESS_KEY_ID={AWS_ACCESS_KEY_ID} --quiet\n",
    "export AWS_SECRET_ACCESS_KEY={AWS_SECRET_ACCESS_KEY}\n",
    "export AWS_SESSION_TOKEN={AWS_SESSION_TOKEN}\n",
    "\n",
    "aws s3 rm --recursive {S3_DEST} --quiet\n",
    "aws s3 sync --exclude .DS_Store {SOURCE_DATA_DIR} {S3_DEST} --quiet\n",
    "aws s3 ls {S3_DEST}\n",
    "\"\"\"\n",
    "else:\n",
    "    cp = f\"\"\"\n",
    "aws s3 rm --recursive {S3_DEST} --quiet\n",
    "aws s3 sync --exclude .DS_Store {SOURCE_DATA_DIR} {S3_DEST} --quiet\n",
    "aws s3 ls {S3_DEST}\n",
    "\"\"\"\n",
    "    \n",
    "# execute the S3 copy\n",
    "os.system(cp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a4b0caa-cd47-4a9a-bef1-4f229d74a063",
   "metadata": {},
   "source": [
    "## Add Data to Database\n",
    "Add the disk data to the managed database using the API [create_kx_changeset](https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/finspace/client/create_kx_changeset.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "28decb39-f1e1-460d-873e-ddc30c431320",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Changeset...\n",
      "{\n",
      "    \"changeRequests\": [\n",
      "        {\n",
      "            \"changeType\": \"PUT\",\n",
      "            \"dbPath\": \"/\",\n",
      "            \"s3Path\": \"s3://kdb-demo-829845998889-kms/data/demo/sym\"\n",
      "        },\n",
      "        {\n",
      "            \"changeType\": \"PUT\",\n",
      "            \"dbPath\": \"/2024.01.01/\",\n",
      "            \"s3Path\": \"s3://kdb-demo-829845998889-kms/data/demo/2024.01.01/\"\n",
      "        }\n",
      "    ],\n",
      "    \"changesetId\": \"MMeu0Yw31LcK9SXhXVYOXQ\",\n",
      "    \"createdTimestamp\": \"2024-05-09 13:55:20.816000+00:00\",\n",
      "    \"databaseName\": \"DEMO_DB\",\n",
      "    \"environmentId\": \"jlcenjvtkgzrdek2qqv7ic\",\n",
      "    \"lastModifiedTimestamp\": \"2024-05-09 13:55:20.816000+00:00\",\n",
      "    \"status\": \"PENDING\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "changes=[]\n",
    "\n",
    "dir_list = os.listdir(f\"{SOURCE_DATA_DIR}\")\n",
    "\n",
    "for f in dir_list:\n",
    "    if os.path.isdir(f\"{SOURCE_DATA_DIR}/{f}\"):\n",
    "        changes.append( { 'changeType': 'PUT', 's3Path': f\"{S3_DEST}{f}/\", 'dbPath': f\"/{f}/\" } )\n",
    "    else:\n",
    "        changes.append( { 'changeType': 'PUT', 's3Path': f\"{S3_DEST}{f}\", 'dbPath': f\"/\" } )\n",
    "\n",
    "if len(dir_list) == 0:\n",
    "    changes.append( { 'changeType': 'PUT', 's3Path': f\"{S3_DEST}\", 'dbPath': f\"/\" } )\n",
    "        \n",
    "resp = client.create_kx_changeset(environmentId=ENV_ID, databaseName=DB_NAME, \n",
    "    changeRequests=changes)\n",
    "\n",
    "resp.pop('ResponseMetadata', None)\n",
    "changeset_id = resp['changesetId']\n",
    "\n",
    "print(\"Changeset...\")\n",
    "print(json.dumps(resp,sort_keys=True,indent=4,default=str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b3444ecf-25d5-4c43-902f-93caddb60370",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Status is IN_PROGRESS, total wait 0:00:00, waiting 10 sec ...\n",
      "Status is IN_PROGRESS, total wait 0:00:10, waiting 10 sec ...\n",
      "Status is IN_PROGRESS, total wait 0:00:20, waiting 10 sec ...\n",
      "Status is IN_PROGRESS, total wait 0:00:30, waiting 10 sec ...\n",
      "Status is IN_PROGRESS, total wait 0:00:40, waiting 10 sec ...\n",
      "**Done**\n"
     ]
    }
   ],
   "source": [
    "wait_for_changeset_status(client, environmentId=ENV_ID, databaseName=DB_NAME, changesetId=changeset_id, show_wait=True)\n",
    "print(\"**Done**\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2be424f-9c29-41a1-bb97-1f2b11e13286",
   "metadata": {},
   "source": [
    "### Managed Database Changesets\n",
    "List the changesets for the Managed database using [list_kx_changesets](https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/finspace/client/list_kx_changesets.html) and get details of each changeset with [get_kx_changeset](https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/finspace/client/get_kx_changeset.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "81ac6ec6-efd1-4349-bd84-8d41637eb5c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n",
      "Database: DEMO_DB, Changesets: 1 \n",
      "====================================================================================================\n",
      "  Changeset: MMeu0Yw31LcK9SXhXVYOXQ: Created: 2024-05-09 13:55:20.816000+00:00 (COMPLETED)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_0bca6\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th id=\"T_0bca6_level0_col0\" class=\"col_heading level0 col0\" >changeType</th>\n",
       "      <th id=\"T_0bca6_level0_col1\" class=\"col_heading level0 col1\" >s3Path</th>\n",
       "      <th id=\"T_0bca6_level0_col2\" class=\"col_heading level0 col2\" >dbPath</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td id=\"T_0bca6_row0_col0\" class=\"data row0 col0\" >PUT</td>\n",
       "      <td id=\"T_0bca6_row0_col1\" class=\"data row0 col1\" >s3://kdb-demo-829845998889-kms/data/demo/sym</td>\n",
       "      <td id=\"T_0bca6_row0_col2\" class=\"data row0 col2\" >/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_0bca6_row1_col0\" class=\"data row1 col0\" >PUT</td>\n",
       "      <td id=\"T_0bca6_row1_col1\" class=\"data row1 col1\" >s3://kdb-demo-829845998889-kms/data/demo/2024.01.01/</td>\n",
       "      <td id=\"T_0bca6_row1_col2\" class=\"data row1 col2\" >/2024.01.01/</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7f9edc33d580>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "note_str = \"\"\n",
    "\n",
    "c_set_list = list_kx_changesets(client, environmentId=ENV_ID, databaseName=DB_NAME)\n",
    "\n",
    "if len(c_set_list) == 0:\n",
    "    note_str = \"<<No changesets>>\"\n",
    "    \n",
    "print(100*\"=\")\n",
    "print(f\"Database: {DB_NAME}, Changesets: {len(c_set_list)} {note_str}\")\n",
    "print(100*\"=\")\n",
    "\n",
    "# sort by create time\n",
    "c_set_list = sorted(c_set_list, key=lambda d: d['createdTimestamp']) \n",
    "\n",
    "for c in c_set_list:\n",
    "    c_set_id = c['changesetId']\n",
    "    print(f\"  Changeset: {c_set_id}: Created: {c['createdTimestamp']} ({c['status']})\")\n",
    "    c_rqs = client.get_kx_changeset(environmentId=ENV_ID, databaseName=DB_NAME, changesetId=c_set_id)['changeRequests']\n",
    "\n",
    "    chs_pdf = pd.DataFrame.from_dict(c_rqs).style.hide(axis='index')\n",
    "    display(chs_pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dae0232-3666-491f-8891-dae30e12c9d8",
   "metadata": {},
   "source": [
    "# Create Scaling Group\n",
    "The scaling group represents the total compute avilable to the application. All clusters will be placed into the scaling group ans share the compute and memory of the scaling group. Create the scaling group with the API [create_kx_scaling_group](https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/finspace/client/create_kx_scaling_group.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "613be7f8-fb82-4415-b30c-186ed470dba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if scaling group exits, only create if it does not\n",
    "resp = get_kx_scaling_group(client=client, environmentId=ENV_ID, scalingGroupName=SCALING_GROUP_NAME)\n",
    "\n",
    "if resp is None:\n",
    "    resp = client.create_kx_scaling_group(\n",
    "        environmentId = ENV_ID, \n",
    "        scalingGroupName = SCALING_GROUP_NAME,\n",
    "        hostType=NODE_TYPE,\n",
    "        availabilityZoneId = AZ_ID\n",
    "    )\n",
    "else:\n",
    "    print(f\"Scaling Group {SCALING_GROUP_NAME} exists\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6943fb16-8989-4199-a0fd-5c7c0d5aa56e",
   "metadata": {},
   "source": [
    "# Create Shared Volume\n",
    "The shared volume is a common storage device for the application. Every cluster using the shared volume will have a writable directory named after the cluster, can read the directories named after other clusters in the application using the volume. Create the volume with the API [create_kx_volume](https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/finspace/client/create_kx_volume.html). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e4a8a247-d029-4f9b-aaf5-c6e2ffe200a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if volume already exists before trying to create one\n",
    "resp = get_kx_volume(client=client, environmentId=ENV_ID, volumeName=VOLUME_NAME)\n",
    "\n",
    "if resp is None:\n",
    "    resp = client.create_kx_volume(\n",
    "        environmentId = ENV_ID, \n",
    "        volumeType = 'NAS_1',\n",
    "        volumeName = VOLUME_NAME,\n",
    "        description = 'Shared volume between TP and RDB',\n",
    "        nas1Configuration = NAS1_CONFIG,\n",
    "        azMode='SINGLE',\n",
    "        availabilityZoneIds=[ AZ_ID ]    \n",
    "    )\n",
    "else:\n",
    "    print(f\"Volume {VOLUME_NAME} exists\")    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e718537-8853-4f21-8cc7-36b489e380c4",
   "metadata": {},
   "source": [
    "# Wait for Volume and Scaling Group\n",
    "Before proceeding to use Volumes and Scaling groups, wait for their creation to complete.\n",
    "\n",
    "Volume will be used by the dataview.    \n",
    "Dataview and Scaling Group will be used by the clusters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a3ed8931-e458-4ffb-83cc-0aa4da4d9f98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scaling Group: DEMO_SCALING_GROUP status is CREATING, total wait 0:00:00, waiting 30 sec ...\n",
      "Scaling Group: DEMO_SCALING_GROUP status is CREATING, total wait 0:00:30, waiting 30 sec ...\n",
      "Scaling Group: DEMO_SCALING_GROUP status is CREATING, total wait 0:01:00, waiting 30 sec ...\n",
      "Scaling Group: DEMO_SCALING_GROUP status is CREATING, total wait 0:01:30, waiting 30 sec ...\n",
      "Scaling Group: DEMO_SCALING_GROUP status is CREATING, total wait 0:02:00, waiting 30 sec ...\n",
      "Scaling Group: DEMO_SCALING_GROUP status is CREATING, total wait 0:02:30, waiting 30 sec ...\n",
      "Scaling Group: DEMO_SCALING_GROUP status is CREATING, total wait 0:03:00, waiting 30 sec ...\n",
      "Scaling Group: DEMO_SCALING_GROUP status is CREATING, total wait 0:03:30, waiting 30 sec ...\n",
      "Scaling Group: DEMO_SCALING_GROUP status is CREATING, total wait 0:04:00, waiting 30 sec ...\n",
      "Scaling Group: DEMO_SCALING_GROUP status is CREATING, total wait 0:04:30, waiting 30 sec ...\n",
      "Scaling Group: DEMO_SCALING_GROUP status is now ACTIVE, total wait 0:05:00\n",
      "** DONE **\n",
      "Volume: DEMO_SHARED_VOLUME status is CREATING, total wait 0:00:00, waiting 30 sec ...\n",
      "Volume: DEMO_SHARED_VOLUME status is CREATING, total wait 0:00:30, waiting 30 sec ...\n",
      "Volume: DEMO_SHARED_VOLUME status is CREATING, total wait 0:01:00, waiting 30 sec ...\n",
      "Volume: DEMO_SHARED_VOLUME status is CREATING, total wait 0:01:30, waiting 30 sec ...\n",
      "Volume: DEMO_SHARED_VOLUME status is CREATING, total wait 0:02:00, waiting 30 sec ...\n",
      "Volume: DEMO_SHARED_VOLUME status is CREATING, total wait 0:02:30, waiting 30 sec ...\n",
      "Volume: DEMO_SHARED_VOLUME status is CREATING, total wait 0:03:00, waiting 30 sec ...\n",
      "Volume: DEMO_SHARED_VOLUME status is now ACTIVE, total wait 0:03:30\n",
      "** DONE **\n"
     ]
    }
   ],
   "source": [
    "# wait for the scaling group to create\n",
    "wait_for_scaling_group_status(client=client, environmentId=ENV_ID, scalingGroupName=SCALING_GROUP_NAME, show_wait=True)\n",
    "print(\"** DONE **\")\n",
    "\n",
    "# wait for the volume to create\n",
    "wait_for_volume_status(client=client, environmentId=ENV_ID, volumeName=VOLUME_NAME, show_wait=True)\n",
    "print(\"** DONE **\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe41eaeb-9c8e-44d3-b8bc-f354142f9140",
   "metadata": {},
   "source": [
    "# Create Dataview\n",
    "Create a dataview, for a specific (static) version of the database and have all of its data cached using the shared volume. Create the view with the API [create_kx_dataview](https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/finspace/client/create_kx_dataview.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "03434316-4ccc-420d-adee-715e6eb1bcd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# do changesets exist?\n",
    "c_set_list = list_kx_changesets(client, environmentId=ENV_ID, databaseName=DB_NAME)\n",
    "\n",
    "if len(c_set_list) != 0:\n",
    "    # sort by create time\n",
    "    c_set_list = sorted(c_set_list, key=lambda d: d['createdTimestamp']) \n",
    "    latest_changeset = c_set_list[-1]['changesetId']\n",
    "\n",
    "    # Check if dataview already exists and is set to the requested changeset_id\n",
    "    resp = get_kx_dataview(client=client, environmentId=ENV_ID, databaseName=DB_NAME, dataviewName=DBVIEW_NAME)\n",
    "\n",
    "    if resp is None:\n",
    "        resp = client.create_kx_dataview(\n",
    "            environmentId = ENV_ID, \n",
    "            databaseName=DB_NAME, \n",
    "            dataviewName=DBVIEW_NAME,\n",
    "            azMode='SINGLE',\n",
    "            availabilityZoneId=AZ_ID,\n",
    "            changesetId=latest_changeset, # latest changeset_id\n",
    "            segmentConfigurations=[\n",
    "                { \n",
    "                    'volumeName': VOLUME_NAME,\n",
    "                    'dbPaths': ['/*'],  # cache all of database\n",
    "    #                \"onDemand\": True,   # cache data onDemand (on read) else will ensure all is cached\n",
    "                }\n",
    "            ],\n",
    "    #        readWrite=True,\n",
    "            autoUpdate=False,\n",
    "            description = f'Dataview of database'\n",
    "        )\n",
    "    elif resp['changesetId'] != latest_changeset:\n",
    "        print(f\"Dataview {DBVIEW_NAME} exists but needs updating...\")\n",
    "        resp = client.update_kx_dataview(\n",
    "        )\n",
    "    else:\n",
    "        print(f\"Dataview {DBVIEW_NAME} exists with current changeset: {latest_changeset}\")\n",
    "    \n",
    "else:\n",
    "    # no changesets, do NOT create view\n",
    "    print(f\"No changeset in database: {DB_NAME}, Dataview {DBVIEW_NAME} not created\")        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "734ab7da-ef78-4701-9a58-1eeacbd9c557",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataview: DEMO_DB_VIEW status is CREATING, total wait 0:00:00, waiting 30 sec ...\n",
      "Dataview: DEMO_DB_VIEW status is CREATING, total wait 0:00:30, waiting 30 sec ...\n",
      "Dataview: DEMO_DB_VIEW status is CREATING, total wait 0:01:00, waiting 30 sec ...\n",
      "Dataview: DEMO_DB_VIEW status is CREATING, total wait 0:01:30, waiting 30 sec ...\n",
      "Dataview: DEMO_DB_VIEW status is CREATING, total wait 0:02:00, waiting 30 sec ...\n",
      "Dataview: DEMO_DB_VIEW status is CREATING, total wait 0:02:30, waiting 30 sec ...\n",
      "Dataview: DEMO_DB_VIEW status is CREATING, total wait 0:03:00, waiting 30 sec ...\n",
      "Dataview: DEMO_DB_VIEW status is CREATING, total wait 0:03:30, waiting 30 sec ...\n",
      "Dataview: DEMO_DB_VIEW status is CREATING, total wait 0:04:00, waiting 30 sec ...\n",
      "Dataview: DEMO_DB_VIEW status is CREATING, total wait 0:04:30, waiting 30 sec ...\n",
      "Dataview: DEMO_DB_VIEW status is CREATING, total wait 0:05:00, waiting 30 sec ...\n",
      "Dataview: DEMO_DB_VIEW status is CREATING, total wait 0:05:30, waiting 30 sec ...\n",
      "Dataview: DEMO_DB_VIEW status is CREATING, total wait 0:06:00, waiting 30 sec ...\n",
      "Dataview: DEMO_DB_VIEW status is now ACTIVE, total wait 0:06:30\n",
      "** DONE **\n"
     ]
    }
   ],
   "source": [
    "# wait for the view to create\n",
    "wait_for_dataview_status(client=client, environmentId=ENV_ID, databaseName=DB_NAME, dataviewName=DBVIEW_NAME, show_wait=True)\n",
    "print(\"** DONE **\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dea431b0-c501-46bb-b72a-a5eb80a335b0",
   "metadata": {},
   "source": [
    "# Create Clusters\n",
    "With foundation resources now completed, create the needed clusters for the application. GP clsuter will be used for processing CVS files, the HDB cluster will serve up the contents of the database. \n",
    "\n",
    "Clusters are created using the API [create_kx_cluster](https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/finspace/client/create_kx_cluster.html). The existance of a clusters is determined with [get_kx_cluster](https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/finspace/client/get_kx_cluster.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5180ffb5-4ca1-4191-a115-8b24b74878c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating: demo_csv_cluster\n"
     ]
    }
   ],
   "source": [
    "# does cluster already exist?\n",
    "resp = get_kx_cluster(client, environmentId=ENV_ID, clusterName=CLUSTER_NAME)\n",
    "\n",
    "if resp is not None:\n",
    "    print(f\"Cluster: {CLUSTER_NAME} already exists\")\n",
    "else:\n",
    "    print(f\"Creating: {CLUSTER_NAME}\")\n",
    "\n",
    "    resp = client.create_kx_cluster(\n",
    "        environmentId=ENV_ID, \n",
    "        clusterName=CLUSTER_NAME,\n",
    "        clusterType=\"GP\",\n",
    "        releaseLabel = '1.0',\n",
    "        executionRole=EXECUTION_ROLE,\n",
    "        databases=DATABASE_CONFIG,\n",
    "        scalingGroupConfiguration={\n",
    "            'memoryReservation': 6,\n",
    "            'nodeCount': 1,\n",
    "            'scalingGroupName': SCALING_GROUP_NAME,\n",
    "        },\n",
    "        savedownStorageConfiguration = { 'volumeName': VOLUME_NAME },\n",
    "        clusterDescription=\"Created with create_all notebook\",\n",
    "    #    code=CODE_CONFIG,\n",
    "    #    initializationScript=cluster_init,\n",
    "        commandLineArguments=CMD_ARGS,\n",
    "        azMode=AZ_MODE,\n",
    "        availabilityZoneId=AZ_ID,\n",
    "        vpcConfiguration={ \n",
    "            'vpcId': VPC_ID,\n",
    "            'securityGroupIds': SECURITY_GROUPS,\n",
    "            'subnetIds': SUBNET_IDS,\n",
    "            'ipAddressType': 'IP_V4' }\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4f22533f-708c-4e41-99f6-41c9e4781256",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating: demo_hdb_cluster\n"
     ]
    }
   ],
   "source": [
    "# cluster already exists\n",
    "resp = get_kx_cluster(client, environmentId=ENV_ID, clusterName=HDB_CLUSTER_NAME)\n",
    "if resp is not None:\n",
    "    print(f\"Cluster: {HDB_CLUSTER_NAME} already exists\")\n",
    "else:\n",
    "    print(f\"Creating: {HDB_CLUSTER_NAME}\")\n",
    "    \n",
    "    resp = client.create_kx_cluster(\n",
    "        environmentId=ENV_ID, \n",
    "        clusterName=HDB_CLUSTER_NAME,\n",
    "        clusterType='HDB',\n",
    "        releaseLabel = '1.0',\n",
    "        executionRole=EXECUTION_ROLE,\n",
    "        databases=DATABASE_CONFIG,\n",
    "        scalingGroupConfiguration={\n",
    "            'memoryLimit': 32*1024,\n",
    "            'memoryReservation': 6,\n",
    "            'nodeCount': 3,\n",
    "            'scalingGroupName': SCALING_GROUP_NAME,\n",
    "        },\n",
    "        clusterDescription=\"Created with create_all notebook\",\n",
    "    #    code=CODE_CONFIG,\n",
    "    #    initializationScript=HDB_INIT_SCRIPT,\n",
    "        commandLineArguments=HDB_CMD_ARGS,\n",
    "        azMode=AZ_MODE,\n",
    "        availabilityZoneId=AZ_ID,\n",
    "        vpcConfiguration={ \n",
    "            'vpcId': VPC_ID,\n",
    "            'securityGroupIds': SECURITY_GROUPS,\n",
    "            'subnetIds': SUBNET_IDS,\n",
    "            'ipAddressType': 'IP_V4' },\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8491391-8c01-4190-a2bd-23fa888bf781",
   "metadata": {},
   "source": [
    "## Wait for all clusters to finish creating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0946ca26-c4b0-410d-ade5-18a47bf2318a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster: demo_csv_cluster status is PENDING, total wait 0:00:00, waiting 30 sec ...\n",
      "Cluster: demo_csv_cluster status is CREATING, total wait 0:00:30, waiting 30 sec ...\n",
      "Cluster: demo_csv_cluster status is CREATING, total wait 0:01:00, waiting 30 sec ...\n",
      "Cluster: demo_csv_cluster status is CREATING, total wait 0:01:30, waiting 30 sec ...\n",
      "Cluster: demo_csv_cluster status is CREATING, total wait 0:02:00, waiting 30 sec ...\n",
      "Cluster: demo_csv_cluster status is CREATING, total wait 0:02:30, waiting 30 sec ...\n",
      "Cluster: demo_csv_cluster status is CREATING, total wait 0:03:00, waiting 30 sec ...\n",
      "Cluster: demo_csv_cluster status is CREATING, total wait 0:03:30, waiting 30 sec ...\n",
      "Cluster: demo_csv_cluster status is CREATING, total wait 0:04:00, waiting 30 sec ...\n",
      "Cluster: demo_csv_cluster status is CREATING, total wait 0:04:30, waiting 30 sec ...\n",
      "Cluster: demo_csv_cluster status is CREATING, total wait 0:05:00, waiting 30 sec ...\n",
      "Cluster: demo_csv_cluster status is CREATING, total wait 0:05:30, waiting 30 sec ...\n",
      "Cluster: demo_csv_cluster status is CREATING, total wait 0:06:00, waiting 30 sec ...\n",
      "Cluster: demo_csv_cluster status is CREATING, total wait 0:06:30, waiting 30 sec ...\n",
      "Cluster: demo_csv_cluster status is CREATING, total wait 0:07:00, waiting 30 sec ...\n",
      "Cluster: demo_csv_cluster status is CREATING, total wait 0:07:30, waiting 30 sec ...\n",
      "Cluster: demo_csv_cluster status is CREATING, total wait 0:08:00, waiting 30 sec ...\n",
      "Cluster: demo_csv_cluster status is CREATING, total wait 0:08:30, waiting 30 sec ...\n",
      "Cluster: demo_csv_cluster status is CREATING, total wait 0:09:00, waiting 30 sec ...\n",
      "Cluster: demo_csv_cluster status is CREATING, total wait 0:09:30, waiting 30 sec ...\n",
      "Cluster: demo_csv_cluster status is CREATING, total wait 0:10:00, waiting 30 sec ...\n",
      "Cluster: demo_csv_cluster status is CREATING, total wait 0:10:30, waiting 30 sec ...\n",
      "Cluster: demo_csv_cluster status is CREATING, total wait 0:11:00, waiting 30 sec ...\n",
      "Cluster: demo_csv_cluster status is CREATING, total wait 0:11:30, waiting 30 sec ...\n",
      "Cluster: demo_csv_cluster status is CREATING, total wait 0:12:00, waiting 30 sec ...\n",
      "Cluster: demo_csv_cluster status is CREATING, total wait 0:12:30, waiting 30 sec ...\n",
      "Cluster: demo_csv_cluster status is CREATING, total wait 0:13:00, waiting 30 sec ...\n",
      "Cluster: demo_csv_cluster status is CREATING, total wait 0:13:30, waiting 30 sec ...\n",
      "Cluster: demo_csv_cluster status is CREATING, total wait 0:14:00, waiting 30 sec ...\n",
      "Cluster: demo_csv_cluster status is CREATING, total wait 0:14:30, waiting 30 sec ...\n",
      "Cluster: demo_csv_cluster status is CREATING, total wait 0:15:00, waiting 30 sec ...\n",
      "Cluster: demo_csv_cluster status is CREATING, total wait 0:15:30, waiting 30 sec ...\n",
      "Cluster: demo_csv_cluster status is now RUNNING, total wait 0:16:00\n",
      "Cluster: demo_hdb_cluster status is now RUNNING, total wait 0:00:00\n",
      "** ALL DONE **\n"
     ]
    }
   ],
   "source": [
    "# Wait for clusters to start\n",
    "wait_for_cluster_status(client, environmentId=ENV_ID, clusterName=CLUSTER_NAME, show_wait=True)\n",
    "wait_for_cluster_status(client, environmentId=ENV_ID, clusterName=HDB_CLUSTER_NAME, show_wait=True)\n",
    "\n",
    "print(\"** ALL DONE **\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e91a23b-b100-4763-9c50-c819f5824202",
   "metadata": {
    "tags": []
   },
   "source": [
    "# All Processes Running\n",
    "All resources are now created, database has data, and the clusters are up and running."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "86f33240-bb12-49f3-8d9c-5783c25eb182",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last Run: 2024-05-09 14:28:00.145754\n"
     ]
    }
   ],
   "source": [
    "print( f\"Last Run: {datetime.datetime.now()}\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81f05fa3-fbe0-443b-986e-428fb1ca4ae6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
