{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "28bea13b-67bd-4a0e-8eab-3b8ffd37259e",
   "metadata": {},
   "source": [
    "# BasicTick V3: Create Everything\n",
    "This notebook will use the AWS Python boto3 APIs to create the needed resources for a basic tick application. This application will simulate a market data capture system. \n",
    "\n",
    "## Architecture\n",
    "<img src=\"images/Deepdive Diagrams-BasicTick V3.drawio.png\"  width=\"80%\">\n",
    "\n",
    "## Abbreviations\n",
    "- CEP: Complex Event Processor    \n",
    "- FH: Feedhandler    \n",
    "- HDB: Historical Dastabase\n",
    "- RDB: Realtime Database    \n",
    "- TP: Tickerplant    \n",
    "\n",
    "## AWS Resources Created\n",
    "- Database   \n",
    "- Changeset (adds data to database)   \n",
    "- Scaling Group in which all clusters are run   \n",
    "- Shared Volume used by database view and clusters  \n",
    "- Dataview of database on the shared volume\n",
    "  - option: view can be auto-updating or static\n",
    "- Clusters: TP, CEP, HDB, Gateway, and RDB   \n",
    "\n",
    "### Non AWS\n",
    "For this demonstration application the FH is run locally and publishes data to the TP.    \n",
    "\n",
    "# References"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0d5f1d4a-ed45-44e3-bf75-9bdb75fcddbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "import boto3\n",
    "import json\n",
    "import datetime\n",
    "\n",
    "import pykx as kx\n",
    "\n",
    "from env import *\n",
    "from managed_kx import *\n",
    "\n",
    "# Cluster names and database\n",
    "from basictick_setup import *\n",
    "\n",
    "# ----------------------------------------------------------------\n",
    "\n",
    "# Source data directory\n",
    "SOURCE_DATA_DIR=\"hdb\"\n",
    "\n",
    "# Code directory\n",
    "CODEBASE=\"basictick\"\n",
    "\n",
    "# S3 Destinations\n",
    "S3_CODE_PATH=\"code\"\n",
    "S3_DATA_PATH=\"data\"\n",
    "\n",
    "#NODE_TYPE=\"kx.sg.4xlarge\"\n",
    "NODE_TYPE=\"kx.sg.2xlarge\"\n",
    "\n",
    "CODE_CONFIG={ 's3Bucket': S3_BUCKET, 's3Key': f'{S3_CODE_PATH}/{CODEBASE}.zip' }\n",
    "\n",
    "NAS1_CONFIG= {\n",
    "        'type': 'SSD_250',\n",
    "        'size': 1200\n",
    "}\n",
    "\n",
    "# Realtime Database (RDB) Configs\n",
    "RDB_INIT_SCRIPT='rdbmkdb.q'\n",
    "RDB_CMD_ARGS=[\n",
    "    { 'key': 's', 'value': '2' }, \n",
    "    { 'key': 'g', 'value': '1' }, \n",
    "    { 'key': 'tp', 'value': TP_CLUSTER_NAME }, \n",
    "    { 'key': 'procName', 'value': RDB_CLUSTER_NAME }, \n",
    "    { 'key': 'volumeName', 'value': VOLUME_NAME }, \n",
    "    { 'key': 'hdbProc', 'value': HDB_CLUSTER_NAME }, \n",
    "    { 'key': 'dbView', 'value': DBVIEW_NAME }, \n",
    "    { 'key': 'AWS_ZIP_DEFAULT', 'value': '17,2,6' },\n",
    "]\n",
    "\n",
    "# CEP Configs\n",
    "CEP_INIT_SCRIPT='cepmkdb.q'\n",
    "CEP_CMD_ARGS = [\n",
    "    { 'key': 's', 'value': '2' }, \n",
    "    { 'key': 'g', 'value': '1' }, \n",
    "    { 'key': 'tp', 'value': TP_CLUSTER_NAME }, \n",
    "    { 'key': 'AWS_ZIP_DEFAULT', 'value': '17,2,6' },\n",
    "]\n",
    "\n",
    "# Tickerplant (TP) Configs\n",
    "TP_INIT_SCRIPT='tick.q'\n",
    "TP_CMD_ARGS=[\n",
    "    { 'key': 'procName', 'value': TP_CLUSTER_NAME }, \n",
    "    { 'key': 'volumeName', 'value': VOLUME_NAME }, \n",
    "    { 'key': 'AWS_ZIP_DEFAULT', 'value': '17,2,6' },\n",
    "    { 'key': 'g', 'value': '1' }, \n",
    "]\n",
    "\n",
    "# Historical Database (HDB) Configs\n",
    "HDB_INIT_SCRIPT='hdbmkdb.q'\n",
    "HDB_CMD_ARGS=[\n",
    "    { 'key': 's', 'value': '2' }, \n",
    "    { 'key': 'g', 'value': '1' }, \n",
    "]\n",
    "\n",
    "# Gateway Configs\n",
    "GW_INIT_SCRIPT='gwmkdb.q'\n",
    "GW_CMD_ARGS=[\n",
    "    { 'key': 's', 'value': '2' }, \n",
    "    { 'key': 'g', 'value': '1' }, \n",
    "    { 'key': 'rdb_name', 'value': RDB_CLUSTER_NAME}, \n",
    "    { 'key': 'hdb_name', 'value': HDB_CLUSTER_NAME}, \n",
    "]\n",
    "\n",
    "# Feedhandler configs\n",
    "FEED_TIMER=10000\n",
    "FH_PORT=5030 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3cfe7d89-9f5d-4ceb-ac8c-1f5054a6f15a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Defaults ...\n"
     ]
    }
   ],
   "source": [
    "# triggers credential get\n",
    "session=None\n",
    "\n",
    "if AWS_ACCESS_KEY_ID is None:\n",
    "    print(\"Using Defaults ...\")\n",
    "    # create AWS session: using access variables\n",
    "    session = boto3.Session()\n",
    "else:\n",
    "    print(\"Using variables ...\")\n",
    "    session = boto3.Session(\n",
    "        aws_access_key_id=AWS_ACCESS_KEY_ID,\n",
    "        aws_secret_access_key=AWS_SECRET_ACCESS_KEY,\n",
    "        aws_session_token=AWS_SESSION_TOKEN\n",
    "    )\n",
    "\n",
    "# create finspace client\n",
    "client = session.client(service_name='finspace', endpoint_url=ENDPOINT_URL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c3d4047-9583-4b09-b75d-98fd2ddd6c36",
   "metadata": {},
   "source": [
    "# Create a Sample Database\n",
    "Create a synthetic database using kxtaqdb.q (takes 1-2 minutes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a871f941-e506-4bc4-9922-7241ec9a2739",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Generated trade|quote records: 872530 4356637\"\n",
      "\"Generated trade|quote records: 899400 4495478\"\n",
      "\"Generated trade|quote records: 879672 4401306\"\n",
      "\"Generated trade|quote records: 894169 4471510\"\n",
      "\"Generated trade|quote records: 941313 4711942\"\n",
      "\"Generated trade|quote records: 924403 4619618\"\n",
      "\"Generated trade|quote records: 907938 4544274\"\n",
      "\"Generated trade|quote records: 867065 4333345\"\n",
      "438M\thdb\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "os.system(f\"rm -rf {SOURCE_DATA_DIR}\")\n",
    "\n",
    "# call local q (using pykx) to create the database\n",
    "kx.q(\"\\l basictick/kxtaqdb.q\")\n",
    "\n",
    "# Database size\n",
    "print( os.system(f\"du -sh {SOURCE_DATA_DIR}\") )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bf690f2-c465-4df8-90f5-1e3b808bb368",
   "metadata": {},
   "source": [
    "## Stage Database Files to S3\n",
    "Using AWS cli, copy hdb to staging S3 bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3ff35f7c-4c3c-47b2-9eaa-2197dbfdadd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Destination: s3://kdb-demo-829845998889-kms/data/hdb/\n",
      "                           PRE 2024.07.29/\n",
      "                           PRE 2024.07.30/\n",
      "                           PRE 2024.07.31/\n",
      "                           PRE 2024.08.01/\n",
      "                           PRE 2024.08.02/\n",
      "                           PRE 2024.08.05/\n",
      "                           PRE 2024.08.06/\n",
      "                           PRE 2024.08.07/\n",
      "2024-08-08 18:44:17         75 sym\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "S3_DEST=f\"s3://{S3_BUCKET}/{S3_DATA_PATH}/{SOURCE_DATA_DIR}/\"\n",
    "\n",
    "if AWS_ACCESS_KEY_ID is not None:\n",
    "    cp = f\"\"\"\n",
    "export AWS_ACCESS_KEY_ID={AWS_ACCESS_KEY_ID} --quiet\n",
    "export AWS_SECRET_ACCESS_KEY={AWS_SECRET_ACCESS_KEY}\n",
    "export AWS_SESSION_TOKEN={AWS_SESSION_TOKEN}\n",
    "\n",
    "aws s3 rm --recursive {S3_DEST} --quiet\n",
    "aws s3 sync --exclude .DS_Store {SOURCE_DATA_DIR} {S3_DEST} --quiet\n",
    "\"\"\"\n",
    "else:\n",
    "    cp = f\"\"\"\n",
    "aws s3 rm --recursive {S3_DEST} --quiet\n",
    "aws s3 sync --exclude .DS_Store {SOURCE_DATA_DIR} {S3_DEST} --quiet\n",
    "\"\"\"\n",
    "    \n",
    "# execute the S3 copy\n",
    "os.system(cp)\n",
    "\n",
    "print( f\"Destination: {S3_DEST}\" )\n",
    "print( os.system(f\"aws s3 ls {S3_DEST}\") )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17c759c4-ee6c-45c5-a9f6-6acacea3a3be",
   "metadata": {},
   "source": [
    "## Create A Managed Database\n",
    "Using the AWS APIs, create a managed database in Managed kdb Insights. The database is initially empty and is populated using changesets.\n",
    "\n",
    "### Reference\n",
    "[Managed kdb Insights databases](https://docs.aws.amazon.com/finspace/latest/userguide/finspace-managed-kdb-db.html)\n",
    "\n",
    "### APIs used\n",
    "[get_kx_database](https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/finspace/client/get_kx_database.html)  \n",
    "[create_kx_database](https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/finspace/client/create_kx_database.html)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d55bd8d3-5629-46f9-bc1f-47bb0308dc0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CREATING Database: basictickdb\n",
      "CREATED Database: basictickdb\n",
      "{\n",
      "    \"createdTimestamp\": \"2024-08-08 18:44:19.065000+00:00\",\n",
      "    \"databaseArn\": \"arn:aws:finspace:us-east-1:829845998889:kxEnvironment/jlcenjvtkgzrdek2qqv7ic/kxDatabase/basictickdb\",\n",
      "    \"databaseName\": \"basictickdb\",\n",
      "    \"description\": \"Basictick kdb database\",\n",
      "    \"environmentId\": \"jlcenjvtkgzrdek2qqv7ic\",\n",
      "    \"lastModifiedTimestamp\": \"2024-08-08 18:44:19.065000+00:00\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# assume it exists\n",
    "create_db=False\n",
    "\n",
    "try:\n",
    "    resp = client.get_kx_database(environmentId=ENV_ID, databaseName=DB_NAME)\n",
    "    resp.pop('ResponseMetadata', None)\n",
    "except:\n",
    "    # does not exist, will create\n",
    "    create_db=True\n",
    "\n",
    "if create_db:\n",
    "    print(f\"CREATING Database: {DB_NAME}\")\n",
    "    resp = client.create_kx_database(environmentId=ENV_ID, databaseName=DB_NAME, description=\"Basictick kdb database\")\n",
    "    resp.pop('ResponseMetadata', None)\n",
    "\n",
    "    print(f\"CREATED Database: {DB_NAME}\")\n",
    "\n",
    "print(json.dumps(resp,sort_keys=True,indent=4,default=str))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26d1194e-0c04-49a3-a7e7-a1d23fcff0d9",
   "metadata": {},
   "source": [
    "## Add Data to Database\n",
    "Add the created database data copied earlier to S3 to the created managed database using create_kx_changeset. \n",
    "\n",
    "### APIs used\n",
    "[create_kx_changeset](https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/finspace/client/create_kx_changeset.html)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eae61f04-1c9c-468e-bb38-b2e0b94897a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding Changeset to Empty database\n",
      "Changeset...\n",
      "{\n",
      "    \"changeRequests\": [\n",
      "        {\n",
      "            \"changeType\": \"PUT\",\n",
      "            \"dbPath\": \"/\",\n",
      "            \"s3Path\": \"s3://kdb-demo-829845998889-kms/data/hdb/sym\"\n",
      "        },\n",
      "        {\n",
      "            \"changeType\": \"PUT\",\n",
      "            \"dbPath\": \"/2024.07.29/\",\n",
      "            \"s3Path\": \"s3://kdb-demo-829845998889-kms/data/hdb/2024.07.29/\"\n",
      "        },\n",
      "        {\n",
      "            \"changeType\": \"PUT\",\n",
      "            \"dbPath\": \"/2024.07.30/\",\n",
      "            \"s3Path\": \"s3://kdb-demo-829845998889-kms/data/hdb/2024.07.30/\"\n",
      "        },\n",
      "        {\n",
      "            \"changeType\": \"PUT\",\n",
      "            \"dbPath\": \"/2024.07.31/\",\n",
      "            \"s3Path\": \"s3://kdb-demo-829845998889-kms/data/hdb/2024.07.31/\"\n",
      "        },\n",
      "        {\n",
      "            \"changeType\": \"PUT\",\n",
      "            \"dbPath\": \"/2024.08.01/\",\n",
      "            \"s3Path\": \"s3://kdb-demo-829845998889-kms/data/hdb/2024.08.01/\"\n",
      "        },\n",
      "        {\n",
      "            \"changeType\": \"PUT\",\n",
      "            \"dbPath\": \"/2024.08.02/\",\n",
      "            \"s3Path\": \"s3://kdb-demo-829845998889-kms/data/hdb/2024.08.02/\"\n",
      "        },\n",
      "        {\n",
      "            \"changeType\": \"PUT\",\n",
      "            \"dbPath\": \"/2024.08.05/\",\n",
      "            \"s3Path\": \"s3://kdb-demo-829845998889-kms/data/hdb/2024.08.05/\"\n",
      "        },\n",
      "        {\n",
      "            \"changeType\": \"PUT\",\n",
      "            \"dbPath\": \"/2024.08.06/\",\n",
      "            \"s3Path\": \"s3://kdb-demo-829845998889-kms/data/hdb/2024.08.06/\"\n",
      "        },\n",
      "        {\n",
      "            \"changeType\": \"PUT\",\n",
      "            \"dbPath\": \"/2024.08.07/\",\n",
      "            \"s3Path\": \"s3://kdb-demo-829845998889-kms/data/hdb/2024.08.07/\"\n",
      "        }\n",
      "    ],\n",
      "    \"changesetId\": \"4MiZpzH05M5DPUBfmFz59w\",\n",
      "    \"createdTimestamp\": \"2024-08-08 18:44:21.099000+00:00\",\n",
      "    \"databaseName\": \"basictickdb\",\n",
      "    \"environmentId\": \"jlcenjvtkgzrdek2qqv7ic\",\n",
      "    \"lastModifiedTimestamp\": \"2024-08-08 18:44:21.099000+00:00\",\n",
      "    \"status\": \"PENDING\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "c_set_list = list_kx_changesets(client, environmentId=ENV_ID, databaseName=DB_NAME)\n",
    "\n",
    "if len(c_set_list) == 0:\n",
    "    print(\"Adding Changeset to Empty database\")\n",
    "    changes=[]\n",
    "\n",
    "    for f in os.listdir(f\"{SOURCE_DATA_DIR}\"):\n",
    "        if os.path.isdir(f\"{SOURCE_DATA_DIR}/{f}\"):\n",
    "            changes.append( { 'changeType': 'PUT', 's3Path': f\"{S3_DEST}{f}/\", 'dbPath': f\"/{f}/\" } )\n",
    "        else:\n",
    "            changes.append( { 'changeType': 'PUT', 's3Path': f\"{S3_DEST}{f}\", 'dbPath': f\"/\" } )\n",
    "\n",
    "    resp = client.create_kx_changeset(environmentId=ENV_ID, databaseName=DB_NAME, \n",
    "        changeRequests=changes)\n",
    "\n",
    "    resp.pop('ResponseMetadata', None)\n",
    "    changeset_id = resp['changesetId']\n",
    "\n",
    "    print(\"Changeset...\")\n",
    "    print(json.dumps(resp,sort_keys=True,indent=4,default=str))\n",
    "else:\n",
    "    changeset_id = c_set_list[0]['changesetId']    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b4422bdd-7d44-4fb0-8018-0bebd6987704",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Status is IN_PROGRESS, total wait 0:00:00, waiting 10 sec ...\n",
      "Status is IN_PROGRESS, total wait 0:00:10, waiting 10 sec ...\n",
      "**Done**\n"
     ]
    }
   ],
   "source": [
    "# Wait for the changeset to be added to the database\n",
    "wait_for_changeset_status(client, environmentId=ENV_ID, databaseName=DB_NAME, changesetId=changeset_id, show_wait=True)\n",
    "\n",
    "print(\"**Done**\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eb4a142-5dd1-4ee3-a746-852db203eb2f",
   "metadata": {},
   "source": [
    "### Contents of the Managed Database\n",
    "Display the changesets of the managed database.\n",
    "\n",
    "### APIs used\n",
    "[list_kx_changesets](https://botocore.amazonaws.com/v1/documentation/api/latest/reference/services/finspace/client/list_kx_changesets.html)   \n",
    "[get_kx_changeset](https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/finspace/client/get_kx_changeset.html)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8ba008f3-4991-474c-9b3e-43a1dca56257",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n",
      "Database: basictickdb, Changesets: 1 \n",
      "====================================================================================================\n",
      "  Changeset: 4MiZpzH05M5DPUBfmFz59w: Created: 2024-08-08 18:44:21.099000+00:00 (COMPLETED)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_b8f28\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th id=\"T_b8f28_level0_col0\" class=\"col_heading level0 col0\" >changeType</th>\n",
       "      <th id=\"T_b8f28_level0_col1\" class=\"col_heading level0 col1\" >s3Path</th>\n",
       "      <th id=\"T_b8f28_level0_col2\" class=\"col_heading level0 col2\" >dbPath</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td id=\"T_b8f28_row0_col0\" class=\"data row0 col0\" >PUT</td>\n",
       "      <td id=\"T_b8f28_row0_col1\" class=\"data row0 col1\" >s3://kdb-demo-829845998889-kms/data/hdb/sym</td>\n",
       "      <td id=\"T_b8f28_row0_col2\" class=\"data row0 col2\" >/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_b8f28_row1_col0\" class=\"data row1 col0\" >PUT</td>\n",
       "      <td id=\"T_b8f28_row1_col1\" class=\"data row1 col1\" >s3://kdb-demo-829845998889-kms/data/hdb/2024.07.29/</td>\n",
       "      <td id=\"T_b8f28_row1_col2\" class=\"data row1 col2\" >/2024.07.29/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_b8f28_row2_col0\" class=\"data row2 col0\" >PUT</td>\n",
       "      <td id=\"T_b8f28_row2_col1\" class=\"data row2 col1\" >s3://kdb-demo-829845998889-kms/data/hdb/2024.07.30/</td>\n",
       "      <td id=\"T_b8f28_row2_col2\" class=\"data row2 col2\" >/2024.07.30/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_b8f28_row3_col0\" class=\"data row3 col0\" >PUT</td>\n",
       "      <td id=\"T_b8f28_row3_col1\" class=\"data row3 col1\" >s3://kdb-demo-829845998889-kms/data/hdb/2024.07.31/</td>\n",
       "      <td id=\"T_b8f28_row3_col2\" class=\"data row3 col2\" >/2024.07.31/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_b8f28_row4_col0\" class=\"data row4 col0\" >PUT</td>\n",
       "      <td id=\"T_b8f28_row4_col1\" class=\"data row4 col1\" >s3://kdb-demo-829845998889-kms/data/hdb/2024.08.01/</td>\n",
       "      <td id=\"T_b8f28_row4_col2\" class=\"data row4 col2\" >/2024.08.01/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_b8f28_row5_col0\" class=\"data row5 col0\" >PUT</td>\n",
       "      <td id=\"T_b8f28_row5_col1\" class=\"data row5 col1\" >s3://kdb-demo-829845998889-kms/data/hdb/2024.08.02/</td>\n",
       "      <td id=\"T_b8f28_row5_col2\" class=\"data row5 col2\" >/2024.08.02/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_b8f28_row6_col0\" class=\"data row6 col0\" >PUT</td>\n",
       "      <td id=\"T_b8f28_row6_col1\" class=\"data row6 col1\" >s3://kdb-demo-829845998889-kms/data/hdb/2024.08.05/</td>\n",
       "      <td id=\"T_b8f28_row6_col2\" class=\"data row6 col2\" >/2024.08.05/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_b8f28_row7_col0\" class=\"data row7 col0\" >PUT</td>\n",
       "      <td id=\"T_b8f28_row7_col1\" class=\"data row7 col1\" >s3://kdb-demo-829845998889-kms/data/hdb/2024.08.06/</td>\n",
       "      <td id=\"T_b8f28_row7_col2\" class=\"data row7 col2\" >/2024.08.06/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td id=\"T_b8f28_row8_col0\" class=\"data row8 col0\" >PUT</td>\n",
       "      <td id=\"T_b8f28_row8_col1\" class=\"data row8 col1\" >s3://kdb-demo-829845998889-kms/data/hdb/2024.08.07/</td>\n",
       "      <td id=\"T_b8f28_row8_col2\" class=\"data row8 col2\" >/2024.08.07/</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7feac755ec40>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "note_str = \"\"\n",
    "\n",
    "c_set_list = list_kx_changesets(client, environmentId=ENV_ID, databaseName=DB_NAME)\n",
    "\n",
    "if len(c_set_list) == 0:\n",
    "    note_str = \"<<Could not get changesets>>\"\n",
    "    \n",
    "print(100*\"=\")\n",
    "print(f\"Database: {DB_NAME}, Changesets: {len(c_set_list)} {note_str}\")\n",
    "print(100*\"=\")\n",
    "\n",
    "# sort by create time\n",
    "c_set_list = sorted(c_set_list, key=lambda d: d['createdTimestamp']) \n",
    "\n",
    "for c in c_set_list:\n",
    "    c_set_id = c['changesetId']\n",
    "    print(f\"  Changeset: {c_set_id}: Created: {c['createdTimestamp']} ({c['status']})\")\n",
    "    c_rqs = client.get_kx_changeset(environmentId=ENV_ID, databaseName=DB_NAME, changesetId=c_set_id)['changeRequests']\n",
    "\n",
    "    chs_pdf = pd.DataFrame.from_dict(c_rqs).style.hide(axis='index')\n",
    "    display(chs_pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dae0232-3666-491f-8891-dae30e12c9d8",
   "metadata": {},
   "source": [
    "# Create Scaling Group\n",
    "The scaling group represents the total compute avilable to the application. All clusters will be placed into the scaling group and will share the compute and memory of the scaling group.\n",
    "\n",
    "## Reference\n",
    "[Managed kdb scaling groups](https://docs.aws.amazon.com/finspace/latest/userguide/finspace-managed-kdb-scaling-groups.html)\n",
    "\n",
    "## APIs used\n",
    "[create_kx_scaling_group](https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/finspace/client/create_kx_scaling_group.html)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "613be7f8-fb82-4415-b30c-186ed470dba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if scaling group exits, only create if it does not\n",
    "resp = get_kx_scaling_group(client=client, environmentId=ENV_ID, scalingGroupName=SCALING_GROUP_NAME)\n",
    "\n",
    "if resp is None:\n",
    "    resp = client.create_kx_scaling_group(\n",
    "        environmentId = ENV_ID, \n",
    "        scalingGroupName = SCALING_GROUP_NAME,\n",
    "        hostType=NODE_TYPE,\n",
    "        availabilityZoneId = AZ_ID\n",
    "    )\n",
    "\n",
    "#    display(resp)\n",
    "else:\n",
    "    print(f\"Scaling Group {SCALING_GROUP_NAME} exists\")    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6943fb16-8989-4199-a0fd-5c7c0d5aa56e",
   "metadata": {},
   "source": [
    "# Create Shared Volume\n",
    "The shared volume is a common storage device for the application. Every cluster using the shared volume will have a writable directory named after the cluster, can read the directories named after other clusters in the application using the volume. Also, there is a common directory for every shared volume as well, all clusters using a volumes can read/write to the common directory.\n",
    "\n",
    "## Directory Structure\n",
    "Any shared volumes will appear in the /opt/kx/app/shared directory of clusters using the volume, with a path is named for shared volume (/opt/kx/app/shared/VOLUME_NAME). Each cluster using the volume will have a directory named for the cluster that only the cluster can write to (/opt/kx/app/shared/VOLUME_NAME/CLUSTER_NAME) and others using the volumes can read from. Last each shared volume has a directory that is read/write to all clusters using the volume (/opt/kx/app/shared/VOLUME_NAME/common)\n",
    "\n",
    "**Root:** /opt/kx/app/shared   \n",
    "**Each Volume:** /opt/kx/app/shared/VOLUME_NAME   \n",
    "**Write per cluster (read otherwise):** /opt/kx/app/shared/VOLUME_NAME/CLUSTER_NAME   \n",
    "**common read/write:** /opt/kx/app/shared/VOLUME_NAME/common   \n",
    "\n",
    "## Reference\n",
    "[FinSpace Managed kdb Volumes](https://docs.aws.amazon.com/finspace/latest/userguide/finspace-managed-kdb-volumes.html)\n",
    "\n",
    "## APIs used\n",
    "[create_kx_volume](https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/finspace/client/create_kx_volume.html) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e4a8a247-d029-4f9b-aaf5-c6e2ffe200a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if volume already exists before trying to create one\n",
    "resp = get_kx_volume(client=client, environmentId=ENV_ID, volumeName=VOLUME_NAME)\n",
    "\n",
    "if resp is None:\n",
    "    resp = client.create_kx_volume(\n",
    "        environmentId = ENV_ID, \n",
    "        volumeType = 'NAS_1',\n",
    "        volumeName = VOLUME_NAME,\n",
    "        description = 'Shared volume between TP and RDB',\n",
    "        nas1Configuration = NAS1_CONFIG,\n",
    "        azMode='SINGLE',\n",
    "        availabilityZoneIds=[ AZ_ID ]    \n",
    "    )\n",
    "\n",
    "#    display(resp)\n",
    "else:\n",
    "    print(f\"Volume {VOLUME_NAME} exists\")        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe41eaeb-9c8e-44d3-b8bc-f354142f9140",
   "metadata": {},
   "source": [
    "# Create Dataview\n",
    "Create a dataview of the database and have all of its data presented (cached) on the shared volume. Customers can also choose to cache only a portion of the database and can also shoose to tier storage on different volumes as well.\n",
    "\n",
    "### Reference\n",
    "[Dataviews for querying data](https://docs.aws.amazon.com/finspace/latest/userguide/finspace-managed-kdb-dataviews.html)\n",
    "\n",
    "### APIs used\n",
    "[create_kx_dataview](https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/finspace/client/create_kx_dataview.html) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ebcbde08-ef39-41e3-bb3e-d97711dbbd1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Volume: RDB_TP_SHARED status is CREATING, total wait 0:00:00, waiting 30 sec ...\n",
      "Volume: RDB_TP_SHARED status is CREATING, total wait 0:00:30, waiting 30 sec ...\n",
      "Volume: RDB_TP_SHARED status is CREATING, total wait 0:01:00, waiting 30 sec ...\n",
      "Volume: RDB_TP_SHARED status is CREATING, total wait 0:01:30, waiting 30 sec ...\n",
      "Volume: RDB_TP_SHARED status is CREATING, total wait 0:02:00, waiting 30 sec ...\n",
      "Volume: RDB_TP_SHARED status is CREATING, total wait 0:02:30, waiting 30 sec ...\n",
      "Volume: RDB_TP_SHARED status is CREATING, total wait 0:03:00, waiting 30 sec ...\n",
      "Volume: RDB_TP_SHARED status is CREATING, total wait 0:03:30, waiting 30 sec ...\n",
      "Volume: RDB_TP_SHARED status is CREATING, total wait 0:04:00, waiting 30 sec ...\n",
      "Volume: RDB_TP_SHARED status is CREATING, total wait 0:04:30, waiting 30 sec ...\n",
      "Volume: RDB_TP_SHARED status is CREATING, total wait 0:05:00, waiting 30 sec ...\n",
      "Volume: RDB_TP_SHARED status is CREATING, total wait 0:05:30, waiting 30 sec ...\n",
      "Volume: RDB_TP_SHARED status is CREATING, total wait 0:06:00, waiting 30 sec ...\n",
      "Volume: RDB_TP_SHARED status is CREATING, total wait 0:06:30, waiting 30 sec ...\n",
      "Volume: RDB_TP_SHARED status is CREATING, total wait 0:07:00, waiting 30 sec ...\n",
      "Volume: RDB_TP_SHARED status is CREATING, total wait 0:07:30, waiting 30 sec ...\n",
      "Volume: RDB_TP_SHARED status is CREATING, total wait 0:08:00, waiting 30 sec ...\n",
      "Volume: RDB_TP_SHARED status is CREATING, total wait 0:08:30, waiting 30 sec ...\n",
      "Volume: RDB_TP_SHARED status is CREATING, total wait 0:09:00, waiting 30 sec ...\n",
      "Volume: RDB_TP_SHARED status is CREATING, total wait 0:09:30, waiting 30 sec ...\n",
      "Volume: RDB_TP_SHARED status is now ACTIVE, total wait 0:10:00\n",
      "** VOLUME is READY **\n"
     ]
    }
   ],
   "source": [
    "# before creating the dataview, be sure the volume is created and ready\n",
    "wait_for_volume_status(client=client, environmentId=ENV_ID, volumeName=VOLUME_NAME, show_wait=True)\n",
    "\n",
    "print(\"** VOLUME is READY **\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "03434316-4ccc-420d-adee-715e6eb1bcd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# do changesets exist?\n",
    "c_set_list = list_kx_changesets(client, environmentId=ENV_ID, databaseName=DB_NAME)\n",
    "\n",
    "if len(c_set_list) != 0:\n",
    "    # sort by create time\n",
    "    c_set_list = sorted(c_set_list, key=lambda d: d['createdTimestamp']) \n",
    "    latest_changeset = c_set_list[-1]['changesetId']\n",
    "\n",
    "    # Check if dataview already exists and is set to the requested changeset_id\n",
    "    resp = get_kx_dataview(client=client, environmentId=ENV_ID, databaseName=DB_NAME, dataviewName=DBVIEW_NAME)\n",
    "\n",
    "    if resp is None:\n",
    "        resp = client.create_kx_dataview(\n",
    "            environmentId = ENV_ID, \n",
    "            databaseName=DB_NAME, \n",
    "            dataviewName=DBVIEW_NAME,\n",
    "            azMode='SINGLE',\n",
    "            availabilityZoneId=AZ_ID,\n",
    "            segmentConfigurations=[\n",
    "                { \n",
    "                    'volumeName': VOLUME_NAME,\n",
    "                    'dbPaths': ['/*'],  # cache all of database\n",
    "                }\n",
    "            ],\n",
    "            autoUpdate=False,\n",
    "            changesetId=latest_changeset, # latest changeset_id for static view\n",
    "#            autoUpdate=True,\n",
    "            description = f'Dataview of database'\n",
    "        )\n",
    "    elif resp['changesetId'] != latest_changeset:\n",
    "        print(f\"Dataview {DBVIEW_NAME} exists but needs updating...\")\n",
    "        resp = client.update_kx_dataview(environmentId=ENV_ID, \n",
    "            databaseName=DB_NAME, \n",
    "            dataviewName=DBVIEW_NAME, \n",
    "            changesetId=latest_changeset, \n",
    "            segmentConfigurations=[\n",
    "                {'dbPaths': ['/*'], 'volumeName': VOLUME_NAME}\n",
    "            ]\n",
    "        )\n",
    "    else:\n",
    "        print(f\"Dataview {DBVIEW_NAME} exists with current changeset: {latest_changeset}\")\n",
    "else:\n",
    "    # no changesets, do NOT create view\n",
    "    print(f\"No changeset in database: {DB_NAME}, Dataview {DBVIEW_NAME} not created\")        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dea431b0-c501-46bb-b72a-a5eb80a335b0",
   "metadata": {},
   "source": [
    "# Create Clusters\n",
    "Create the needed clusters for the application. \n",
    "\n",
    "Code to be used in this application must be staged to an S3 bucket the service can read from, that code will be deployed to each cluster as part of the cluster creation process.\n",
    "\n",
    "## Reference\n",
    "[Managed kdb Insights clusters](https://docs.aws.amazon.com/finspace/latest/userguide/finspace-managed-kdb-clusters.html)   \n",
    "[Cluster types](https://docs.aws.amazon.com/finspace/latest/userguide/kdb-cluster-types.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b502a0a5-8610-4fc8-b6b7-04c47e89ba75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 60K\n",
      "-rw-rw-r-- 1 ec2-user ec2-user  274 Jul 30 21:05 funcDownHandle.q\n",
      "-rw-rw-r-- 1 ec2-user ec2-user 3.1K Jul 30 21:05 connectmkdb.q\n",
      "-rw-rw-r-- 1 ec2-user ec2-user 3.0K Jul 30 21:05 gwmkdb.q\n",
      "-rw-rw-r-- 1 ec2-user ec2-user 4.5K Jul 30 21:05 kxtaqfeed.q\n",
      "-rw-rw-r-- 1 ec2-user ec2-user 2.7K Jul 30 21:05 kxtaqdb.q\n",
      "-rw-rw-r-- 1 ec2-user ec2-user  695 Jul 30 21:05 query.q\n",
      "-rw-rw-r-- 1 ec2-user ec2-user  212 Jul 30 21:05 taq.schema.q\n",
      "drwxrwxr-x 3 ec2-user ec2-user   43 Aug  5 20:15 tick\n",
      "-rw-rw-r-- 1 ec2-user ec2-user  752 Aug  5 20:32 kxtaqsubscriber.q\n",
      "-rw-rw-r-- 1 ec2-user ec2-user  655 Aug  6 14:59 hdbmkdb.q\n",
      "-rw-rw-r-- 1 ec2-user ec2-user 3.2K Aug  6 14:59 cepmkdb.q\n",
      "-rw-rw-r-- 1 ec2-user ec2-user 2.8K Aug  7 12:40 tick.q\n",
      "-rw-rw-r-- 1 ec2-user ec2-user 4.8K Aug  7 17:59 rdbmkdb.q\n",
      "drwxrwxr-x 2 ec2-user ec2-user  213 Aug  8 18:37 .ipynb_checkpoints\n",
      "drwxrwxr-x 4 ec2-user ec2-user  266 Aug  8 18:37 .\n",
      "drwxrwxr-x 8 ec2-user ec2-user 4.0K Aug  8 18:53 ..\n",
      "updating: connectmkdb.q (deflated 63%)\n",
      "updating: funcDownHandle.q (deflated 33%)\n",
      "updating: gwmkdb.q (deflated 61%)\n",
      "updating: hdbmkdb.q (deflated 42%)\n",
      "updating: kxtaqfeed.q (deflated 50%)\n",
      "updating: kxtaqdb.q (deflated 44%)\n",
      "updating: kxtaqsubscriber.q (deflated 42%)\n",
      "updating: query.q (deflated 49%)\n",
      "updating: rdbmkdb.q (deflated 58%)\n",
      "updating: taq.schema.q (deflated 45%)\n",
      "updating: tick/ (stored 0%)\n",
      "updating: tick/u.q (deflated 32%)\n",
      "updating: tick.q (deflated 49%)\n",
      "  adding: cepmkdb.q (deflated 56%)\n",
      "upload: ./basictick.zip to s3://kdb-demo-829845998889-kms/code/basictick.zip\n",
      "2024-08-08 18:55:00      43721 basictick.zip\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# code that will be deployed\n",
    "os.system(f\"ls -lrtha {CODEBASE}\")\n",
    "\n",
    "# create zipfile of the code\n",
    "os.system(f\"cd {CODEBASE}; zip -r -X ../{CODEBASE}.zip . -x '*.ipynb_checkpoints*';\")\n",
    "\n",
    "# Copy command with credentials\n",
    "if AWS_ACCESS_KEY_ID is not None:\n",
    "    cp = f\"\"\"\n",
    "export AWS_ACCESS_KEY_ID={AWS_ACCESS_KEY_ID}\n",
    "export AWS_SECRET_ACCESS_KEY={AWS_SECRET_ACCESS_KEY}\n",
    "export AWS_SESSION_TOKEN={AWS_SESSION_TOKEN}\n",
    "\n",
    "aws s3 cp  --exclude .DS_Store {CODEBASE}.zip s3://{S3_BUCKET}/code/{CODEBASE}.zip\n",
    "\"\"\"\n",
    "else:\n",
    "    cp = f\"\"\"\n",
    "aws s3 cp  --exclude .DS_Store {CODEBASE}.zip s3://{S3_BUCKET}/code/{CODEBASE}.zip\n",
    "\"\"\"\n",
    "    \n",
    "# Copy the code\n",
    "os.system(cp)\n",
    "\n",
    "# Code on S3\n",
    "os.system(f\"aws s3 ls s3://{S3_BUCKET}/code/{CODEBASE}.zip\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92a6d9d0-1043-4914-be9a-074629ed174d",
   "metadata": {},
   "source": [
    "## Wait for Scaling Group to be Ready\n",
    "Before creating clusters in a scaling group, be sure the scaling group is ready."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "862d8607-a3aa-47c4-b7e5-a3c89aebe241",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scaling Group: SCALING_GROUP_basictickdb status is now ACTIVE, total wait 0:00:00\n",
      "** DONE **\n"
     ]
    }
   ],
   "source": [
    "# wait for the scaling group to create\n",
    "wait_for_scaling_group_status(client=client, environmentId=ENV_ID, scalingGroupName=SCALING_GROUP_NAME, show_wait=True)\n",
    "\n",
    "print(\"** DONE **\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88716ead-2205-4971-a5b0-33ffe96d7f85",
   "metadata": {},
   "source": [
    "## Create Tickerplant (TP) Cluster\n",
    "Tickerplant will deliver data from feedhandler to subscribing RDB.\n",
    "\n",
    "### APIs used\n",
    "[create_kx_cluster](https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/finspace/client/create_kx_cluster.html) \n",
    "\n",
    "#### Notes\n",
    "- No database used by TP, databases argument is not used   \n",
    "- Use tickerplantLogConfiguration **not** savedownStorageConfiguration   \n",
    "  - tickerplantLogVolumes uses the same shared volume as other clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "28bf38a1-7733-4eb2-839a-a302a57c8225",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating: TP_basictickdb\n"
     ]
    }
   ],
   "source": [
    "# does cluster already exist?\n",
    "resp = get_kx_cluster(client, environmentId=ENV_ID, clusterName=TP_CLUSTER_NAME)\n",
    "\n",
    "if resp is not None:\n",
    "    print(f\"Cluster: {TP_CLUSTER_NAME} already exists\")\n",
    "else:\n",
    "    print(f\"Creating: {TP_CLUSTER_NAME}\")\n",
    "\n",
    "    resp = client.create_kx_cluster(\n",
    "        environmentId=ENV_ID, \n",
    "        clusterName=TP_CLUSTER_NAME,\n",
    "        clusterType='TICKERPLANT',\n",
    "        releaseLabel = '1.0',\n",
    "        executionRole=EXECUTION_ROLE,\n",
    "        scalingGroupConfiguration={\n",
    "#            'memoryLimit': 1*1024,\n",
    "            'memoryReservation': 6,\n",
    "            'nodeCount': 1,\n",
    "            'scalingGroupName': SCALING_GROUP_NAME,\n",
    "        },\n",
    "        tickerplantLogConfiguration ={ 'tickerplantLogVolumes': [ VOLUME_NAME ] },\n",
    "        clusterDescription=\"Created with create_all notebook\",\n",
    "        code=CODE_CONFIG,\n",
    "        initializationScript=TP_INIT_SCRIPT,\n",
    "        commandLineArguments=TP_CMD_ARGS,\n",
    "        azMode=AZ_MODE,\n",
    "        availabilityZoneId=AZ_ID,\n",
    "        vpcConfiguration={ \n",
    "            'vpcId': VPC_ID,\n",
    "            'securityGroupIds': SECURITY_GROUPS,\n",
    "            'subnetIds': SUBNET_IDS,\n",
    "            'ipAddressType': 'IP_V4' \n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a2a31ee-07e1-408b-8fe9-2a9fb92f4df2",
   "metadata": {},
   "source": [
    "## Create Historical Database (HDB) Cluster\n",
    "A multi-node HDB cluster will serve up queries for T+1 and older data. \n",
    "\n",
    "### APIs used\n",
    "[create_kx_cluster](https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/finspace/client/create_kx_cluster.html) \n",
    "\n",
    "#### Notes\n",
    "- **databases**: defines which database and view to use\n",
    "  - View used by the HDB cluster must be up and running   \n",
    "- No a tickerplant, no tickerplantLogConfiguration argument   \n",
    "- No savedown needed, no savedownStorageConfiguration argument  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dc77c3a1-dcb6-446b-9756-59431b109b20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataview: basictickdb_DBVIEW status is CREATING, total wait 0:00:00, waiting 30 sec ...\n",
      "Dataview: basictickdb_DBVIEW status is CREATING, total wait 0:00:30, waiting 30 sec ...\n",
      "Dataview: basictickdb_DBVIEW status is CREATING, total wait 0:01:00, waiting 30 sec ...\n",
      "Dataview: basictickdb_DBVIEW status is CREATING, total wait 0:01:30, waiting 30 sec ...\n",
      "Dataview: basictickdb_DBVIEW status is CREATING, total wait 0:02:00, waiting 30 sec ...\n",
      "Dataview: basictickdb_DBVIEW status is CREATING, total wait 0:02:30, waiting 30 sec ...\n",
      "Dataview: basictickdb_DBVIEW status is CREATING, total wait 0:03:00, waiting 30 sec ...\n",
      "Dataview: basictickdb_DBVIEW status is CREATING, total wait 0:03:30, waiting 30 sec ...\n",
      "Dataview: basictickdb_DBVIEW status is CREATING, total wait 0:04:00, waiting 30 sec ...\n",
      "Dataview: basictickdb_DBVIEW status is CREATING, total wait 0:04:30, waiting 30 sec ...\n",
      "Dataview: basictickdb_DBVIEW status is CREATING, total wait 0:05:00, waiting 30 sec ...\n",
      "Dataview: basictickdb_DBVIEW status is CREATING, total wait 0:05:30, waiting 30 sec ...\n",
      "Dataview: basictickdb_DBVIEW status is CREATING, total wait 0:06:00, waiting 30 sec ...\n",
      "Dataview: basictickdb_DBVIEW status is now ACTIVE, total wait 0:06:30\n",
      "** Dataview is READY **\n"
     ]
    }
   ],
   "source": [
    "# Dataview must be ready before creating the HDB Cluster\n",
    "wait_for_dataview_status(client=client, environmentId=ENV_ID, databaseName=DB_NAME, dataviewName=DBVIEW_NAME, show_wait=True)\n",
    "\n",
    "print(\"** Dataview is READY **\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3abd68fa-5690-4374-bb68-4277bb87cf26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating: HDB_basictickdb\n"
     ]
    }
   ],
   "source": [
    "# does cluster already exist?\n",
    "resp = get_kx_cluster(client, environmentId=ENV_ID, clusterName=HDB_CLUSTER_NAME)\n",
    "\n",
    "if resp is not None:\n",
    "    print(f\"Cluster: {HDB_CLUSTER_NAME} already exists\")\n",
    "else:\n",
    "    print(f\"Creating: {HDB_CLUSTER_NAME}\")\n",
    "\n",
    "    resp = client.create_kx_cluster(\n",
    "        environmentId=ENV_ID, \n",
    "        clusterName=HDB_CLUSTER_NAME,\n",
    "        clusterType='HDB',\n",
    "        releaseLabel = '1.0',\n",
    "        executionRole=EXECUTION_ROLE,\n",
    "        databases=[{ 'databaseName': DB_NAME, 'dataviewName': DBVIEW_NAME }],\n",
    "        scalingGroupConfiguration={\n",
    "#            'memoryLimit': 1*1024,\n",
    "            'memoryReservation': 6,\n",
    "            'nodeCount': 2,\n",
    "            'scalingGroupName': SCALING_GROUP_NAME,\n",
    "        },\n",
    "        clusterDescription=\"Created with create_all notebook\",\n",
    "        code=CODE_CONFIG,\n",
    "        initializationScript=HDB_INIT_SCRIPT,\n",
    "        commandLineArguments=HDB_CMD_ARGS,\n",
    "        azMode=AZ_MODE,\n",
    "        availabilityZoneId=AZ_ID,\n",
    "        vpcConfiguration={ \n",
    "            'vpcId': VPC_ID,\n",
    "            'securityGroupIds': SECURITY_GROUPS,\n",
    "            'subnetIds': SUBNET_IDS,\n",
    "            'ipAddressType': 'IP_V4' \n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "932a21ff-a51f-47d3-9b70-c4eb863fa34f",
   "metadata": {},
   "source": [
    "## Create Gateway (GW) Cluster\n",
    "The Gateway will handle client queries for data in the RDB and HDB. Gateways act as single API access point for data queries and will query both the RDB and HDB and aggregate results back to requestor.\n",
    "\n",
    "### APIs used\n",
    "[create_kx_cluster](https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/finspace/client/create_kx_cluster.html) \n",
    "\n",
    "#### Notes\n",
    "- Gateways connect to other clusters and aggregate results   \n",
    "  - No databases, tickerplantLogConfiguration, or savedownStorageConfiguration arguments\n",
    "- execution role required, role is used when connecting to other clusters  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2f27b7a1-4e8a-4b32-9175-a83ffb97f2de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating: GATEWAY_basictickdb\n"
     ]
    }
   ],
   "source": [
    "# does cluster already exist?\n",
    "resp = get_kx_cluster(client, environmentId=ENV_ID, clusterName=GW_CLUSTER_NAME)\n",
    "\n",
    "if resp is not None:\n",
    "    print(f\"Cluster: {GW_CLUSTER_NAME} already exists\")\n",
    "else:\n",
    "    print(f\"Creating: {GW_CLUSTER_NAME}\")\n",
    "\n",
    "    resp = client.create_kx_cluster(\n",
    "        environmentId=ENV_ID, \n",
    "        clusterName=GW_CLUSTER_NAME,\n",
    "        clusterType='GATEWAY',\n",
    "        releaseLabel = '1.0',\n",
    "        scalingGroupConfiguration={\n",
    "#            'memoryLimit': 1*1024,\n",
    "            'memoryReservation': 6,\n",
    "            'nodeCount': 1,\n",
    "            'scalingGroupName': SCALING_GROUP_NAME,\n",
    "        },\n",
    "        clusterDescription=\"Created with create_all notebook\",\n",
    "        executionRole=EXECUTION_ROLE,\n",
    "        code=CODE_CONFIG,\n",
    "        initializationScript=GW_INIT_SCRIPT,\n",
    "        commandLineArguments=GW_CMD_ARGS,\n",
    "        azMode=AZ_MODE,\n",
    "        availabilityZoneId=AZ_ID,\n",
    "        vpcConfiguration={ \n",
    "            'vpcId': VPC_ID,\n",
    "            'securityGroupIds': SECURITY_GROUPS,\n",
    "            'subnetIds': SUBNET_IDS,\n",
    "            'ipAddressType': 'IP_V4' \n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fd1688e-850d-423a-b98f-8cc67eb9ca8d",
   "metadata": {},
   "source": [
    "## Create Realtime Database (RDB)\n",
    "The RDB will subscribe to the tickerplant and capture real time data published by the tickerplant (as published by the feedhandler).\n",
    "\n",
    "Since the RDB clusters depend on the TP cluster, will check that its up before creating the RDBs.\n",
    "\n",
    "### APIs used\n",
    "[create_kx_cluster](https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/finspace/client/create_kx_cluster.html) \n",
    "\n",
    "####  Notes\n",
    "- **databases:**  must include database and view   \n",
    "  - RDB will update the dbview of the database as part of end of day processing\n",
    "- **savedownStorageConfiguration:** defines storage used   \n",
    "  - End of day data is first saved to this location before updating the database "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8efea149-22e9-4503-9c31-903b742a77eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster: TP_basictickdb status is CREATING, total wait 0:00:00, waiting 30 sec ...\n",
      "Cluster: TP_basictickdb status is CREATING, total wait 0:00:30, waiting 30 sec ...\n",
      "Cluster: TP_basictickdb status is CREATING, total wait 0:01:00, waiting 30 sec ...\n",
      "Cluster: TP_basictickdb status is CREATING, total wait 0:01:30, waiting 30 sec ...\n",
      "Cluster: TP_basictickdb status is CREATING, total wait 0:02:00, waiting 30 sec ...\n",
      "Cluster: TP_basictickdb status is CREATING, total wait 0:02:30, waiting 30 sec ...\n",
      "Cluster: TP_basictickdb status is CREATING, total wait 0:03:00, waiting 30 sec ...\n",
      "Cluster: TP_basictickdb status is CREATING, total wait 0:03:30, waiting 30 sec ...\n",
      "Cluster: TP_basictickdb status is CREATING, total wait 0:04:00, waiting 30 sec ...\n",
      "Cluster: TP_basictickdb status is CREATING, total wait 0:04:30, waiting 30 sec ...\n",
      "Cluster: TP_basictickdb status is CREATING, total wait 0:05:00, waiting 30 sec ...\n",
      "Cluster: TP_basictickdb status is CREATING, total wait 0:05:30, waiting 30 sec ...\n",
      "Cluster: TP_basictickdb status is CREATING, total wait 0:06:00, waiting 30 sec ...\n",
      "Cluster: TP_basictickdb status is CREATING, total wait 0:06:30, waiting 30 sec ...\n",
      "Cluster: TP_basictickdb status is CREATING, total wait 0:07:00, waiting 30 sec ...\n",
      "Cluster: TP_basictickdb status is CREATING, total wait 0:07:30, waiting 30 sec ...\n",
      "Cluster: TP_basictickdb status is CREATING, total wait 0:08:00, waiting 30 sec ...\n",
      "Cluster: TP_basictickdb status is CREATING, total wait 0:08:30, waiting 30 sec ...\n",
      "Cluster: TP_basictickdb status is CREATING, total wait 0:09:00, waiting 30 sec ...\n",
      "Cluster: TP_basictickdb status is CREATING, total wait 0:09:30, waiting 30 sec ...\n",
      "Cluster: TP_basictickdb status is CREATING, total wait 0:10:00, waiting 30 sec ...\n",
      "Cluster: TP_basictickdb status is CREATING, total wait 0:10:30, waiting 30 sec ...\n",
      "Cluster: TP_basictickdb status is CREATING, total wait 0:11:00, waiting 30 sec ...\n",
      "Cluster: TP_basictickdb status is CREATING, total wait 0:11:30, waiting 30 sec ...\n",
      "Cluster: TP_basictickdb status is CREATING, total wait 0:12:00, waiting 30 sec ...\n",
      "Cluster: TP_basictickdb status is now RUNNING, total wait 0:12:30\n",
      "TP is running\n"
     ]
    }
   ],
   "source": [
    "# TP must be running before creating the RDBs\n",
    "wait_for_cluster_status(client, environmentId=ENV_ID, clusterName=TP_CLUSTER_NAME, show_wait=True)\n",
    "\n",
    "print(\"TP is running\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3e39ebf3-6940-40f1-a7f8-90efb3846f7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating: RDB_basictickdb\n"
     ]
    }
   ],
   "source": [
    "# does cluster already exist?\n",
    "resp = get_kx_cluster(client, environmentId=ENV_ID, clusterName=RDB_CLUSTER_NAME)\n",
    "\n",
    "if resp is not None:\n",
    "    print(f\"Cluster: {RDB_CLUSTER_NAME} already exists\")\n",
    "else:\n",
    "    print(f\"Creating: {RDB_CLUSTER_NAME}\")\n",
    "\n",
    "    resp = client.create_kx_cluster(\n",
    "        environmentId=ENV_ID, \n",
    "        clusterName=RDB_CLUSTER_NAME,\n",
    "        clusterType='RDB',\n",
    "        releaseLabel = '1.0',\n",
    "        executionRole=EXECUTION_ROLE,\n",
    "        databases=[{ 'databaseName': DB_NAME }], #, 'dataviewName': DBVIEW_NAME }],\n",
    "        scalingGroupConfiguration={\n",
    "#            'memoryLimit': 1*1024,\n",
    "            'memoryReservation': 6,\n",
    "            'nodeCount': 1,\n",
    "            'scalingGroupName': SCALING_GROUP_NAME,\n",
    "        },\n",
    "        savedownStorageConfiguration ={ 'volumeName': VOLUME_NAME },\n",
    "        clusterDescription=\"Created with create_all notebook\",\n",
    "        code=CODE_CONFIG,\n",
    "        initializationScript=RDB_INIT_SCRIPT,\n",
    "        commandLineArguments=RDB_CMD_ARGS,\n",
    "        azMode=AZ_MODE,\n",
    "        availabilityZoneId=AZ_ID,\n",
    "        vpcConfiguration={ \n",
    "            'vpcId': VPC_ID,\n",
    "            'securityGroupIds': SECURITY_GROUPS,\n",
    "            'subnetIds': SUBNET_IDS,\n",
    "            'ipAddressType': 'IP_V4' \n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0c1f4f3",
   "metadata": {},
   "source": [
    "## Create Complex Event Processor (CEP)\n",
    "The CEP is similar to the RDB, and will subscribe to the tickerplant to capture real time data, however it will also produce/publish derived data that can be subscribed to.\n",
    "\n",
    "### APIs used\n",
    "[create_kx_cluster](https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/finspace/client/create_kx_cluster.html) \n",
    "\n",
    "#### Notes\n",
    "- Connects to TP clusters, subscribesfor data and publishes its calculations   \n",
    "  - No databases, tickerplantLogConfiguration, or savedownStorageConfiguration needed\n",
    "- execution role required, role is used when connecting to TP cluster   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a51df448",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating: CEP_basictickdb\n"
     ]
    }
   ],
   "source": [
    "# does cluster already exist?\n",
    "resp = get_kx_cluster(client, environmentId=ENV_ID, clusterName=CEP_CLUSTER_NAME)\n",
    "\n",
    "if resp is not None:\n",
    "    print(f\"Cluster: {CEP_CLUSTER_NAME} already exists\")\n",
    "else:\n",
    "    print(f\"Creating: {CEP_CLUSTER_NAME}\")\n",
    "    \n",
    "    resp = client.create_kx_cluster(\n",
    "        environmentId=ENV_ID, \n",
    "        clusterName=CEP_CLUSTER_NAME,\n",
    "        clusterType='RDB',\n",
    "        releaseLabel = '1.0',\n",
    "        executionRole=EXECUTION_ROLE,\n",
    "        scalingGroupConfiguration={\n",
    "#            'memoryLimit': 1*1024,\n",
    "            'memoryReservation': 6,\n",
    "            'nodeCount': 1,\n",
    "            'scalingGroupName': SCALING_GROUP_NAME,\n",
    "        },\n",
    "        clusterDescription=\"Created with create_all notebook\",\n",
    "        code=CODE_CONFIG,\n",
    "        initializationScript=CEP_INIT_SCRIPT,\n",
    "        commandLineArguments=CEP_CMD_ARGS,\n",
    "        azMode=AZ_MODE,\n",
    "        availabilityZoneId=AZ_ID,\n",
    "        vpcConfiguration={ \n",
    "            'vpcId': VPC_ID,\n",
    "            'securityGroupIds': SECURITY_GROUPS,\n",
    "            'subnetIds': SUBNET_IDS,\n",
    "            'ipAddressType': 'IP_V4' \n",
    "        }\n",
    "    )\n",
    "\n",
    "#display(resp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "230207e8-c297-4d7e-af65-4396fa5b4deb",
   "metadata": {},
   "source": [
    "# List All Clusters\n",
    "List all clusters, but first be sure all are in running state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0946ca26-c4b0-410d-ade5-18a47bf2318a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster: TP_basictickdb status is now RUNNING, total wait 0:00:00\n",
      "Cluster: RDB_basictickdb status is PENDING, total wait 0:00:00, waiting 30 sec ...\n",
      "Cluster: RDB_basictickdb status is CREATING, total wait 0:00:30, waiting 30 sec ...\n",
      "Cluster: RDB_basictickdb status is CREATING, total wait 0:01:00, waiting 30 sec ...\n",
      "Cluster: RDB_basictickdb status is CREATING, total wait 0:01:30, waiting 30 sec ...\n",
      "Cluster: RDB_basictickdb status is CREATING, total wait 0:02:00, waiting 30 sec ...\n",
      "Cluster: RDB_basictickdb status is CREATING, total wait 0:02:30, waiting 30 sec ...\n",
      "Cluster: RDB_basictickdb status is CREATING, total wait 0:03:00, waiting 30 sec ...\n",
      "Cluster: RDB_basictickdb status is CREATING, total wait 0:03:30, waiting 30 sec ...\n",
      "Cluster: RDB_basictickdb status is CREATING, total wait 0:04:00, waiting 30 sec ...\n",
      "Cluster: RDB_basictickdb status is CREATING, total wait 0:04:30, waiting 30 sec ...\n",
      "Cluster: RDB_basictickdb status is CREATING, total wait 0:05:00, waiting 30 sec ...\n",
      "Cluster: RDB_basictickdb status is CREATING, total wait 0:05:30, waiting 30 sec ...\n",
      "Cluster: RDB_basictickdb status is CREATING, total wait 0:06:00, waiting 30 sec ...\n",
      "Cluster: RDB_basictickdb status is CREATING, total wait 0:06:30, waiting 30 sec ...\n",
      "Cluster: RDB_basictickdb status is CREATING, total wait 0:07:00, waiting 30 sec ...\n",
      "Cluster: RDB_basictickdb status is CREATING, total wait 0:07:30, waiting 30 sec ...\n",
      "Cluster: RDB_basictickdb status is CREATING, total wait 0:08:00, waiting 30 sec ...\n",
      "Cluster: RDB_basictickdb status is CREATING, total wait 0:08:30, waiting 30 sec ...\n",
      "Cluster: RDB_basictickdb status is CREATING, total wait 0:09:00, waiting 30 sec ...\n",
      "Cluster: RDB_basictickdb status is CREATING, total wait 0:09:30, waiting 30 sec ...\n",
      "Cluster: RDB_basictickdb status is CREATING, total wait 0:10:00, waiting 30 sec ...\n",
      "Cluster: RDB_basictickdb status is CREATING, total wait 0:10:30, waiting 30 sec ...\n",
      "Cluster: RDB_basictickdb status is CREATING, total wait 0:11:00, waiting 30 sec ...\n",
      "Cluster: RDB_basictickdb status is CREATING, total wait 0:11:30, waiting 30 sec ...\n",
      "Cluster: RDB_basictickdb status is CREATING, total wait 0:12:00, waiting 30 sec ...\n",
      "Cluster: RDB_basictickdb status is CREATING, total wait 0:12:30, waiting 30 sec ...\n",
      "Cluster: RDB_basictickdb status is CREATING, total wait 0:13:00, waiting 30 sec ...\n",
      "Cluster: RDB_basictickdb status is CREATING, total wait 0:13:30, waiting 30 sec ...\n",
      "Cluster: RDB_basictickdb status is CREATING, total wait 0:14:00, waiting 30 sec ...\n",
      "Cluster: RDB_basictickdb status is CREATING, total wait 0:14:30, waiting 30 sec ...\n",
      "Cluster: RDB_basictickdb status is CREATING, total wait 0:15:00, waiting 30 sec ...\n",
      "Cluster: RDB_basictickdb status is now RUNNING, total wait 0:15:30\n",
      "Cluster: HDB_basictickdb status is now RUNNING, total wait 0:00:00\n",
      "Cluster: GATEWAY_basictickdb status is now RUNNING, total wait 0:00:00\n",
      "Cluster: CEP_basictickdb status is now RUNNING, total wait 0:00:00\n",
      "** ALL CLUSTERS DONE **\n"
     ]
    }
   ],
   "source": [
    "# Wait for all clusters be in running state\n",
    "for c in all_clusters.values():\n",
    "    wait_for_cluster_status(client, environmentId=ENV_ID, clusterName=c, show_wait=True)\n",
    "\n",
    "print(\"** ALL CLUSTERS DONE **\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1c50c578-05e8-49e7-8deb-1f6b94b10221",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>clusterName</th>\n",
       "      <th>status</th>\n",
       "      <th>clusterType</th>\n",
       "      <th>capacityConfiguration</th>\n",
       "      <th>commandLineArguments</th>\n",
       "      <th>clusterDescription</th>\n",
       "      <th>lastModifiedTimestamp</th>\n",
       "      <th>createdTimestamp</th>\n",
       "      <th>databaseName</th>\n",
       "      <th>cacheConfigurations</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CEP_basictickdb</td>\n",
       "      <td>RUNNING</td>\n",
       "      <td>RDB</td>\n",
       "      <td>None</td>\n",
       "      <td>[{'key': 's', 'value': '2'}, {'key': 'g', 'value': '1'}, {'key': 'tp', 'value': 'TP_basictickdb'}, {'key': 'AWS_ZIP_DEFAULT', 'value': '17,2,6'}]</td>\n",
       "      <td>Created with create_all notebook</td>\n",
       "      <td>2024-08-08 19:30:09.273000+00:00</td>\n",
       "      <td>2024-08-08 19:14:36.519000+00:00</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>GATEWAY_basictickdb</td>\n",
       "      <td>RUNNING</td>\n",
       "      <td>GATEWAY</td>\n",
       "      <td>None</td>\n",
       "      <td>[{'key': 's', 'value': '2'}, {'key': 'g', 'value': '1'}, {'key': 'rdb_name', 'value': 'RDB_basictickdb'}, {'key': 'hdb_name', 'value': 'HDB_basictickdb'}]</td>\n",
       "      <td>Created with create_all notebook</td>\n",
       "      <td>2024-08-08 19:13:43.443000+00:00</td>\n",
       "      <td>2024-08-08 19:01:48.261000+00:00</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>HDB_basictickdb</td>\n",
       "      <td>RUNNING</td>\n",
       "      <td>HDB</td>\n",
       "      <td>None</td>\n",
       "      <td>[{'key': 's', 'value': '2'}, {'key': 'g', 'value': '1'}]</td>\n",
       "      <td>Created with create_all notebook</td>\n",
       "      <td>2024-08-08 19:13:43.587000+00:00</td>\n",
       "      <td>2024-08-08 19:01:45.488000+00:00</td>\n",
       "      <td>basictickdb</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>RDB_basictickdb</td>\n",
       "      <td>RUNNING</td>\n",
       "      <td>RDB</td>\n",
       "      <td>None</td>\n",
       "      <td>[{'key': 's', 'value': '2'}, {'key': 'g', 'value': '1'}, {'key': 'tp', 'value': 'TP_basictickdb'}, {'key': 'procName', 'value': 'RDB_basictickdb'}, {'key': 'volumeName', 'value': 'RDB_TP_SHARED'}, {'key': 'hdbProc', 'value': 'HDB_basictickdb'}, {'key': 'dbView', 'value': 'basictickdb_DBVIEW'}, {'key': 'AWS_ZIP_DEFAULT', 'value': '17,2,6'}]</td>\n",
       "      <td>Created with create_all notebook</td>\n",
       "      <td>2024-08-08 19:30:09.801000+00:00</td>\n",
       "      <td>2024-08-08 19:14:33.629000+00:00</td>\n",
       "      <td>basictickdb</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TP_basictickdb</td>\n",
       "      <td>RUNNING</td>\n",
       "      <td>TICKERPLANT</td>\n",
       "      <td>None</td>\n",
       "      <td>[{'key': 'procName', 'value': 'TP_basictickdb'}, {'key': 'volumeName', 'value': 'RDB_TP_SHARED'}, {'key': 'AWS_ZIP_DEFAULT', 'value': '17,2,6'}, {'key': 'g', 'value': '1'}]</td>\n",
       "      <td>Created with create_all notebook</td>\n",
       "      <td>2024-08-08 19:13:59.200000+00:00</td>\n",
       "      <td>2024-08-08 18:55:06.046000+00:00</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           clusterName   status  clusterType capacityConfiguration  \\\n",
       "0      CEP_basictickdb  RUNNING          RDB                  None   \n",
       "1  GATEWAY_basictickdb  RUNNING      GATEWAY                  None   \n",
       "2      HDB_basictickdb  RUNNING          HDB                  None   \n",
       "3      RDB_basictickdb  RUNNING          RDB                  None   \n",
       "4       TP_basictickdb  RUNNING  TICKERPLANT                  None   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                    commandLineArguments  \\\n",
       "0                                                                                                                                                                                                      [{'key': 's', 'value': '2'}, {'key': 'g', 'value': '1'}, {'key': 'tp', 'value': 'TP_basictickdb'}, {'key': 'AWS_ZIP_DEFAULT', 'value': '17,2,6'}]   \n",
       "1                                                                                                                                                                                             [{'key': 's', 'value': '2'}, {'key': 'g', 'value': '1'}, {'key': 'rdb_name', 'value': 'RDB_basictickdb'}, {'key': 'hdb_name', 'value': 'HDB_basictickdb'}]   \n",
       "2                                                                                                                                                                                                                                                                                               [{'key': 's', 'value': '2'}, {'key': 'g', 'value': '1'}]   \n",
       "3  [{'key': 's', 'value': '2'}, {'key': 'g', 'value': '1'}, {'key': 'tp', 'value': 'TP_basictickdb'}, {'key': 'procName', 'value': 'RDB_basictickdb'}, {'key': 'volumeName', 'value': 'RDB_TP_SHARED'}, {'key': 'hdbProc', 'value': 'HDB_basictickdb'}, {'key': 'dbView', 'value': 'basictickdb_DBVIEW'}, {'key': 'AWS_ZIP_DEFAULT', 'value': '17,2,6'}]   \n",
       "4                                                                                                                                                                           [{'key': 'procName', 'value': 'TP_basictickdb'}, {'key': 'volumeName', 'value': 'RDB_TP_SHARED'}, {'key': 'AWS_ZIP_DEFAULT', 'value': '17,2,6'}, {'key': 'g', 'value': '1'}]   \n",
       "\n",
       "                 clusterDescription            lastModifiedTimestamp  \\\n",
       "0  Created with create_all notebook 2024-08-08 19:30:09.273000+00:00   \n",
       "1  Created with create_all notebook 2024-08-08 19:13:43.443000+00:00   \n",
       "2  Created with create_all notebook 2024-08-08 19:13:43.587000+00:00   \n",
       "3  Created with create_all notebook 2024-08-08 19:30:09.801000+00:00   \n",
       "4  Created with create_all notebook 2024-08-08 19:13:59.200000+00:00   \n",
       "\n",
       "                  createdTimestamp databaseName cacheConfigurations  \n",
       "0 2024-08-08 19:14:36.519000+00:00         None                 NaN  \n",
       "1 2024-08-08 19:01:48.261000+00:00         None                 NaN  \n",
       "2 2024-08-08 19:01:45.488000+00:00  basictickdb                None  \n",
       "3 2024-08-08 19:14:33.629000+00:00  basictickdb                  []  \n",
       "4 2024-08-08 18:55:06.046000+00:00         None                 NaN  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cdf = get_clusters(client, environmentId=ENV_ID)\n",
    "\n",
    "if cdf is not None:\n",
    "    cdf = cdf[cdf['clusterName'].isin(all_clusters.values())]\n",
    "\n",
    "display(cdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc3cb064-adfc-4f57-8104-a730184197c3",
   "metadata": {},
   "source": [
    "# Start FeedHandler\n",
    "With all clusters running start a feedhandler to send data to the running tickerplant (TP).\n",
    "\n",
    "### Environment/Configuration Details\n",
    "Environment variable QHOME is set to where q is locally to this notebook.   \n",
    "Environment varuable SSL_VERIFY_SERVER is set to 0 so as not to verify the connection to the TP.\n",
    "\n",
    "### Console Example\n",
    "```\n",
    "$ TP_CONN=\"<connection string to cluster>\"\n",
    "$ cd basictick\n",
    "$ q kxtaqfeed.q -g 1 -p 5030 -tp $TP_CONN\n",
    "```\n",
    "\n",
    "Here we use Python to get the connection string, set environment variables, and run the feedhandler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ccfe94d9-206c-40cb-b6a5-332d489872c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"connected to tp\"\n",
      "()\n"
     ]
    }
   ],
   "source": [
    "# get the connection string to the TP cluster\n",
    "conn_str = get_kx_connection_string(client, environmentId=ENV_ID, clusterName=TP_CLUSTER_NAME, userName=KDB_USERNAME, boto_session=session)\n",
    "\n",
    "# populate the environment variable with connection string\n",
    "os.putenv(\"CONN_STR\", conn_str)\n",
    "\n",
    "feed_debug = 0\n",
    "\n",
    "# start q process kxtaqfeed to connect to the TP at $TP_CONN\n",
    "\n",
    "if os.getenv('QHOME') is not None:\n",
    "    pid=subprocess.Popen(f'cd {CODEBASE}; nohup $QHOME/l64/q kxtaqfeed.q -g 1 -p {FH_PORT} -tp $CONN_STR -debug {feed_debug} -t {FEED_TIMER}', shell=True)\n",
    "\n",
    "else:\n",
    "    print(\"Environment variable QHOME is not set, please set to where kdb is installed\")\n",
    "\n",
    "# wait for feedhandler to start doing its thing\n",
    "time.sleep(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "588fa530-58e1-4f58-a699-2f99b2e31f95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>process</th>\n",
       "      <th>connected</th>\n",
       "      <th>handle</th>\n",
       "      <th>address</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>tp</td>\n",
       "      <td>1b</td>\n",
       "      <td>5i</td>\n",
       "      <td>:tcps://vpce-096c514436f930ded-zvyfok7h.vpce-svc-023944f523c6e1c21.us-east-1.vpce.amazonaws.com:443:bob:Host=vpce-096c514436f930ded-zvyfok7h.vpce-svc-023944f523c6e1c21.us-east-1.vpce.amazonaws.com&amp;Port=443&amp;User=bob&amp;Action=finspace%3AConnectKxCluster&amp;X-Amz-Security-Token=IQoJb3JpZ2luX2VjEDwaCXVzLWVhc3QtMSJHMEUCIQDJTbcWftvFGCT%2BJf%2B86PKTaz4uBcmqPdKfy0eSTIRxwwIgbJUw6WQ9o%2BaCHoKWXwUNM6HND0ARA0aC9Xpn7gO%2BQAkq9gIINRAAGgw4Mjk4NDU5OTg4ODkiDN8t0pyJpUaWRThr7yrTAoGvP00rSHjLVoc%2BBEd3E7LHVVamuUngALX0duNQOUIpl8L0%2Fu3eeqQaywe%2Bs0jYH02g03vNcso%2BsQbrmqEz8v4L%2Fx3lPfMiRxR80eAscALWP%2F0aPSgMJqlSOPzOjpRfeDKpvgUf%2FmpGFvWi8tzo2WXUWKKndJ6xyKqq0s1SXtZs%2Be982xBsyvbRv9RG6qsB1MNBlFCLXm0LdaODd1vFYuiss%2FFCN2hCN65lkFKw%2Fu1xvWEPfWYwfHppuip1CqgNwW%2BVNCOPQBvuaFRYe%2BO90gzHx7QaZTtB%2BP39GoXNOLIWoWjOQa9q%2Fz%2F5NMyL7p9rWgWza4wtzSo7MQymmLb2gefgC0LyOKplMdhAv%2Fmr0%2BYE51d7mkOCGzt8UZsF6I%2BCnH0p58DYGrdITfXVWIqYeUPl6iiS51XPtMlSDoqqQZzIb3HHx7wAhEYfXHy3GMLaC5D1izDPudS1Bjq%2FAQ0kf9%2BH0i5yanZ520dyO%2Fm9gOGM87if7HnUWjRbwTABNFaMUSQLSpRfff1%2FV1q0dJZ7fKHDQyj8vF57q8ct%2FCsUKBXRV8RC3zmvAQ%2FCHkQYfU1MFi%2BXjd2VGKlbuby6xOUc5RivFbgsZ08yajs2PQPhn9Ym6sOojkYFz0ti0BSkcfbSaPpmbPBD6S7Gk8uRIJ6aUdw7F2JhFcGRWqbeOvfHiY1xmuBWoUzN75XyTB24pRCNe4ruJdmB19cJKlk6&amp;X-Amz-Algorithm=AWS4-HMAC-SHA256&amp;X-Amz-Date=20240808T193023Z&amp;X-Amz-SignedHeaders=host&amp;X-Amz-Expires=900&amp;X-Amz-Credential=ASIA4CNVNBUU6N6MHID6%2F20240808%2Fus-east-1%2Ffinspace-apricot%2Faws4_request&amp;X-Amz-Signature=2e2ac20ab1dae7b444e79909fe3c04ca39a1205190f032935fa9f5e4a3fa9641</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "pykx.Table(pykx.q('\n",
       "process connected handle address                                             ..\n",
       "-----------------------------------------------------------------------------..\n",
       "tp      1         5      :tcps://vpce-096c514436f930ded-zvyfok7h.vpce-svc-023..\n",
       "'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Check Feedhandler connections, should show connected (1b, e.g. True)\n",
    "fh=kx.QConnection(port=FH_PORT)\n",
    "\n",
    "display( fh (\"select process,connected,handle,address from .conn.procs\") )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e91a23b-b100-4763-9c50-c819f5824202",
   "metadata": {},
   "source": [
    "# All Processes Running\n",
    "All completed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "86f33240-bb12-49f3-8d9c-5783c25eb182",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last Run: 2024-08-08 19:30:25.559770\n"
     ]
    }
   ],
   "source": [
    "print( f\"Last Run: {datetime.datetime.now()}\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81f05fa3-fbe0-443b-986e-428fb1ca4ae6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
