{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "28bea13b-67bd-4a0e-8eab-3b8ffd37259e",
   "metadata": {},
   "source": [
    "# TorQ: Create Everything\n",
    "This notebook will use the AWS boto3 APIs to create the needed resources for a TorQ based application. The notebook will first clone the relevant gihub code (TorQ and TorQ AMazon FinSpace Starter Pack) then proceed to create the necessary AWS resources. \n",
    "\n",
    "Once you have create all clusters, you can see how to query for data through the gateway, see the [pykx_query_all](pykx_query_all.ipynb) notebook\n",
    "\n",
    "To cleanup (delete) all resources, run the [delete_all](delete_all.ipynb) notebook.\n",
    "\n",
    "## AWS Resources Created\n",
    "- Database   \n",
    "- Changeset to add data to database   \n",
    "- Scaling Group that will contain all clusters   \n",
    "- Shared Volume   \n",
    "- Dataview of database on the shared volume   \n",
    "- Clusters\n",
    "\n",
    "This notebook is based on the TorQ Amazon FinSpace starter pack but uses Scaling Groups and Shared Volumes for cost savings.\n",
    "\n",
    "[TorQ Amazon FinSpace Starter Pack](https://dataintellecttech.github.io/TorQ-Amazon-FinSpace-Starter-Pack/)\n",
    "\n",
    "### Branches\n",
    "\n",
    "**TorQ-Amazon-FinSpace-Starter-Pack**: v1.0.2   \n",
    "**TorQ**: v5.0.3\n",
    "\n",
    "**Note**: For other branches, be sure to update the git clone statements below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c036169-ee58-48d0-a20e-f95364a6adf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf torq_app*.zip "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97bf936e-20f2-419b-9e20-320f00f4d6c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf TorQ TorQ-Amazon-FinSpace-Starter-Pack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94703544-445a-4142-9c65-28618797d1e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "!git -c advice.detachedHead=false clone --depth 1 --branch  v1.0.2 https://github.com/DataIntellectTech/TorQ-Amazon-FinSpace-Starter-Pack.git "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb69788a-4523-41d4-9134-cf5e117fbbbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "!git -c advice.detachedHead=false clone --depth 1 --branch v5.0.3 https://github.com/DataIntellectTech/TorQ.git "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b2b0ba1-3169-47d2-a445-243686a91cba",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ln -sf ../finspace_torq.q TorQ-Amazon-FinSpace-Starter-Pack\n",
    "# this is the one modification over what is in the starter-pack on github"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d5f1d4a-ed45-44e3-bf75-9bdb75fcddbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "import boto3\n",
    "import json\n",
    "import datetime\n",
    "\n",
    "import pykx as kx\n",
    "\n",
    "from managed_kx import *\n",
    "from env import *\n",
    "\n",
    "from clusters import *\n",
    "\n",
    "# ----------------------------------------------------------------\n",
    "\n",
    "TORQ_CODEBASE=\"TorQ\"\n",
    "TORQ_FINSPACE_CODEBASE=\"TorQ-Amazon-FinSpace-Starter-Pack\"\n",
    "\n",
    "# Source data directory\n",
    "SOURCE_DATA_DIR=f\"{TORQ_FINSPACE_CODEBASE}/hdb\"\n",
    "\n",
    "# Code directory\n",
    "CODEBASE=\"torq_app\"\n",
    "\n",
    "# S3 Destinations\n",
    "S3_CODE_PATH=\"code\"\n",
    "S3_DATA_PATH=\"data\"\n",
    "\n",
    "NODE_TYPE=\"kx.sg.4xlarge\"\n",
    "\n",
    "DATABASE_CONFIG=[{ \n",
    "    'databaseName': DB_NAME,\n",
    "    'dataviewName': DBVIEW_NAME\n",
    "    }]\n",
    "CODE_CONFIG={ 's3Bucket': S3_BUCKET, 's3Key': f'{S3_CODE_PATH}/{CODEBASE}.zip' }\n",
    "\n",
    "NAS1_CONFIG= {\n",
    "        'type': 'SSD_250',\n",
    "        'size': 1200\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cfe7d89-9f5d-4ceb-ac8c-1f5054a6f15a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using credentials and create service client\n",
    "session = boto3.Session()\n",
    "\n",
    "# create finspace client\n",
    "client = session.client(service_name='finspace')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c3d4047-9583-4b09-b75d-98fd2ddd6c36",
   "metadata": {},
   "source": [
    "# Create the Database\n",
    "Create a database from the supplied data in hdb.tar.gz.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bf690f2-c465-4df8-90f5-1e3b808bb368",
   "metadata": {},
   "source": [
    "## Stage HDB Data on S3\n",
    "Using AWS cli, copy hdb to staging bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aca7f0d7-32cd-443b-b642-e7209b8516ef",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "S3_DEST=f\"s3://{S3_BUCKET}/{S3_DATA_PATH}/{SOURCE_DATA_DIR}/\"\n",
    "\n",
    "if AWS_ACCESS_KEY_ID is not None:\n",
    "    cp = f\"\"\"\n",
    "export AWS_ACCESS_KEY_ID={AWS_ACCESS_KEY_ID}\n",
    "export AWS_SECRET_ACCESS_KEY={AWS_SECRET_ACCESS_KEY}\n",
    "export AWS_SESSION_TOKEN={AWS_SESSION_TOKEN}\n",
    "\n",
    "aws s3 sync --quiet --exclude .DS_Store {SOURCE_DATA_DIR} {S3_DEST}\n",
    "aws s3 ls {S3_DEST}\n",
    "\"\"\"\n",
    "else:\n",
    "    cp = f\"\"\"\n",
    "aws s3 sync --quiet --exclude .DS_Store {SOURCE_DATA_DIR} {S3_DEST}\n",
    "aws s3 ls {S3_DEST}\n",
    "\"\"\"\n",
    "    \n",
    "# execute the S3 copy\n",
    "os.system(cp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17c759c4-ee6c-45c5-a9f6-6acacea3a3be",
   "metadata": {},
   "source": [
    "## Create Managed Database\n",
    "Using the AWS APIs, create a managed database in Managed kdb Insights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d55bd8d3-5629-46f9-bc1f-47bb0308dc0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# assume it exists\n",
    "create_db=False\n",
    "\n",
    "try:\n",
    "    resp = client.get_kx_database(environmentId=ENV_ID, databaseName=DB_NAME)\n",
    "    resp.pop('ResponseMetadata', None)\n",
    "except:\n",
    "    # does not exist, will create\n",
    "    create_db=True\n",
    "\n",
    "if create_db:\n",
    "    print(f\"CREATING Database: {DB_NAME}\")\n",
    "    resp = client.create_kx_database(environmentId=ENV_ID, databaseName=DB_NAME, description=\"Basictick kdb database\")\n",
    "    resp.pop('ResponseMetadata', None)\n",
    "\n",
    "    print(f\"CREATED Database: {DB_NAME}\")\n",
    "\n",
    "print(json.dumps(resp,sort_keys=True,indent=4,default=str))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26d1194e-0c04-49a3-a7e7-a1d23fcff0d9",
   "metadata": {},
   "source": [
    "## Add HDB Data to Database\n",
    "Add the data in the local hdb directory to the managed database using the changeset mechanism. The Data will be copied to S3 then ingested with the create-kx-changeset API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e03d62c-655b-4c64-aa9e-aaa04455a42c",
   "metadata": {},
   "outputs": [],
   "source": [
    "c_set_list = list_kx_changesets(client, environmentId=ENV_ID, databaseName=DB_NAME)\n",
    "len(c_set_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eae61f04-1c9c-468e-bb38-b2e0b94897a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: check is there is a changeset in the database, if so, no need to add another\n",
    "c_set_list = list_kx_changesets(client, environmentId=ENV_ID, databaseName=DB_NAME)\n",
    "\n",
    "if len(c_set_list) == 0:\n",
    "    # if changesets exist, set chagnset_id to last created one\n",
    "\n",
    "    changes=[]\n",
    "\n",
    "    for f in os.listdir(f\"{SOURCE_DATA_DIR}\"):\n",
    "        if os.path.isdir(f\"{SOURCE_DATA_DIR}/{f}\"):\n",
    "            changes.append( { 'changeType': 'PUT', 's3Path': f\"{S3_DEST}{f}/\", 'dbPath': f\"/{f}/\" } )\n",
    "        else:\n",
    "            changes.append( { 'changeType': 'PUT', 's3Path': f\"{S3_DEST}{f}\", 'dbPath': f\"/\" } )\n",
    "\n",
    "    resp = client.create_kx_changeset(environmentId=ENV_ID, databaseName=DB_NAME, \n",
    "        changeRequests=changes)\n",
    "\n",
    "    resp.pop('ResponseMetadata', None)\n",
    "    changeset_id = resp['changesetId']\n",
    "\n",
    "    print(\"Changeset...\")\n",
    "    print(json.dumps(resp,sort_keys=True,indent=4,default=str))\n",
    "else:\n",
    "    c_set_list=sorted(c_set_list, key=lambda d: d['createdTimestamp']) \n",
    "    changeset_id=c_set_list[-1]['changesetId']\n",
    "    print(f\"Using Last changeset: {changeset_id}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4422bdd-7d44-4fb0-8018-0bebd6987704",
   "metadata": {},
   "outputs": [],
   "source": [
    "wait_for_changeset_status(client, environmentId=ENV_ID, databaseName=DB_NAME, changesetId=changeset_id, show_wait=True)\n",
    "print(\"**Done**\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ba008f3-4991-474c-9b3e-43a1dca56257",
   "metadata": {},
   "outputs": [],
   "source": [
    "note_str = \"\"\n",
    "\n",
    "c_set_list = list_kx_changesets(client, environmentId=ENV_ID, databaseName=DB_NAME)\n",
    "\n",
    "if len(c_set_list) == 0:\n",
    "    note_str = \"<<Could not get changesets>>\"\n",
    "    \n",
    "print(100*\"=\")\n",
    "print(f\"Database: {DB_NAME}, Changesets: {len(c_set_list)} {note_str}\")\n",
    "print(100*\"=\")\n",
    "\n",
    "# sort by create time\n",
    "c_set_list = sorted(c_set_list, key=lambda d: d['createdTimestamp']) \n",
    "\n",
    "for c in c_set_list:\n",
    "    c_set_id = c['changesetId']\n",
    "    print(f\"  Changeset: {c_set_id}: Created: {c['createdTimestamp']} ({c['status']})\")\n",
    "    c_rqs = client.get_kx_changeset(environmentId=ENV_ID, databaseName=DB_NAME, changesetId=c_set_id)['changeRequests']\n",
    "\n",
    "    chs_pdf = pd.DataFrame.from_dict(c_rqs).style.hide(axis='index')\n",
    "    display(chs_pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dae0232-3666-491f-8891-dae30e12c9d8",
   "metadata": {},
   "source": [
    "# Create Scaling Group\n",
    "The scaling group represents the total compute avilable to the application. All clusters will be placed into the scaling group ans share the compute and memory of the scaling group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "613be7f8-fb82-4415-b30c-186ed470dba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if scaling group exits, only create if it does not\n",
    "resp = get_kx_scaling_group(client=client, environmentId=ENV_ID, scalingGroupName=SCALING_GROUP_NAME)\n",
    "\n",
    "if resp is None:\n",
    "    resp = client.create_kx_scaling_group(\n",
    "        environmentId = ENV_ID, \n",
    "        scalingGroupName = SCALING_GROUP_NAME,\n",
    "        hostType=NODE_TYPE,\n",
    "        availabilityZoneId = AZ_ID\n",
    "    )\n",
    "else:\n",
    "    print(f\"Scaling Group {SCALING_GROUP_NAME} exists\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cab8d287-4c24-4e53-8dbd-642c73c7faf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "resp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6943fb16-8989-4199-a0fd-5c7c0d5aa56e",
   "metadata": {},
   "source": [
    "# Create Shared Volume\n",
    "The shared volume is a common storage device for the application. Every cluster using the shared volume will have a writable directory named after the cluster, can read the directories named after other clusters in the application using the volume. Also, there is a common "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4a8a247-d029-4f9b-aaf5-c6e2ffe200a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if volume already exists before trying to create one\n",
    "resp = get_kx_volume(client=client, environmentId=ENV_ID, volumeName=VOLUME_NAME)\n",
    "\n",
    "if resp is None:\n",
    "    resp = client.create_kx_volume(\n",
    "        environmentId = ENV_ID, \n",
    "        volumeType = 'NAS_1',\n",
    "        volumeName = VOLUME_NAME,\n",
    "        description = 'Shared volume between TP and RDB',\n",
    "        nas1Configuration = NAS1_CONFIG,\n",
    "        azMode='SINGLE',\n",
    "        availabilityZoneIds=[ AZ_ID ]    \n",
    "    )\n",
    "else:\n",
    "    print(f\"Volume {VOLUME_NAME} exists\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ec4876f-5ecc-420e-aff8-9c88c8c9deb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "resp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e718537-8853-4f21-8cc7-36b489e380c4",
   "metadata": {},
   "source": [
    "# Wait for Volume and Scaling Group\n",
    "Before proceeding to use Volumes and Scaling groups, wait for their creation to complete.\n",
    "\n",
    "Volume will be used by the dataview.    \n",
    "Dataview and Scaling Group will be used by the clusters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3ed8931-e458-4ffb-83cc-0aa4da4d9f98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# wait for the scaling group to create\n",
    "wait_for_scaling_group_status(client=client, environmentId=ENV_ID, scalingGroupName=SCALING_GROUP_NAME, show_wait=True)\n",
    "print(\"** DONE **\")\n",
    "\n",
    "# wait for the volume to create\n",
    "wait_for_volume_status(client=client, environmentId=ENV_ID, volumeName=VOLUME_NAME, show_wait=True)\n",
    "print(\"** DONE **\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe41eaeb-9c8e-44d3-b8bc-f354142f9140",
   "metadata": {},
   "source": [
    "# Create Dataview\n",
    "Create a dataview, for a specific (static) version of the database and have all of its data cached using the shared volume."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03434316-4ccc-420d-adee-715e6eb1bcd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if dataview already exists and is set to the requested changeset_id\n",
    "resp = get_kx_dataview(client=client, environmentId=ENV_ID, databaseName=DB_NAME, dataviewName=DBVIEW_NAME)\n",
    "\n",
    "if resp is None:\n",
    "    # sort changeset list by create time\n",
    "    c_set_list = sorted(c_set_list, key=lambda d: d['createdTimestamp']) \n",
    "\n",
    "    resp = client.create_kx_dataview(\n",
    "        environmentId = ENV_ID, \n",
    "        databaseName=DB_NAME, \n",
    "        dataviewName=DBVIEW_NAME,\n",
    "        azMode='SINGLE',\n",
    "        availabilityZoneId=AZ_ID,\n",
    "        changesetId=c_set_list[-1]['changesetId'],\n",
    "        segmentConfigurations=[\n",
    "            { \n",
    "                'dbPaths': ['/*'],\n",
    "                'volumeName': VOLUME_NAME\n",
    "            }\n",
    "        ],\n",
    "        autoUpdate=False,\n",
    "        description = f'Dataview of database'\n",
    "    )\n",
    "else:\n",
    "    print(f\"Dataview {DBVIEW_NAME} exists\")        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "734ab7da-ef78-4701-9a58-1eeacbd9c557",
   "metadata": {},
   "outputs": [],
   "source": [
    "# wait for the view to create\n",
    "wait_for_dataview_status(client=client, environmentId=ENV_ID, databaseName=DB_NAME, dataviewName=DBVIEW_NAME, show_wait=True)\n",
    "print(\"** DONE **\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dea431b0-c501-46bb-b72a-a5eb80a335b0",
   "metadata": {},
   "source": [
    "# Create Clusters\n",
    "With foundation resources now completed, create the needed clusters for the application."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f0c06ab-4dcb-4cc6-abc9-2c77ff3a4242",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Stage Code to S3\n",
    "Code to be used in this application must be staged to an S3 bucket the service can read from, that code will then be deployed to the clusters as part of their creation workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b502a0a5-8610-4fc8-b6b7-04c47e89ba75",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# zip the code\n",
    "os.system(f\"zip -q -r {CODEBASE}.zip {TORQ_CODEBASE}/ {TORQ_FINSPACE_CODEBASE}/ -x '*.ipynb_checkpoints*' -x '*/hdb/*' -x '*.git*' -x '*/tests/*' -x '*/terraform-deployment/*' -x '*/docs/*' -x '*/lib/*' -x '*/html/*' -x '*/datadog/*'  -x '*/monit/*'\")\n",
    "\n",
    "# copy code to S3\n",
    "if AWS_ACCESS_KEY_ID is not None:\n",
    "    cp = f\"\"\"\n",
    "export AWS_ACCESS_KEY_ID={AWS_ACCESS_KEY_ID}\n",
    "export AWS_SECRET_ACCESS_KEY={AWS_SECRET_ACCESS_KEY}\n",
    "export AWS_SESSION_TOKEN={AWS_SESSION_TOKEN}\n",
    "\n",
    "aws s3 cp --exclude .DS_Store {CODEBASE}.zip s3://{S3_BUCKET}/code/{CODEBASE}.zip\n",
    "aws s3 ls s3://{S3_BUCKET}/code/\n",
    "\"\"\"\n",
    "else:\n",
    "    cp = f\"\"\"\n",
    "aws s3 cp --exclude .DS_Store {CODEBASE}.zip s3://{S3_BUCKET}/code/{CODEBASE}.zip\n",
    "aws s3 ls s3://{S3_BUCKET}/code/\n",
    "\"\"\"\n",
    "    \n",
    "# execute the S3 copy\n",
    "os.system(cp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0a75d53-e567-4a2d-b47b-170829d79f63",
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in clusters:\n",
    "#for c in clusters[5:6]:\n",
    "    # wait for a cluster?\n",
    "    if c['type'] == \"WAIT\":\n",
    "        wait_for_cluster_status(client, environmentId=ENV_ID, clusterName=c['name'], show_wait=True)\n",
    "        continue\n",
    "    \n",
    "    cluster_name = c['name']\n",
    "    cluster_type = c['type']\n",
    "    cluster_init = c['init']\n",
    "    cluster_args = c['args']\n",
    "    \n",
    "    # cluster already exists\n",
    "    resp = get_kx_cluster(client, environmentId=ENV_ID, clusterName=cluster_name)\n",
    "    if resp is not None:\n",
    "        print(f\"Cluster: {cluster_name} already exists\")\n",
    "        continue\n",
    "    \n",
    "    print(f\"Creating: {cluster_name}\")\n",
    "    \n",
    "    resp = client.create_kx_cluster(\n",
    "        environmentId=ENV_ID, \n",
    "        clusterName=cluster_name,\n",
    "        clusterType=cluster_type,\n",
    "        releaseLabel = '1.0',\n",
    "        executionRole=EXECUTION_ROLE,\n",
    "        databases=DATABASE_CONFIG,\n",
    "        scalingGroupConfiguration={\n",
    "            'memoryReservation': 6,\n",
    "            'nodeCount': 1,\n",
    "            'scalingGroupName': SCALING_GROUP_NAME,\n",
    "        },\n",
    "        savedownStorageConfiguration ={ 'volumeName': VOLUME_NAME },\n",
    "#        tickerplantLogConfiguration ={ 'tickerplantLogVolumes': [ VOLUME_NAME ] },\n",
    "        clusterDescription=\"Created with create_all notebook\",\n",
    "        code=CODE_CONFIG,\n",
    "        initializationScript=cluster_init,\n",
    "        commandLineArguments=cluster_args,\n",
    "        azMode=AZ_MODE,\n",
    "        availabilityZoneId=AZ_ID,\n",
    "        vpcConfiguration={ \n",
    "            'vpcId': VPC_ID,\n",
    "            'securityGroupIds': SECURITY_GROUPS,\n",
    "            'subnetIds': SUBNET_IDS,\n",
    "            'ipAddressType': 'IP_V4' }\n",
    "    )\n",
    "    \n",
    "    display(resp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8491391-8c01-4190-a2bd-23fa888bf781",
   "metadata": {},
   "source": [
    "## Wait for all clusters to finish creating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0946ca26-c4b0-410d-ade5-18a47bf2318a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wait for all clusters to start\n",
    "for c in clusters:\n",
    "    cluster_name = c['name']\n",
    "    wait_for_cluster_status(client, environmentId=ENV_ID, clusterName=cluster_name, show_wait=True)\n",
    "\n",
    "print(\"** ALL DONE **\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "230207e8-c297-4d7e-af65-4396fa5b4deb",
   "metadata": {},
   "source": [
    "# List Clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c50c578-05e8-49e7-8deb-1f6b94b10221",
   "metadata": {},
   "outputs": [],
   "source": [
    "cdf = get_clusters(client, environmentId=ENV_ID)\n",
    "\n",
    "all_clusters = [d['name'] for d in clusters if 'name' in d]\n",
    "\n",
    "if cdf is not None:\n",
    "    cdf = cdf[cdf['clusterName'].isin(all_clusters)]\n",
    "\n",
    "display(cdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e91a23b-b100-4763-9c50-c819f5824202",
   "metadata": {},
   "source": [
    "# All Processes Running"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86f33240-bb12-49f3-8d9c-5783c25eb182",
   "metadata": {},
   "outputs": [],
   "source": [
    "print( f\"Last Run: {datetime.datetime.now()}\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81f05fa3-fbe0-443b-986e-428fb1ca4ae6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
